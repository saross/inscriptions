{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918911a7-e881-464f-80a8-08cb1dde88e0",
   "metadata": {},
   "source": [
    "# A Summed Probability Analysis of inscriptions from the Latin-speaking Roman Empire\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project aims to apply summed probability analysis (SPA) to Latin inscriptions. For decades, SPA has been applied to radiocarbon dates in archaeology. SPA relies on the fact that each date represtents data about human occupation, and so tallying them provides insights into demographic changes (i.e., more radiocarbon dates equals more people). Problems with SPA using radiocarbon dates are addressed through use of the large datasets and thoughtful statistical approaches. \n",
    "\n",
    "Like radiocarbon dates, inscriptions provide a data-point: a place and a date of human activity. Also like radiocarbon dates, inscription dates come with temporal uncertainty, and are subject to the vagaries of preservation. Inscriptions have the advantage, however, of directly representing a human act in the past (the placing of an inscription), not just the taking of a sample by a modern researcher that may or may not be related to past human activity. \n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used for this analysis is Latin Inscriptions of the Roman Empire (LIRE; https://doi.org/10.5281/zenodo.8147298), which has been compiled and cleaned from various digital inscription corpora by the SDAM group at Aarhus University, Denmark. It contains over 182,000 inscriptions dating from 50 BC - AD 350. \n",
    "\n",
    "If this analysis proves successful, I will consider expanding it to the Latin Inscriptions in Space and Time (LIST) dataset, which requires additional cleaning but extends the data forward past AD 350, perhaps helping to clarify 4th-century trends.\n",
    "\n",
    "## Approach\n",
    "\n",
    "After some initial (minor) cleaning and the addition of a 'date_range' column (subtracting 'not_before' from 'not_after' for each inscription) and a 'province_language' column (to identify primarily Latin-speaking provinces), a series of statistical steps are undertaken:\n",
    "\n",
    "1. Generate descriptive statistics of date ranges, including inscription counts under certain date-range thresholds, to get an idea of how much termporal uncertainty exists in the dataset.\n",
    "2. Explore the minimum sample size needed for SPA when examining spatially, temporally, or otherwise limited subsets of the data, ensuring this sample size remains representative.\n",
    "3. Run SPA of the entire dataset, including under various date-range thresholds and sample sizes, to shed light on the trade-offs between sample size and temporal uncertainty.\n",
    "4. Run SPA on various subsets of the data:\n",
    "    * Latin-speaking empire with and without Roma\n",
    "    * Each province\n",
    "    * Each urban area\n",
    "5. Reproduce Hanson's results showing statistically significant relationship between independent calculation of urban area population and inscription count.\n",
    "    * Characterise the strength and explanatory power of this relationship.\n",
    "    * Re-run this correlation analysis using letter count, not just inscription count, to test Hanson's information content theory explaining the relationship between population and inscription count.\n",
    "    * Check to see if the letter count approach correlates more or less than the inscription count approach to population.\n",
    "6. Re-run the SPA using letter count..\n",
    "7. Calculate urban area, province, and empire population changes between AD 1 - 300 using either inscription count or letter count, depending on which looks most plausible.\n",
    "\n",
    "## Results\n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad1030-a1a4-4475-a2b4-d844cebdc602",
   "metadata": {},
   "source": [
    "*ChatGPT-4 used throughout to generate code snippets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60cd32-d760-4749-936b-833ef8d8f7cd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799f33b-81f6-4e78-88e2-d68d2d417dca",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9849fcaf-00e4-4382-93d5-1ec9f6c9bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anyio==4.3.0 (from -r requirements.txt (line 1))\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting argon2-cffi==23.1.0 (from -r requirements.txt (line 2))\n",
      "  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (21.2.0)\n",
      "Collecting arrow==1.3.0 (from -r requirements.txt (line 4))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting asttokens==2.4.1 (from -r requirements.txt (line 5))\n",
      "  Using cached asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: async-lru==2.0.4 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (2.0.4)\n",
      "Collecting attrs==23.2.0 (from -r requirements.txt (line 7))\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting Babel==2.15.0 (from -r requirements.txt (line 8))\n",
      "  Using cached Babel-2.15.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting beautifulsoup4==4.12.3 (from -r requirements.txt (line 9))\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach==6.1.0 (from -r requirements.txt (line 10))\n",
      "  Using cached bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: certifi==2024.2.2 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (2024.2.2)\n",
      "Requirement already satisfied: cffi==1.16.0 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (1.16.0)\n",
      "Collecting charset-normalizer==3.3.2 (from -r requirements.txt (line 13))\n",
      "  Using cached charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: click==8.1.7 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (8.1.7)\n",
      "Collecting click-plugins==1.1.1 (from -r requirements.txt (line 15))\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting cligj==0.7.2 (from -r requirements.txt (line 16))\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting comm==0.2.2 (from -r requirements.txt (line 17))\n",
      "  Using cached comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting contourpy==1.2.1 (from -r requirements.txt (line 18))\n",
      "  Using cached contourpy-1.2.1-cp39-cp39-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cramjam==2.8.3 (from -r requirements.txt (line 19))\n",
      "  Using cached cramjam-2.8.3-cp39-none-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting cycler==0.12.1 (from -r requirements.txt (line 20))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting debugpy==1.8.1 (from -r requirements.txt (line 21))\n",
      "  Using cached debugpy-1.8.1-cp39-cp39-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 22)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 23)) (0.7.1)\n",
      "Collecting executing==2.0.1 (from -r requirements.txt (line 24))\n",
      "  Using cached executing-2.0.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting fastjsonschema==2.19.1 (from -r requirements.txt (line 25))\n",
      "  Using cached fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fastparquet==2024.2.0 (from -r requirements.txt (line 26))\n",
      "  Using cached fastparquet-2024.2.0-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting fiona==1.9.6 (from -r requirements.txt (line 27))\n",
      "  Using cached fiona-1.9.6-cp39-cp39-win_amd64.whl.metadata (51 kB)\n",
      "Requirement already satisfied: fonttools==4.51.0 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 28)) (4.51.0)\n",
      "Collecting fqdn==1.5.1 (from -r requirements.txt (line 29))\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 30)) (2024.3.1)\n",
      "Collecting geopandas==0.14.4 (from -r requirements.txt (line 31))\n",
      "  Using cached geopandas-0.14.4-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting h11==0.14.0 (from -r requirements.txt (line 32))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httpcore==1.0.5 (from -r requirements.txt (line 33))\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting httpx==0.27.0 (from -r requirements.txt (line 34))\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: idna==3.7 in d:\\users\\shawn\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 35)) (3.7)\n",
      "Collecting ipykernel==6.29.4 (from -r requirements.txt (line 36))\n",
      "  Using cached ipykernel-6.29.4-py3-none-any.whl.metadata (6.3 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 7.1.0, 7.30.0, 8.13.0, 8.16.0, 8.17.0\n",
      "ERROR: Ignored the following versions that require a different python version: 8.19.0 Requires-Python >=3.10; 8.20.0 Requires-Python >=3.10; 8.21.0 Requires-Python >=3.10; 8.22.0 Requires-Python >=3.10; 8.22.1 Requires-Python >=3.10; 8.22.2 Requires-Python >=3.10; 8.23.0 Requires-Python >=3.10; 8.24.0 Requires-Python >=3.10; 8.25.0 Requires-Python >=3.10\n",
      "ERROR: Could not find a version that satisfies the requirement ipython==8.24.0 (from versions: 0.10, 0.10.1, 0.10.2, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.13.2, 1.0.0, 1.1.0, 1.2.0, 1.2.1, 2.0.0, 2.1.0, 2.2.0, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 3.0.0, 3.1.0, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 4.0.0b1, 4.0.0, 4.0.1, 4.0.2, 4.0.3, 4.1.0rc1, 4.1.0rc2, 4.1.0, 4.1.1, 4.1.2, 4.2.0, 4.2.1, 5.0.0b1, 5.0.0b2, 5.0.0b3, 5.0.0b4, 5.0.0rc1, 5.0.0, 5.1.0, 5.2.0, 5.2.1, 5.2.2, 5.3.0, 5.4.0, 5.4.1, 5.5.0, 5.6.0, 5.7.0, 5.8.0, 5.9.0, 5.10.0, 6.0.0rc1, 6.0.0, 6.1.0, 6.2.0, 6.2.1, 6.3.0, 6.3.1, 6.4.0, 6.5.0, 7.0.0b1, 7.0.0rc1, 7.0.0, 7.0.1, 7.1.1, 7.2.0, 7.3.0, 7.4.0, 7.5.0, 7.6.0, 7.6.1, 7.7.0, 7.8.0, 7.9.0, 7.10.0, 7.10.1, 7.10.2, 7.11.0, 7.11.1, 7.12.0, 7.13.0, 7.14.0, 7.15.0, 7.16.0, 7.16.1, 7.16.2, 7.16.3, 7.17.0, 7.18.0, 7.18.1, 7.19.0, 7.20.0, 7.21.0, 7.22.0, 7.23.0, 7.23.1, 7.24.0, 7.24.1, 7.25.0, 7.26.0, 7.27.0, 7.28.0, 7.29.0, 7.30.1, 7.31.0, 7.31.1, 7.32.0, 7.33.0, 7.34.0, 8.0.0a1, 8.0.0b1, 8.0.0rc1, 8.0.0, 8.0.1, 8.1.0, 8.1.1, 8.2.0, 8.3.0, 8.4.0, 8.5.0, 8.6.0, 8.7.0, 8.8.0, 8.9.0, 8.10.0, 8.11.0, 8.12.0, 8.12.1, 8.12.2, 8.12.3, 8.13.1, 8.13.2, 8.14.0, 8.15.0, 8.16.1, 8.17.1, 8.17.2, 8.18.0, 8.18.1)\n",
      "ERROR: No matching distribution found for ipython==8.24.0\n"
     ]
    }
   ],
   "source": [
    "# in jupyter notebooks, normal install using !pip install doesn't work, use the solution from:\n",
    "# https://www.geeksforgeeks.org/install-python-package-using-jupyter-notebook/\n",
    "\n",
    "# Requirements now defined in a text file. This approach allows the notebook to run in BinderHub. \n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e40eb7",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bc95b8-b28f-4feb-a48e-2ccc3edad2ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fastparquet\n",
    "import tempun # Vojtek's code to model temporal uncertainty\n",
    "import random\n",
    "import os # Saving and loading bootstrap samples\n",
    "import re # Regular expressions to fix bootsrtap filenames\n",
    "import hashlib # Generating hashes for bootstrap sample filenames\n",
    "import statsmodels.api as sm # OLS regression and other statistics\n",
    "import scipy.stats as stats # Spearman's Rank Correlation and non-normal distributions\n",
    "import seaborn as sns # Better charts and graphs\n",
    "import tabulate # Prettier tables\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.ndimage import uniform_filter1d  # More efficient moving average calculation\n",
    "from tqdm.auto import tqdm # Adds a progress bar to lengthy operations; tqdm.aut automatically adapts to the environment it is run it to provide a GUI-based progress bar where possible\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std # Compute the prediction standard errors \n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial # Bootstrapped Negative Binomial Regresson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf5df7-697e-4c3c-9886-ff16fca504e9",
   "metadata": {},
   "source": [
    "## Import LIRE_v2.3 dataset \n",
    "\n",
    "**Dataset DOI**: https://doi.org/10.5281/zenodo.8147298 \n",
    "\n",
    "**Version**: 2.3 (14 July 2023)\n",
    "\n",
    "**Format**: parquet\n",
    "\n",
    "**License**: CC-BY-4.0\n",
    "\n",
    "**Cite as**: Kaše, Vojtěch, Heřmánková, Petra, & Sobotková, Adéla. (2023). LIRE (v2.3) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.8147298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21834c-d8f9-4d11-bcdd-ef64baf5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lire = gpd.read_parquet('data/LIRE_v2-3.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b156d-4233-4168-9770-a8014ac5fb74",
   "metadata": {},
   "source": [
    "### Display first five rows of LIRE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823a26c-2bdb-4bdd-ba9e-d70a033cb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lire.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111e418-5c89-4eaa-b4c6-bb14c4ce3660",
   "metadata": {},
   "source": [
    "### Display column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffc928-29ef-4fbb-b753-4f860dfc5888",
   "metadata": {},
   "source": [
    "Also set column names as a variable so that column names can be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8994a-0e2b-434f-952f-213febd2a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lire_columns = lire.columns\n",
    "print(\"\\n\".join(lire_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1aa61a-da0d-4b16-92cd-1443cf078c93",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831d9e1-f996-4804-a4d5-66d493a83964",
   "metadata": {},
   "source": [
    "The quality of the dataset is good, but I found some errors where the date range is negative. This block of code lists any inscriptions where the 'not_before' date is greater than 'not_after' date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba67b65-5461-487b-a5a4-b162b838f80d",
   "metadata": {},
   "source": [
    "### Identify records with a date range less than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2c502-46ca-4565-b0e1-a7157fb1ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where date_range is less than 0\n",
    "error_rows = lire[lire['not_before'] > lire['not_after']]\n",
    "\n",
    "# Display only specific columns for the filtered records\n",
    "print(error_rows[['LIST-ID', 'raw_dating', 'not_before', 'not_after']])\n",
    "\n",
    "error_rows.to_csv(\"error_rows\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e5e80-f54b-44db-a7ab-e42b11d0f02e",
   "metadata": {},
   "source": [
    "### Swap the 'not_before' and 'not_after' values to resolve negative date-range error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6650-eff9-4c37-93ac-74e2657fb4e0",
   "metadata": {},
   "source": [
    "For any rows where the 'not_before' date is greater than the 'not_after' date (yielding a negative date range), reverse the dates. This solution assumes transposition as the source of the error, which seems to be the cause of the problem in most cases, when looking at the problematic records. In some cases the 'not_before' and 'not_after' dates do not match the 'raw_dating' values; I need to ask the dataset authors if these changes are intentional or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0853a69-18b3-4eae-873f-7e8e5b4845f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to swap 'not_before' and 'not_after' when 'not_before' > 'not_after'\n",
    "def swap_dates(row):\n",
    "    if row['not_before'] > row['not_after']:\n",
    "        row['not_before'], row['not_after'] = row['not_after'], row['not_before']\n",
    "    return row\n",
    "\n",
    "# Apply the function to swap the dates\n",
    "lire = lire.apply(swap_dates, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c60795-5846-4f48-9e6f-1301665ae97d",
   "metadata": {},
   "source": [
    "## Add ``date_range`` column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4731c-72c6-463d-a221-6ad3eb0ff0a5",
   "metadata": {},
   "source": [
    "SPA operates on an inscription's date range, calculated by subtracting the earliest possible date for the inscription ('not_before') from the latest possible date ('not-after'). \n",
    "\n",
    "This block of code checcks to see if a 'date_range' column already exists, and calculates one if it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206b6d3-eb76-4ae7-9283-d8808cedcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'date_range' column already exists\n",
    "if 'date_range' not in lire.columns:\n",
    "    # If it doesn't exist, calculate date range for each inscription and add it to the LIRE dataframe\n",
    "    lire['date_range'] = lire['not_after'] - lire['not_before']\n",
    "    # Show the updated DataFrame to check that the new column has been added successfully\n",
    "    print(\"date_range column added.\")\n",
    "    # print(lire.head()) # Option to print the first few records of the dataset to confirm\n",
    "else:\n",
    "    print(\"date_range column already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e11a8-0266-42ce-a44e-3950a69eff5c",
   "metadata": {},
   "source": [
    "## Add a ``province_language`` column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b9c46-478a-4654-ae43-3f5624bf6796",
   "metadata": {},
   "source": [
    "This dataset contains mostly Latin inscriptions. As a result, inscription counts mean something different in Latin-speaking provinces versus Greek-speaking provinces. Here I assume that Latin inscriptions are more representative of demographics in Latin-speaking provinces, whereas they represent particular types of official activities, displays of Roman identity, or other more restricted meanings in Greak-speaking provinces. Counts of Latin inscriptions, moreover, are lower in Greek-speaking provinces.\n",
    "\n",
    "This code designates each province as Latin or Greek speaking, and adds that information in a new column, so that Latin-speaking provinces can be analysed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a397c-952b-4750-b461-6f69728dd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each province to its primary language\n",
    "province_language_map = {\n",
    "    'Roma': 'Latin',\n",
    "    'Latium et Campania / Regio I': 'Latin',\n",
    "    'Dalmatia': 'Latin',\n",
    "    'Hispania citerior': 'Latin',\n",
    "    'Germania superior': 'Latin',\n",
    "    'Venetia et Histria / Regio X': 'Latin',\n",
    "    'Dacia': 'Latin',\n",
    "    'Britannia': 'Latin',\n",
    "    'Pannonia superior': 'Latin',\n",
    "    'Samnium / Regio IV': 'Latin',\n",
    "    'Africa proconsularis': 'Latin',\n",
    "    'Germania inferior': 'Latin',\n",
    "    'Apulia et Calabria / Regio II': 'Latin',\n",
    "    'Pannonia inferior': 'Latin',\n",
    "    'Numidia': 'Latin',\n",
    "    'Etruria / Regio VII': 'Latin',\n",
    "    'Umbria / Regio VI': 'Latin',\n",
    "    'Noricum': 'Latin',\n",
    "    'Baetica': 'Latin',\n",
    "    'Transpadana / Regio XI': 'Latin',\n",
    "    'Moesia inferior': 'Latin',\n",
    "    'Lusitania': 'Latin',\n",
    "    'Moesia superior': 'Latin',\n",
    "    'Sardinia': 'Latin',\n",
    "    'Belgica': 'Latin',\n",
    "    'Gallia Narbonensis': 'Latin',\n",
    "    'Aemilia / Regio VIII': 'Latin',\n",
    "    'Picenum / Regio V': 'Latin',\n",
    "    'Raetia': 'Latin',\n",
    "    'Macedonia': 'Greek',\n",
    "    'Aquitani(c)a': 'Latin',\n",
    "    'Bruttium et Lucania / Regio III': 'Latin',\n",
    "    'Liguria / Regio IX': 'Latin',\n",
    "    'Lugudunensis': 'Latin',\n",
    "    'Mauretania Caesariensis': 'Latin',\n",
    "    'Asia': 'Greek',\n",
    "    'Belgica | Germania superior': 'Latin',\n",
    "    'Sicilia': 'Latin',\n",
    "    'Syria': 'Greek',\n",
    "    'Achaia': 'Greek',\n",
    "    'Alpes Cottiae': 'Latin',\n",
    "    'Alpes Maritimae': 'Latin',\n",
    "    'Galatia': 'Greek',\n",
    "    'Thracia': 'Greek',\n",
    "    'Aegyptus': 'Greek'\n",
    "}\n",
    "\n",
    "# Check if the 'province_language' column already exists\n",
    "if 'province_language' not in lire.columns:\n",
    "    # If it doesn't exist, add a new column to the DataFrame, 'province_language', \n",
    "    # mapping the province to its primary language\n",
    "    lire['province_language'] = lire['province'].map(province_language_map)\n",
    "    # Show the updated DataFrame to check that the new column has been added successfully\n",
    "    print(\"province_language column added.\")\n",
    "    # print(lire.head()) # Option to print the first few records of the dataset to confirm\n",
    "else:\n",
    "    print(\"province_language column already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80feda7a-ab88-48c1-b394-c374d5afc8d8",
   "metadata": {},
   "source": [
    "# Exploring the LIRE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fe3c3-ad23-4fb3-90fb-d3a101d034f8",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics for date ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106f809-342d-4967-b87a-1bc95122dba4",
   "metadata": {},
   "source": [
    "Display the total number of records, average date range, median date range, and standard deviation of date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bb647-1c16-4494-9d11-b31f34423e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the total number of records\n",
    "total_records = lire.shape[0]\n",
    "\n",
    "# Find the largest date range\n",
    "max_range = lire['date_range'].max()\n",
    "\n",
    "# Calculate the average date range\n",
    "average_range = lire['date_range'].mean()\n",
    "\n",
    "# Calculate the median date range\n",
    "median_range = lire['date_range'].median()\n",
    "\n",
    "# Calculate the standard deviation of the date range assuming a uniform distribution\n",
    "lire['std_dev_range_uniform'] = np.sqrt((lire['not_after'] - lire['not_before'])**2 / 12)\n",
    "\n",
    "# Calculate the average standard deviation\n",
    "average_std_dev_range_uniform = lire['std_dev_range_uniform'].mean()\n",
    "\n",
    "# Calculate the coefficient of variation\n",
    "cv_range = (average_std_dev_range_uniform / average_range) * 100\n",
    "\n",
    "# Calculate the range of the date_range column\n",
    "range_data = lire['date_range'].max() - lire['date_range'].min()\n",
    "\n",
    "# Calculate the variance of the date range assuming a uniform distribution\n",
    "lire['variance_range_uniform'] = (lire['not_after'] - lire['not_before'])**2 / 12\n",
    "\n",
    "# Calculate the average variance\n",
    "average_variance_range_uniform = lire['variance_range_uniform'].mean()\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(f\"Total number of records: {total_records}\")\n",
    "print(f\"Maximum Date Range: {max_range}\")\n",
    "print(f\"Average Date Range: {average_range}\")\n",
    "print(f\"Median Date Range: {median_range}\")\n",
    "print(f\"Standard Deviation of Date Range: {average_std_dev_range_uniform}\")\n",
    "print(f\"Coefficient of Variation of Date Range: {cv_range}%\")\n",
    "print(f\"Range of Date Range: {range_data}\")\n",
    "print(f\"Average Variance of Date Range (Uniform Distribution): {average_variance_range_uniform}\")\n",
    "\n",
    "plt.boxplot(lire['date_range'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6917903-8b11-4d65-91a5-c24a12947d84",
   "metadata": {},
   "source": [
    "## Create a histogram of date ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e4b6b-2635-4168-8d57-964a609f3f94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b5a716",
   "metadata": {},
   "source": [
    "Create a historgram of date ranges using 10-year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(lire['date_range'], bins=np.arange(0, lire['date_range'].max() + 10, 10), edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date Range (Years)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Date Ranges for Roman Inscriptions (10-year bins)')\n",
    "\n",
    "# set axis limits and grid\n",
    "plt.xlim([0, 600])\n",
    "plt.ylim([0, 50000])\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(0, 600, 50), rotation='vertical')  # Change 10 to the desired tick interval for the x-axis\n",
    "plt.yticks(np.arange(0, 50000, 10000))  # Change 500 to the desired tick interval for the y-axis\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017dc15c-0bce-4b8b-8ff9-d76f62a14540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate histogram using seaborn\n",
    "# To do: continue work on seaborn chart and decide whether or not to use this package for other charts and graphs\n",
    "\n",
    "snsplot = sns.displot(lire, x=\"date_range\", bins=np.arange(0, lire['date_range'].max() + 10, 10), edgecolor='black', alpha=0.7)\n",
    "snsplot.set(xlim=(0, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58849105-3272-48f5-8158-4c5a366b465e",
   "metadata": {},
   "source": [
    "### Discussion: descriptive statitics and histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548480af-9b9c-4cad-a725-60d943deebd6",
   "metadata": {},
   "source": [
    "The dataset has over 182k records. The median and average date range are close (101.3 and 99.0 years respectively). The standard devation is 29.24 years (coefficient of variation = 28.87). Circa 30 SD and coefficient of variance are moderate-high. Average varience is high compared to the average value (1346 versus 101), indicating a relatively high level of dispersion. **This dataset has high variability and many outliers**.\n",
    "\n",
    "The box plot shows a couple of outliers near 2000 years, which probably need to be fixed in the data (max date range should be about 600 years).\n",
    "\n",
    "Inscription date-range has spikes at round numbers (e.g., 0-10, 40-50, 90-100, 140-150, 190-200, 290-300 - but not 240-250), reflecting that researchers who assigned approximate dates tended to choose such round numbers. Above 80 years, there are few inscriptions in bins other than the round-numbered ones. from 0-80 years, however, there are 1,000s of inscriptons in each bin (with spikes at 0-10 and 40-50). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbad81-48c0-45bc-a548-9db73be308a1",
   "metadata": {},
   "source": [
    "## Count number of inscriptions *between* various date-range thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4a826-cafe-48dd-b5bd-c2ccab3e9e0f",
   "metadata": {},
   "source": [
    "This code block calculates the number of inscriptions falling within certain date ranges, e.g., 0-1 year, 1-10 years, 10-25 years, etc., in order to get a sense of typical chronological uncertainty associated with the inscriptions, and a sense of the distribution of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831f739-271a-4a2a-ac0b-f1d8b594ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "thresholds = [1, 10, 25, 50, 100, 200, 300] \n",
    "\n",
    "# Create bins using the thresholds\n",
    "bins = [0] + thresholds + [lire['date_range'].max() + 1]\n",
    "\n",
    "# Categorize the date ranges based on the bins\n",
    "lire['date_range_bins'] = pd.cut(lire['date_range'], bins, right=False) # Count is right-exclusive\n",
    "\n",
    "# Count the number of inscriptions in each bin\n",
    "inscription_counts = lire['date_range_bins'].value_counts().sort_index()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Counts of inscriptions with date ranges between certain thresholds:\")\n",
    "print(inscription_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fb9bc-9274-4386-9fd0-2316f0cd84cd",
   "metadata": {},
   "source": [
    "## Count number of inscriptions with date ranges *under* various thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a015243-7108-43fb-b2e6-e124c6fc78db",
   "metadata": {},
   "source": [
    "As above, but cumulative total of inscriptions under particular date-range thresholds (e.g., <1 year, <10 years, <25 years, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccba6d-151c-4110-bcc1-49e993cdc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "thresholds = [1, 10, 25, 50, 100, 200, 300]\n",
    "\n",
    "# Initialize an empty dictionary to store counts\n",
    "cumulative_counts = {}\n",
    "\n",
    "# Calculate counts for each threshold\n",
    "for threshold in thresholds:\n",
    "    count = len(lire[lire[\"date_range\"] < threshold])  # counts are right-exclusive\n",
    "    cumulative_counts[threshold] = count\n",
    "\n",
    "# Display the counts\n",
    "print(\"Cumulative counts of inscriptions with date ranges under various thresholds:\")\n",
    "for threshold, count in cumulative_counts.items():\n",
    "    print(f\"0-{threshold} years: {count}\")\n",
    "\n",
    "tabulate.tabulate(\n",
    "    cumulative_counts.items(), tablefmt=\"html\", headers=[\"Threshold (Years)\", \"Count\"], intfmt=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f524c2-46df-4bab-b805-d50d335497bd",
   "metadata": {},
   "source": [
    "### Discussion: Date-range distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4f1be-5d88-4372-a936-28416f239adf",
   "metadata": {},
   "source": [
    "The single largest bin for date ranges is 50-100 years. The next largest bins are 25-50 years and 100-200 years. About two-thirds of inscriptions, however,have an uncertainty of 100 years or less. About one-third have an uncertainty of 50 years or less.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f1744-2860-4ab4-bdfc-a4febea111a0",
   "metadata": {},
   "source": [
    "## Preliminary SPA calculation, all inscriptions, uniform distribution, 5-year bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698a738-3744-4188-9229-95e967f8815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the summed probability distribution\n",
    "earliest_date = lire['not_before'].min()\n",
    "latest_date = lire['not_after'].max()\n",
    "resolution = 5 # Resolution defines the size of bins or segments in years\n",
    "\n",
    "# Creat an array of zeros to hold the summed probabilities\n",
    "num_bins = int((latest_date - earliest_date) / resolution) + 1\n",
    "summed_prob = np.zeros(num_bins)\n",
    "\n",
    "# Loop through each inscription to update the summed probability distribution\n",
    "for index, row in lire.iterrows():\n",
    "    start = row['not_before']\n",
    "    end = row['not_after']\n",
    "    \n",
    "    start_idx = int((start - earliest_date) / resolution)\n",
    "    end_idx = int((end - earliest_date) / resolution)\n",
    "   \n",
    "    # Calculate uniform distribution for this inscription,\n",
    "    # accounting for date combinations that add up to zero\n",
    "    try:\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1)\n",
    "    except ZeroDivisionError:\n",
    "        uniform_prob = 1  # set default value here\n",
    "\n",
    "    # Add this distribution to the overall summed distribution\n",
    "    summed_prob[start_idx : end_idx + 1] += uniform_prob\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(np.arange(earliest_date, latest_date + resolution, resolution), summed_prob, width=resolution, color='blue', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Summed Probability')\n",
    "plt.title('Summed Probability Analysis of Roman Inscriptions')\n",
    "\n",
    "# set axis limits and grid\n",
    "plt.xlim([-100, 600])\n",
    "plt.ylim([0, 4000])\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(-100, 600, 50), rotation='vertical')  # Sets X-axis tick interval; change 50 to the desired interval\n",
    "plt.yticks(np.arange(0, 4000, 250))  # Sets Y-axis tick interval; change 250 to the desired interval\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c961e2f-1cce-416a-ad89-922d166a283b",
   "metadata": {},
   "source": [
    "### Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6bfa5-291a-4fb8-afc5-314441c6fa21",
   "metadata": {},
   "source": [
    "Looking at this overview plot:\n",
    "\n",
    "* Several of the peaks seem to correspond to dynasty changes.\n",
    "* Start date should probably be 1 AD\n",
    "* End date should probably be 400 AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871114c8-b57b-4def-9256-b6ed7085f468",
   "metadata": {},
   "source": [
    "# Minimum required sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057433d1",
   "metadata": {},
   "source": [
    "SPA assumes that a large enough regional sample will yield a quasi-random sample despite the fact that it consists of small non-systematic samples. It is thus crucial to determine minimum sample size for robust and reproducible SPA (Williams 2012, 580). Reliable Summed Probability Distributions (SPDs) have statistical fluxuation (S<sub>f</sub>) of <20% (Michczynska and Pazdur 2004, 734). Combining the span of the time series with the mean of the standard deviations of date ranges (mean uncertainty; &Delta;T) can indicate the minimum sample size to meet this statistical fluxuation threshold (Michcynska 2007, 800; Michczynska and Pazdur 2004, figure 2; Williams 2012, 580). Literature review shows a wide range of sample sizes used in radiocarbon studies from less than 100 to over 25,000 (Williams 2012 580).\n",
    "\n",
    "Below, I first attempt to replicate the minimum sample size calculations of Michczynska and Pazdur (2004, 580-581), unfortunately with little success (I need to consult a statistician as to why).\n",
    "\n",
    "Then I go on replicate the calculations of Williams 2012 (580-581) with more success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe0727",
   "metadata": {},
   "source": [
    "## MSSD between full dataset and samples of various sizes (replication of Michczynska and Pazdur 2004, 736-737)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805294fb",
   "metadata": {},
   "source": [
    "Michczynska and Pazdur generate a random dataset as the reference, but here I use the actual dataset, then follow the same sampling and analysis procedure:\n",
    "\n",
    "1. Create 1000 insription dates with the same error mean and SD, then calculate a SPD for the dataset\n",
    "2. Randomly sample N<sub>i</sub> dates from the dataset\n",
    "3. Calculate SPD<sub>i</sub> for the seleccted N<sub>i</sub> dates\n",
    "4. Calculate the sum of squared deviation between the original SPD and the secondary SPD<sub>i</sub>\n",
    "5. Repeat steps 2-4 100 times and calculate the mean sum of squared deviations (MSSD)\n",
    "6. Repeat steps 2-6 for N<sub>i</sub> = 50, 100, 150,...1000 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the parameters for the random dataset\n",
    "num_dates = 1000  # The number of dates to generate\n",
    "mean_date_range = 100  # The mean date range for the dates\n",
    "std_dev_date_range = 30  # The standard deviation of the date range for the dates\n",
    "start_year = -100  # The earliest year for the dates\n",
    "end_year = 500  # The latest year for the dates\n",
    "\n",
    "# Generate the dates evenly distributed between the start year and the end year\n",
    "dates = np.linspace(start_year, end_year, num_dates)\n",
    "\n",
    "# Generate the date ranges from a normal distribution with the specified mean and standard deviation\n",
    "date_ranges = np.random.normal(mean_date_range, std_dev_date_range, num_dates)\n",
    "\n",
    "# Calculate the 'not_before' and 'not_after' dates by subtracting and adding half the date range to the date, respectively\n",
    "not_before = dates - date_ranges / 2\n",
    "not_after = dates + date_ranges / 2\n",
    "\n",
    "# Create a DataFrame with the generated dates and uncertainties\n",
    "random_dates_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'not_before': not_before,\n",
    "    'not_after': not_after\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "random_dates_df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the parameters for the SPD calculation\n",
    "earliest_date = start_year  # The earliest year for the SPD\n",
    "latest_date = end_year  # The latest year for the SPD\n",
    "resolution = 1  # The resolution for the SPD\n",
    "num_bins = int((latest_date - earliest_date) / resolution) + 1  # The number of bins for the SPD\n",
    "\n",
    "# Initialize the summed probability distribution as an array of zeros\n",
    "summed_prob = np.zeros(num_bins)\n",
    "\n",
    "# Loop through each date in the DataFrame to update the summed probability distribution\n",
    "for index, row in random_dates_df.iterrows():\n",
    "    start = row['not_before']\n",
    "    end = row['not_after']\n",
    "    \n",
    "    # Calculate the indices for the start and end years in the SPD\n",
    "    start_idx = int((start - earliest_date) / resolution)\n",
    "    end_idx = int((end - earliest_date) / resolution)\n",
    "   \n",
    "    # Calculate a uniform distribution for this date, accounting for date combinations that add up to zero\n",
    "    try:\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1)\n",
    "    except ZeroDivisionError:\n",
    "        uniform_prob = 1  # If the start and end years are the same, set the uniform probability to 1\n",
    "\n",
    "    # Add this distribution to the overall summed distribution\n",
    "    summed_prob[start_idx : end_idx + 1] += uniform_prob\n",
    "\n",
    "# Plot the summed probability distribution\n",
    "plt.plot(range(earliest_date, latest_date + 1), summed_prob)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Summed Probability')\n",
    "plt.title('Summed Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50278341",
   "metadata": {},
   "source": [
    "Useful plot, indicates that the first 100 years and last 50+ years are not meaningful, need to trim the results (but not the input data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a list to store the sample sizes\n",
    "sample_sizes = list(range(50, 1000, 50))\n",
    "\n",
    "# Initialize a list to store the MSSDs\n",
    "mssds = []\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "    # Initialize a list to store the SSDs\n",
    "    ssds = []\n",
    "\n",
    "    # Repeat the process 100 times\n",
    "    for _ in range(100):\n",
    "        # Take a random sample of the given size from the random_dates_df dataset\n",
    "        sample = random_dates_df.sample(sample_size)\n",
    "\n",
    "        # Initialize the summed probability distribution for the sample\n",
    "        sample_summed_prob = np.zeros(num_bins)\n",
    "\n",
    "        # Loop through each date in the sample to update the summed probability distribution\n",
    "        for index, row in sample.iterrows():\n",
    "            start = row['not_before']\n",
    "            end = row['not_after']\n",
    "            \n",
    "            start_idx = int((start - earliest_date) / resolution)\n",
    "            end_idx = int((end - earliest_date) / resolution)\n",
    "           \n",
    "            # Calculate uniform distribution for this date,\n",
    "            # accounting for date combinations that add up to zero\n",
    "            try:\n",
    "                uniform_prob = 1.0 / (end_idx - start_idx + 1)\n",
    "            except ZeroDivisionError:\n",
    "                uniform_prob = 1  # set default value here\n",
    "\n",
    "            # Add this distribution to the overall summed distribution\n",
    "            sample_summed_prob[start_idx : end_idx + 1] += uniform_prob\n",
    "\n",
    "        # Calculate the sum of squared deviation between the original SPD and the new SPD\n",
    "        ssd = np.sum((summed_prob - sample_summed_prob) ** 2)\n",
    "        \n",
    "        # Add the SSD to the list\n",
    "        ssds.append(ssd)\n",
    "\n",
    "    # Calculate the mean sum of squared deviations (MSSD)\n",
    "    mssd = np.mean(ssds)\n",
    "    \n",
    "    # Add the MSSD to the list\n",
    "    mssds.append(mssd)\n",
    "\n",
    "# Plot the MSSDs versus the sample sizes\n",
    "plt.plot(sample_sizes, mssds)\n",
    "plt.scatter(sample_sizes, mssds)  # Add points for each sample size data point\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('MSSD')\n",
    "plt.title('MSSD versus Sample Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae636bc4",
   "metadata": {},
   "source": [
    "### Discussion: MSSDs as per Michczynska and Pazdur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934aa482",
   "metadata": {},
   "source": [
    "I was able to replicate the approach of Machczynska and Pazdur, but the results are unexpected. The curve should have dropped quickly (e.g., in their article, it was nearly flat after a sample size of 200), but it does not. I am not sure if this difference is because of:\n",
    "\n",
    "* The relatively large mean error versus the date range (mean date-range is 100 years with a mean of date-range standard deviations of 30 years across a 600-year date range; in M&P, they have a 14k date range with a 120-year mean of date-range standard deviations)\n",
    "* The fact that the error distribuiton is uniform rather than gaussian, as it is for radiocarbon dates.\n",
    "\n",
    "On the other hand, seeing a randomly generated SPD was useful - if nothing else in indicates how much of the beginning and end of the series needs to be trimmed (about 100 years)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed30d9",
   "metadata": {},
   "source": [
    "## Calculate range of statistical fluctuation (S<sub>f</sub>) for inscription SPDs (as per Michdzynska and Pazdur 2004, 734-735)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b25a771",
   "metadata": {},
   "source": [
    "This estimate of minimum sample size is based on a monte carlo simulation involving various sample sizes but with a fixed mean of the date-range standard deviations (30).\n",
    "\n",
    "1. Assume inscription dates are uniform from -100 to +500 years\n",
    "2. Set mean date range to 100 years\n",
    "3. Set mean of date-range standard deviations to 30 years\n",
    "4. Generate 1000 SPDs for each sample size\n",
    "5. Estimate fluctuation range as 95% fonfidence intervals\n",
    "6. Calculate statistical fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d817348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set the parameters for the random dataset\n",
    "num_dates = 10000  # The maximum number of dates to generate\n",
    "mean_date_range = 100  # The mean date range for the dates\n",
    "std_dev_date_range = 30  # The standard deviation of the date range for the dates\n",
    "start_year = 0  # The earliest year for the dates\n",
    "end_year = 600  # The latest year for the dates\n",
    "\n",
    "# Generate the dates evenly distributed between the start year and the end year\n",
    "dates = np.linspace(start_year, end_year, num_dates)\n",
    "\n",
    "# Generate the date ranges from a normal distribution with the specified mean and standard deviation\n",
    "date_ranges = np.random.normal(mean_date_range, std_dev_date_range, num_dates)\n",
    "\n",
    "# Calculate the 'not_before' and 'not_after' dates by subtracting and adding half the date range to the date, respectively\n",
    "not_before = dates - date_ranges / 2\n",
    "not_after = dates + date_ranges / 2\n",
    "\n",
    "# Create a DataFrame with the generated dates and uncertainties\n",
    "random_dates_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'not_before': not_before,\n",
    "    'not_after': not_after\n",
    "})\n",
    "\n",
    "# Define the sample sizes\n",
    "sample_sizes = [50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Define the number of SPDs to generate for each sample size\n",
    "num_spds = 1000\n",
    "\n",
    "# Initialize a dictionary to store the SPDs and their 95% confidence intervals\n",
    "spds_and_cis = {}\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "    # Initialize a list to store the SPDs\n",
    "    spds = []\n",
    "\n",
    "    # Generate the SPDs\n",
    "    for _ in range(num_spds):\n",
    "        # Take a random sample of the given size from the random_dates_df dataset\n",
    "        sample = random_dates_df.sample(sample_size)\n",
    "\n",
    "        # Generate the SPD for the sample\n",
    "        spd = np.histogram(sample['date'], bins=np.arange(start_year, end_year + 1), density=True)[0]\n",
    "\n",
    "        # Add the SPD to the list\n",
    "        spds.append(spd)\n",
    "\n",
    "    # Calculate the 95% confidence intervals for the SPDs\n",
    "    ci_lower = np.percentile(spds, 2.5, axis=0)\n",
    "    ci_upper = np.percentile(spds, 97.5, axis=0)\n",
    "\n",
    "    # Store the SPDs and their 95% confidence intervals in the dictionary\n",
    "    spds_and_cis[sample_size] = {\n",
    "        'spds': spds,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper\n",
    "    }\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store the sample sizes and the limits of the 95% confidence intervals\n",
    "sample_sizes = []\n",
    "ci_lowers = []\n",
    "ci_uppers = []\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size, data in spds_and_cis.items():\n",
    "    # Add the sample size to the list\n",
    "    sample_sizes.append(sample_size)\n",
    "\n",
    "    # Add the lower and upper limits of the 95% confidence interval to the respective lists\n",
    "    ci_lowers.append(data['ci_lower'].mean())\n",
    "    ci_uppers.append(data['ci_upper'].mean())\n",
    "\n",
    "# Create a scatter plot of the lower limits of the 95% confidence intervals against the sample sizes\n",
    "plt.scatter(sample_sizes, ci_lowers, color='blue')\n",
    "plt.plot(sample_sizes, ci_lowers, color='blue', label='Lower Limit')\n",
    "\n",
    "# Create a scatter plot of the upper limits of the 95% confidence intervals against the sample sizes\n",
    "plt.scatter(sample_sizes, ci_uppers, color='red')\n",
    "plt.plot(sample_sizes, ci_uppers, color='red', label='Upper Limit')\n",
    "\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('95% Confidence Interval Limit')\n",
    "plt.title('95% Confidence Interval Limits vs Sample Size')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b5397",
   "metadata": {},
   "source": [
    "Results look plausible compared to Michczynska and Pazdur (2004, figure 1), noting that I am only modelling a 30-year mean of date-range standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda3371",
   "metadata": {},
   "source": [
    "Now attempting to replicate figure 2, calculating Statistical fluctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random dataset generated above ('dates')\n",
    "\n",
    "# Initialize lists to store the sample sizes and the statistical fluctuations\n",
    "sample_sizes = []\n",
    "statistical_fluctuations = []\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size, data in spds_and_cis.items():\n",
    "    # Calculate the mean and standard deviation of the SPDs\n",
    "    mean_spd = np.mean(data['spds'])\n",
    "    std_dev_spd = np.std(data['spds'])\n",
    "\n",
    "    # Calculate the statistical fluctuation\n",
    "    statistical_fluctuation = std_dev_spd / mean_spd * 100\n",
    "\n",
    "    # Print the values\n",
    "    print(f'Sample size: {sample_size}, Mean SPD: {mean_spd}, Std Dev SPD: {std_dev_spd}, Statistical Fluctuation: {statistical_fluctuation}%')\n",
    "\n",
    "    # Add the sample size and statistical fluctuation to the lists\n",
    "    sample_sizes.append(sample_size)\n",
    "    statistical_fluctuations.append(statistical_fluctuation)\n",
    "# Create a scatter plot of the statistical fluctuations against the sample sizes\n",
    "plt.scatter(sample_sizes, statistical_fluctuations, color='green')\n",
    "plt.plot(sample_sizes, statistical_fluctuations, color='green')\n",
    "\n",
    "plt.xlabel('Number of Dates')\n",
    "plt.ylabel('Statistical Fluctuations (%)')\n",
    "plt.title('Statistical Fluctuations vs Number of Dates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90689ba5",
   "metadata": {},
   "source": [
    "Not working. Copilot interpreted 'statistical fluctuation' as the standard deviation divided by the mean, and the shape of the curve is correct, but that produces results dissimilar from Michczynska and Pazdur (where statistical fluctuation was never more than about 70% even with a small sample). It's not clear from M&P what parameters of the SPD they are calculating on, or if they are using the 'SD/mean' definition of S<sub>f</sub>. I am not sure what is wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f365f39-92d0-4339-8242-bec708adec4e",
   "metadata": {},
   "source": [
    "## Calculate &Delta;T: mean of date-range standard deviations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4581591",
   "metadata": {},
   "source": [
    "I think I misunderstood something here; Williams 2012 doesn't do his calculations on &Delta;T, he argues that they should be reported, and references work by Michczynska and Pazdur (2004) to characterise necessary sample sizes given a particular &Delta;T. The calculations below are useful in (a) indicating the &Delta;T of the entire dataset (29.24) and the &Delta;T of random samples with various maximum date-ranges.\n",
    "\n",
    "This work should be expanded to include bootstrapping for calculating &Delta;T for various date-range thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca26a7-ed73-4ab8-9d84-fa8c1f17768d",
   "metadata": {},
   "source": [
    "### Full dataset &Delta;T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2be0a-86b4-4757-b36d-77c338e13466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the date-range standard deviations for each record\n",
    "lire['date_range_std_dev'] = (lire['not_after'] - lire['not_before']) / np.sqrt(12)  # Assuming uniform distribution\n",
    "\n",
    "# Calculate the mean of these standard deviations\n",
    "mean_std_dev = lire['date_range_std_dev'].mean()\n",
    "\n",
    "# Count the number of records in the dataset\n",
    "num_records = len(lire)\n",
    "\n",
    "print(f\"The number of records in the dataset is: {num_records}\")\n",
    "print(f\"The mean of the date-range standard deviations is: {mean_std_dev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497892b-d070-47ff-bcec-c036dc13a286",
   "metadata": {},
   "source": [
    "### &Delta;T for various date-range thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range thresholds for filtering\n",
    "thresholds = [25, 50, 100, 200, 300, 600]\n",
    "\n",
    "# Initialize lists to store the means and thresholds\n",
    "means = []\n",
    "thresholds_for_plot = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Filter the data for records with a date range less than or equal to the current threshold\n",
    "    filtered_lire = lire[lire['date_range'] <= threshold].copy()  # Create a copy of the slice to avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Calculate the date-range standard deviations for each record in the filtered DataFrame\n",
    "    filtered_lire.loc[:, 'date_range_std_dev'] = (filtered_lire['not_after'] - filtered_lire['not_before']) / np.sqrt(12)\n",
    "    \n",
    "    # Calculate the mean of these standard deviations in the filtered DataFrame\n",
    "    mean_std_dev_filtered = filtered_lire['date_range_std_dev'].mean()\n",
    "    \n",
    "    # Count the number of records in the filtered DataFrame\n",
    "    num_records_filtered = len(filtered_lire)\n",
    "    \n",
    "    print(f\"For date range cut-off less than or equal to {threshold} years:\")\n",
    "    print(f\"The mean of the date-range standard deviations is: {mean_std_dev_filtered}\")\n",
    "    print(f\"The number of records in the filtered dataset is: {num_records_filtered}\")\n",
    "    print('-'*50)\n",
    "    \n",
    "    # Append the mean and threshold to the respective lists\n",
    "    means.append(mean_std_dev_filtered)\n",
    "    thresholds_for_plot.append(threshold)\n",
    "\n",
    "# Plot the means against the thresholds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds_for_plot, means, marker='o')\n",
    "# plt.gca().invert_xaxis()  # Reverse the x-axis\n",
    "plt.xlabel('Date Range Cut-Off (years)', fontsize=14)\n",
    "plt.ylabel('Mean Date-Range Standard Deviation', fontsize=14)\n",
    "plt.title('Mean Date-Range Standard Deviation vs. Date Range Cut-Off', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224396d",
   "metadata": {},
   "source": [
    "## Calculate mean date-range standard deviation and standard error (SE) of the mean standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a0212",
   "metadata": {},
   "source": [
    "Use bootstrapping to calculate 95% confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define sample sizes and number of bootstrap samples\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "num_bootstrap_samples = 1000\n",
    "\n",
    "# Initialize lists to store the means, standard errors, and sample sizes\n",
    "means = []\n",
    "standard_errors = []\n",
    "sizes_for_plot = []\n",
    "\n",
    "# Calculate the date-range standard deviations for the full dataset\n",
    "lire.loc[:, 'date_range_std_dev'] = (lire['not_after'] - lire['not_before']) / np.sqrt(12)\n",
    "\n",
    "# Calculate the mean and standard error of these standard deviations in the full dataset\n",
    "mean_std_dev_full = lire['date_range_std_dev'].mean()\n",
    "\n",
    "for size in sample_sizes:\n",
    "    # Initialize a list to store the bootstrap sample means\n",
    "    bootstrap_means = []\n",
    "    \n",
    "    for _ in range(num_bootstrap_samples):\n",
    "        # Take a random sample of the specified size from the LIRE dataset\n",
    "        sample_lire = lire.sample(n=size, replace=True)\n",
    "        \n",
    "        # Calculate the date-range standard deviations for each record in the sample\n",
    "        sample_lire.loc[:, 'date_range_std_dev'] = (sample_lire['not_after'] - sample_lire['not_before']) / np.sqrt(12)\n",
    "        \n",
    "        # Calculate the mean of these standard deviations in the sample\n",
    "        mean_std_dev_sample = sample_lire['date_range_std_dev'].mean()\n",
    "        \n",
    "        # Append the mean to the list of bootstrap sample means\n",
    "        bootstrap_means.append(mean_std_dev_sample)\n",
    "    \n",
    "    # Calculate the mean and standard error of the bootstrap sample means\n",
    "    mean_bootstrap_means = np.mean(bootstrap_means)\n",
    "    se_bootstrap_means = np.std(bootstrap_means)\n",
    "    \n",
    "    print(f\"For sample size of {size}:\")\n",
    "    print(f\"The mean of the date-range standard deviations is: {mean_bootstrap_means}\")\n",
    "    print(f\"The standard error of the mean is: {se_bootstrap_means}\")\n",
    "    print('-'*50)\n",
    "    \n",
    "    # Append the mean, standard error, and sample size to the respective lists\n",
    "    means.append(mean_bootstrap_means)\n",
    "    standard_errors.append(se_bootstrap_means)\n",
    "    sizes_for_plot.append(size)\n",
    "\n",
    "# Calculate the 95% confidence intervals\n",
    "lower_bound = np.array(means) - 1.96 * np.array(standard_errors)\n",
    "upper_bound = np.array(means) + 1.96 * np.array(standard_errors)\n",
    "\n",
    "# Plot the means with 95% confidence intervals against the sample sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(sizes_for_plot, means, yerr=[means - lower_bound, upper_bound - means], fmt='o', color='b', ecolor='r', capsize=5)\n",
    "plt.axhline(y=mean_std_dev_full, color='r', linestyle='-')\n",
    "plt.xlabel('Sample Size', fontsize=14)\n",
    "plt.ylabel('Mean Date-Range Standard Deviation', fontsize=14)\n",
    "plt.title('Mean Date-Range Standard Deviation vs. Sample Size with 95% Confidence Intervals', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the standard errors against the sample sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes_for_plot, standard_errors, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Sample Size', fontsize=14)\n",
    "plt.ylabel('Standard Error of the Mean', fontsize=14)\n",
    "plt.title('Standard Error of the Mean vs. Sample Size', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155e94e",
   "metadata": {},
   "source": [
    "#### Dataset &Delta;T discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b094a37",
   "metadata": {},
   "source": [
    "&Delta;T (uncertainty) for the entire dataset is just under 30 years (29.24). This uncertainty is smaller in magnatude to the error assoicated with radiocarbon dates. Michczynska and Pazdur (2004, 734), for example, chose a 120-year &Delta;T when generating synthetic data to determine minimum dataset size required for SPA. All else being equial, this smaller uncertainty indicates that smaller datasets will be required for meaningful SPA results. Note, however, that Michczynska and Pazdur were modelling dates distributed over a 14,000 year period, whereas the LIRE dataset spans only 600 years, so our uncertainty is a much larger percentage of the total span (ca. 5.0% vs. 0.86%). \n",
    "\n",
    "&Delta;t is much lower for subsets of the data with narrower date ranges (as expected).\n",
    "\n",
    "Finally, using bootstrapping to repeatedly subsample the datasate indicates that the Standard Error of &Delta;T decreases rapidly with sample size. \n",
    "\n",
    "These plots are intended to illustrate trade-offs between (a) restricting the dates used to those with narrower uncertainties (yielding smaller samples) versus (b) the fact that the larger samples have smaller errors. \n",
    "\n",
    "*To do: fix formatting to match other graphs*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980da0f-8e65-4d54-9bab-fb72d11b1e4e",
   "metadata": {},
   "source": [
    "## SE of the date-range mean via bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ec2b7-588d-405f-aa6a-cca4ba39a240",
   "metadata": {},
   "source": [
    "None of my attempts to calculate MSE according to Willaims 2012 worked. Instead used the approach described at https://ealizadeh.com/blog/statistics-data-vs-sampling-distribution/. First compared the mean, standard deviation, and SE of date ranges in the sample to the mean of date ranges in the population, then plotted the SE of the mean date-range against the sample size.\n",
    "\n",
    "To do: needs more write-up of the analysis, referring back to the source linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fbee5-7df2-4941-9f9f-29e7f158dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming lire is your DataFrame\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "n_iterations = 1000\n",
    "\n",
    "# Sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Initialize empty list to store standard error values for plotting\n",
    "standard_errors = []\n",
    "\n",
    "# New list to store the means for each sample size\n",
    "sample_means = []\n",
    "\n",
    "# Initialize empty dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Calculate mean and standard deviation of full dataset for comparison\n",
    "mean_date_range_full = lire['date_range'].mean()\n",
    "std_date_range_full = lire['date_range'].std()\n",
    "\n",
    "# Make sure the directory for saving the bootstrap samples exists\n",
    "if not os.path.exists(\"bootstrap-samples/standard-error/means_sd\"):\n",
    "    os.makedirs(\"bootstrap-samples/standard-error/means_sd\", exist_ok=True)\n",
    "    \n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "    \n",
    "    # Create a dynamic file name\n",
    "    saved_sample_path = f\"bootstrap-samples/standard-error/means_sd/n_{sample_size}.npy\"\n",
    "    \n",
    "    if os.path.exists(saved_sample_path): # Check if saved bootstrap sample exists\n",
    "        # Load saved sample\n",
    "        print(\"Loading saved bootstrap sample...\")\n",
    "        saved_data = np.load(saved_sample_path, allow_pickle=True).item()\n",
    "        bootstrap_sample_means = saved_data['means']\n",
    "        bootstrap_sample_stds = saved_data['stds']\n",
    "    else:\n",
    "        # Create bootstrap sample\n",
    "        print(f\"Performing bootstrap operation for sample size {sample_size}...\")\n",
    "        bootstrap_sample_means = []\n",
    "        bootstrap_sample_stds = []  \n",
    "    \n",
    "        # Bootstrap sampling\n",
    "        for i in range(n_iterations):\n",
    "            # Sample with replacement from the dataset\n",
    "            bootstrap_sample = lire['date_range'].sample(n=sample_size, replace=True)\n",
    "            # Calculate the mean of the bootstrap sample\n",
    "            bootstrap_sample_mean = bootstrap_sample.mean()\n",
    "            # Calculate the standard deviation of the bootstrap sample\n",
    "            bootstrap_sample_std = bootstrap_sample.std()\n",
    "            # Store the mean and standard deviation in their respective lists\n",
    "            bootstrap_sample_means.append(bootstrap_sample_mean)\n",
    "            bootstrap_sample_stds.append(bootstrap_sample_std)\n",
    "            \n",
    "        # Save the sample means and standard deviations\n",
    "        print(\"Saving bootstrap sample means and standard deviations...\")\n",
    "        saved_data = {\n",
    "            'means': bootstrap_sample_means,\n",
    "            'stds': bootstrap_sample_stds\n",
    "        }\n",
    "        np.save(saved_sample_path, saved_data)\n",
    "    \n",
    "    # Calculate the standard error for the sample mean\n",
    "    standard_error = np.std(bootstrap_sample_means)\n",
    "    \n",
    "    # Store the standard error for plotting\n",
    "    standard_errors.append(standard_error)\n",
    "    \n",
    "    # Store the sample means for plotting\n",
    "    sample_means.append(np.mean(bootstrap_sample_means))\n",
    "\n",
    "    # Calculate the mean and standard deviation for the bootstrap samples\n",
    "    mean_bootstrap_samples = np.mean(bootstrap_sample_means)\n",
    "    std_bootstrap_samples = np.mean(bootstrap_sample_stds)\n",
    "\n",
    "    # Store results in the dictionary\n",
    "    results[sample_size] = {\n",
    "        'mean_bootstrap_samples': mean_bootstrap_samples,\n",
    "        'std_bootstrap_samples': std_bootstrap_samples,\n",
    "        'standard_error': standard_error\n",
    "    }\n",
    "\n",
    "print(f\"Mean date range for full dataset: {mean_date_range_full}\")\n",
    "print(f\"Standard Deviation of date range for full dataset: {std_date_range_full}\")\n",
    "\n",
    "# Display the results\n",
    "for sample_size, metrics in results.items():\n",
    "    print(f\"\\nFor a sample size of {sample_size}:\")\n",
    "    print(f\"  Mean date range for bootstrap samples: {metrics['mean_bootstrap_samples']}\")\n",
    "    print(f\"  Standard Deviation of date range for bootstrap samples: {metrics['std_bootstrap_samples']}\")\n",
    "    print(f\"  Standard Error of the mean date range: {metrics['standard_error']}\")\n",
    "\n",
    "# Plotting the standard errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, standard_errors, marker='o')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Standard Error of the Mean Date Range')\n",
    "plt.title('Standard Error vs. Sample Size')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the means with confidence interval\n",
    "\n",
    "# Calculate the 95% confidence intervals\n",
    "lower_bound = np.array(sample_means) - 1.96 * np.array(standard_errors)\n",
    "upper_bound = np.array(sample_means) + 1.96 * np.array(standard_errors)\n",
    "\n",
    "# Plotting the mean and 95% CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(sample_sizes, sample_means, yerr=[sample_means - lower_bound, upper_bound - sample_means], fmt='o', label='Sample Means with 95% CI')\n",
    "plt.axhline(y=mean_date_range_full, color='r', linestyle='-', label='Full Dataset Mean') \n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Mean Date Range')\n",
    "plt.title('Mean Date Range with 95% Confidence Intervals for Various Sample Sizes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b9cae-398b-4a03-a774-76a75915df92",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test of 'date_range' subsets\n",
    "\n",
    "Another view on minimum sample size for SPA\n",
    "\n",
    "The D-statistic is the maximum difference between the CDFs of the two samples, and the P-value is the significance level. A smaller P-value would indicate that it is unlikely that the sample and the dataset come from the same distribution, assuming the null hypothesis is true. This code calculates a D-statistic for date_range. In other words, it compares the difference in CDFs between each sample size (25, 50, 100, 150, 500, 1000, 2000 records) and the dataset as a whole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3afc33-e245-4eff-8b3e-64582cce21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming lire is your DataFrame\n",
    "\n",
    "# Sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Create a dictionary to hold Kolmogorov-Smirnov test results for each sample size\n",
    "ks_results = {}\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    # Sample without replacement from the dataset\n",
    "    sample = lire['date_range'].sample(n=sample_size)\n",
    "    \n",
    "    # Perform the Kolmogorov-Smirnov test\n",
    "    d_stat, p_value = stats.ks_2samp(sample, lire['date_range'])\n",
    "    \n",
    "    # Store the result in the dictionary\n",
    "    ks_results[sample_size] = {\n",
    "        'D-statistic': d_stat,\n",
    "        'P-value': p_value\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for sample_size, metrics in ks_results.items():\n",
    "    print(f\"\\nFor a sample size of {sample_size}:\")\n",
    "    print(f\"  D-statistic: {metrics['D-statistic']}\")\n",
    "    print(f\"  P-value: {metrics['P-value']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e48a5-79d8-4ff2-b018-7b27db93812c",
   "metadata": {},
   "source": [
    "#### K-S with bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc712a",
   "metadata": {},
   "source": [
    "This section needs to be redone. First, it should only deal with one variable at a time, to avoid complex save / load / calculation issues. Focus on date_range as above, but add bootstrapping to get 95% confidence intervals. \n",
    "\n",
    "In fact, rethink and start over. It might make more sense to do this using summed probability values per five-year bin. Or start with date-range, then try the SP values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ec7db-cb78-45f5-9594-7e0d8b5b8ea9",
   "metadata": {},
   "source": [
    "### Standard Error (SE) of the date-range summed probability values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671f886-f507-4029-855d-53263fe74342",
   "metadata": {},
   "source": [
    "To-do: this appraoch appears incorrect; looking at Williams 2012, 580-582, especially figure 4, it looks like the relevant statistic is Summed Mean Square Error, not Mean Standard Error. Also, summed probability versus date-range doesn't make any sense, it should be versus date bin in the SPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9ef16-c36c-4ecc-818a-03271a0facfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming lire is your DataFrame\n",
    "# lire = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "n_iterations = 1000\n",
    "\n",
    "# Sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Create 5-year bins for full dataset\n",
    "bins = np.arange(lire['date_range'].min(), lire['date_range'].max() + 5, 5)\n",
    "full_data_summed_prob = np.histogram(lire['date_range'], bins=bins, density=True)[0]\n",
    "\n",
    "# Initialize a dictionary to hold results for each sample size\n",
    "results = {}\n",
    "\n",
    "# Initialize empty list to store standard error values for plotting\n",
    "standard_errors = []\n",
    "\n",
    "# Make sure the directory for saving the bootstrap samples exists\n",
    "if not os.path.exists(\"bootstrap-samples/standard-error/summed-probabilities\"):\n",
    "    os.makedirs(\"bootstrap-samples/standard-error/summed-probabilities\", exist_ok=True)\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    # Create a dynamic file name\n",
    "    saved_sample_path = f\"bootstrap-samples/standard-error/summed-probabilities/n_{sample_size}.npy\"\n",
    "    \n",
    "    if os.path.exists(saved_sample_path): # Check if saved bootstrap sample exists\n",
    "        print(\"Loading saved bootstrap sample...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path, allow_pickle=True)\n",
    "    else:\n",
    "        # Initialize a list to hold the summed probabilities for each bootstrap sample\n",
    "        bootstrap_summed_probs = []\n",
    "    \n",
    "        # Bootstrap sampling\n",
    "        for i in range(n_iterations):\n",
    "            # Sample with replacement from the dataset\n",
    "            bootstrap_sample = lire['date_range'].sample(n=sample_size, replace=True)\n",
    "            \n",
    "            # Calculate the summed probability for this bootstrap sample\n",
    "            bootstrap_summed_prob = np.histogram(bootstrap_sample, bins=bins, density=True)[0]\n",
    "            \n",
    "            # Store in the list\n",
    "            bootstrap_summed_probs.append(bootstrap_summed_prob)\n",
    "\n",
    "        # Save the sample\n",
    "        print(\"Saving bootstrap sample...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "\n",
    "    # Convert to NumPy array for easier calculations\n",
    "    bootstrap_summed_probs = np.array(bootstrap_summed_probs)\n",
    "    \n",
    "    # Calculate the standard error for each bin\n",
    "    standard_errors_for_bins = np.std(bootstrap_summed_probs, axis=0)\n",
    "    \n",
    "    # Calculate the mean standard error for this sample size\n",
    "    mean_standard_error = np.mean(standard_errors_for_bins)\n",
    "    \n",
    "    # Store the standard error for plotting\n",
    "    standard_errors.append(mean_standard_error)\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    results[sample_size] = {\n",
    "        'standard_errors_for_bins': standard_errors_for_bins,\n",
    "        'mean_standard_error': mean_standard_error,\n",
    "        'bootstrap_summed_probs': bootstrap_summed_probs  # <-- Add this line\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for sample_size, metrics in results.items():\n",
    "    print(f\"\\nFor a sample size of {sample_size}:\")\n",
    "    print(f\"  Mean Standard Error for Summed Probabilities: {metrics['mean_standard_error']}\")\n",
    "\n",
    "# Plotting the standard errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, standard_errors, marker='o')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Mean Standard Error of Summed Probability')\n",
    "plt.title('Standard Error vs. Sample Size')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaca631-eb56-44f0-8b18-d664dbbffc6e",
   "metadata": {},
   "source": [
    "#### Discussion of MSE of SP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f2622-45e9-4431-8948-c8cc11201124",
   "metadata": {},
   "source": [
    "To-do: write discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a66bf-a7bf-41ff-980e-988961cd2ad1",
   "metadata": {},
   "source": [
    "# Summed Probability Analysis of entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f7560-fd01-4bba-bda0-c7ecca9dd4a1",
   "metadata": {},
   "source": [
    "## Various date-ranges and sample sizes; error based on date-range SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e27de-195f-4ddb-8e68-29b524f4bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# Filter your DataFrame to only include records with a date range <= thresholds\n",
    "\n",
    "# Define date range thresholds for filtering; note that calculations are now *below* rather than *between* thresholds\n",
    "thresholds = [(0, 0), (0, 10), (0, 25), (0, 50), (0, 100), (0, 200), (0, 300), (0, float('inf'))]\n",
    "\n",
    "# Define overall earliest and latest date in the dataset, and bin resolution\n",
    "earliest_date, latest_date = filtered_lire['not_before'].min(), filtered_lire['not_after'].max()\n",
    "resolution = 5\n",
    "\n",
    "# Define sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Add special value to signify using all records\n",
    "sample_sizes.append('all')\n",
    "\n",
    "# Loop through each date range threshold\n",
    "for start, end in thresholds:\n",
    "    \n",
    "    # Filter DataFrame based on the current threshold\n",
    "    if end == float('inf'):\n",
    "        subset_df = filtered_lire[filtered_lire['date_range'] >= start]\n",
    "        label = f\"{start}+\"\n",
    "    elif start == 0 and end == 0:\n",
    "        subset_df = filtered_lire[filtered_lire['date_range'] == 0]\n",
    "        label = f\"{start}\"\n",
    "    else:\n",
    "        subset_df = filtered_lire[(filtered_lire['date_range'] >= start) & (filtered_lire['date_range'] <= end)]\n",
    "        label = f\"{start}-{end}\"\n",
    "    \n",
    "    # Loop through each sample size\n",
    "    for sample_size in sample_sizes:\n",
    "\n",
    "         # Special case to use all records in filtered DataFrame\n",
    "        if sample_size == 'all':\n",
    "            subsample = filtered_lire\n",
    "            size_label = \"all\"\n",
    "        # Or, create a random subsample from the filtered DataFrame\n",
    "        else:\n",
    "            subsample = subset_df.sample(n=min(sample_size, len(subset_df)), random_state=42)\n",
    "            size_label = str(sample_size)\n",
    "\n",
    "        # Calculate the summed probability and standard deviations for the subsample\n",
    "        summed_prob, std_devs = compute_summed_probability(subsample, earliest_date, latest_date, resolution=5)\n",
    "        \n",
    "        # Generate x-values for plotting, making sure they align in shape with summed_prob\n",
    "        x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "        \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        \n",
    "        # Plot the summed probability distribution\n",
    "        plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "        \n",
    "        # Calculate and plot the moving average\n",
    "        window_size = 5\n",
    "        moving_avg = np.convolve(summed_prob, np.ones(window_size)/window_size, mode='valid')\n",
    "        \n",
    "        # Calculate and plot the error margins\n",
    "        moving_std_dev = np.convolve(std_devs, np.ones(window_size)/window_size, mode='valid')\n",
    "        \n",
    "        # Generate x-values for the moving average and error margins\n",
    "        moving_avg_x = x_values[int(window_size/2):-int(window_size/2)+1][:len(moving_avg)]\n",
    "        \n",
    "        # Plot the moving average and fill between for the error margin\n",
    "        plt.plot(moving_avg_x, moving_avg, color='red', label=f\"{window_size}-bin Moving Average\")\n",
    "        plt.fill_between(moving_avg_x, moving_avg - moving_std_dev, moving_avg + moving_std_dev, color='red', alpha=0.2, label='Error Margin')\n",
    "        \n",
    "        plt.title(f\"Summed Probability Distribution for Date Range {label}, Sample Size = {size_label}\")\n",
    "        plt.xlabel('Date')\n",
    "        plt.xlim([-100, 600])\n",
    "        plt.ylabel('Summed Probability')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab588f-234d-4ba0-a6b0-9d94735f3602",
   "metadata": {},
   "source": [
    "## Various date-ranges and sample sizes; bootstrapped confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc03c33-1cfd-4f85-a4bc-e6bb95a280b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "# Initialize parameters\n",
    "n_iterations = 1000  # Number of bootstrap iterations\n",
    "thresholds = [(0, 0), (0, 10), (0, 25), (0, 50), (0, 100), (0, 200), (0, 300), (0, float('inf'))]\n",
    "earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "resolution = 5\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "sample_sizes.append('all') # Add special value to signify using all records\n",
    "window_size = 5 # Define the size of the moving window for the moving average\n",
    "\n",
    "# Define directory to store bootstrap samples and create it if it doesn't exist\n",
    "bootstrap_samples_dir = \"bootstrap-samples/spa-ci/date-ranges/subsamples\"\n",
    "os.makedirs(bootstrap_samples_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each date range threshold\n",
    "for start, end in thresholds:\n",
    "    # Filter DataFrame based on the current threshold\n",
    "    if end == float('inf'):\n",
    "        subset_df = lire[lire['date_range'] >= start]\n",
    "        label = f\"{start}+\"\n",
    "    elif start == 0 and end == 0:\n",
    "        subset_df = lire[lire['date_range'] == 0]\n",
    "        label = f\"{start}\"\n",
    "    else:\n",
    "        subset_df = lire[(lire['date_range'] >= start) & (lire['date_range'] <= end)]\n",
    "        label = f\"{start}-{end}\"\n",
    "    # Save the number of inscriptions to a variable\n",
    "    n_inscriptions = subset_df.shape[0]\n",
    "\n",
    "    # Loop through each sample size\n",
    "    for sample_size in sample_sizes:\n",
    "        # Check for special case of sample_size = 'all'\n",
    "        if sample_size == 'all':\n",
    "            subsample = subset_df\n",
    "            size_label = \"all\"\n",
    "        # Or, create a random subsample from the filtered DataFrame\n",
    "        else:\n",
    "            subsample = subset_df.sample(n=min(sample_size, len(subset_df)), random_state=42)\n",
    "            size_label = str(sample_size)\n",
    "        # Generate filename for the bootstrap sample\n",
    "        print(f\"Checking for bootstrap sample file for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "        filename_info = f\"date-range_{label}_sample-size_{size_label}\"\n",
    "        filename_hash = hashlib.md5(filename_info.encode()).hexdigest()\n",
    "        saved_sample_path = os.path.join(bootstrap_samples_dir, f\"{filename_hash}.npy\")\n",
    "\n",
    "        # Check if bootstrap sample file already exists\n",
    "        if os.path.exists(saved_sample_path):\n",
    "            print(f\"Loading bootstrap sample file for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "            bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "        # If no bootstrap sample file exists, generate bootstrap sample and save it\n",
    "        else:\n",
    "            print(f\"Calculating bootstrap samples for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "            bootstrap_summed_probs = np.zeros((n_iterations, int((latest_date - earliest_date) / resolution)))# Bootstrap sampling  <-- New block\n",
    "            for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "                bootstrap_sample = subsample.sample(n=len(subsample), replace=True)\n",
    "                bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "                bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "            \n",
    "            # Save the computed bootstrap samples\n",
    "            print(f\"Saving bootstrap sample file for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "            np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "\n",
    "        # Calculate 95% confidence intervals\n",
    "        lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "\n",
    "        # Calculate the summed probability and standard deviations for the subset\n",
    "        summed_prob, _ = compute_summed_probability(subsample, earliest_date, latest_date, resolution)\n",
    "\n",
    "        # Generate x-values for plotting\n",
    "        x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "        # Calculate moving average for summed_prob\n",
    "        moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "\n",
    "        # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "        moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "        moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "\n",
    "        # Plot the summed probability distribution\n",
    "        plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "        \n",
    "        # Plot the 95% confidence intervals  <-- New line\n",
    "        plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "        \n",
    "        # Plot the moving averages\n",
    "        plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "\n",
    "        # Add fill_between for the moving averages of the 95% CI\n",
    "        plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "\n",
    "        # Plot title; include number of inscriptions if subsample = 'all'\n",
    "        if size_label == \"all\":\n",
    "            plt.title(f\"Summed Probability Distribution for Date Range {label}, all records (n={n_inscriptions})\")\n",
    "        else:\n",
    "            plt.title(f\"Summed Probability Distribution for Date Range {label}, Sample Size {size_label}\")\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.xlim([-100, 600])\n",
    "        plt.ylabel('Summed Probability')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87dda20-2bca-4d08-b3aa-15298cbf0a7a",
   "metadata": {},
   "source": [
    "## Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e57d99-2f7c-46d9-aaa2-0d8645516d41",
   "metadata": {},
   "source": [
    "The output of the SPA appears, at first glance, plausible as a proxy for either the population of the Latin-speaking regions of the Roman Empire, or perhaps the broader sociopolitical complexity, between about AD 1 - 400. Is so, it would indicate a high and slowly growing population (with some ups and downs) between ca. 1 - 200 AD, then a major downturn during the 'Crisis of the 3rd Century', followed by a partial rebound in the late 3rd and early 4th centuries AD, concluding with decline betwwen about the second quarter and the end of the 4th century. \n",
    "\n",
    "I now need to:\n",
    "\n",
    "* compare this pattern with population estimates from Hansen and others\n",
    "* refresh my lit review from 2020, looking especially to see if anyone has done SPA on inscriptions since then (I could find no indication of this approach at that time)\n",
    "* apply this approach to subsets of the data: provinces, cities, etc., and see what those patterns look like\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e5c3a-7326-4877-9d53-86b0a2d2edd0",
   "metadata": {},
   "source": [
    "# SPA on major subsets of LIRE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3e55c-1eab-40a4-bedb-da5d96397067",
   "metadata": {},
   "source": [
    "Use code from above, except:\n",
    "\n",
    "1. Remove the sample_sizes loop and list to eliminate subsampling.\n",
    "2. Exclude entries from the province of Roma by filtering out such rows from lire dataframe.\n",
    "3. Update the folder path to save/load the bootstrap samples\n",
    "\n",
    "No need for subsamples, that was exploratory. Will, however, still generate SPA for various date-ranges.\n",
    "\n",
    "Changes in the code were so slight that I've attempted to run the three variations as part of one code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b476dd0-e7aa-4fa7-824f-43f67edf5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "# Function to run the analysis\n",
    "\n",
    "def run_analysis(lire, bootstrap_samples_dir, file_prefix):\n",
    "        \n",
    "    # Constants\n",
    "    X_LIM_LOW = -100\n",
    "    X_LIM_HIGH = 600\n",
    "    \n",
    "    # Initialize parameters\n",
    "    n_iterations = 1000  # Number of bootstrap iterations\n",
    "    thresholds = [(0, 0), (0, 10), (0, 25), (0, 50), (0, 100), (0, 200), (0, 300), (0, float('inf'))]\n",
    "    earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "    resolution = 5\n",
    "    window_size = 5 # Define the size of the moving window for the moving average\n",
    "    \n",
    "    # Creat new bootstrap samples directory if it doesn't exist (parameter passed to this function)\n",
    "    os.makedirs(bootstrap_samples_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through each date range threshold\n",
    "    for start, end in thresholds:\n",
    "        # Filter DataFrame based on the current threshold\n",
    "        if end == float('inf'):\n",
    "            subset_df = lire[lire['date_range'] >= start]\n",
    "            label = f\"{start}+\"\n",
    "        elif start == 0 and end == 0:\n",
    "            subset_df = lire[lire['date_range'] == 0]\n",
    "            label = f\"{start}\"\n",
    "        else:\n",
    "            subset_df = lire[(lire['date_range'] >= start) & (lire['date_range'] <= end)]\n",
    "            label = f\"{start}-{end}\"\n",
    "    \n",
    "        # Save the number of inscriptions to a variable\n",
    "        n_inscriptions = subset_df.shape[0]\n",
    "    \n",
    "        # Generate filename for the bootstrap sample\n",
    "        print(f\"Checking for bootstrap sample file for: date-range {label} {file_prefix}...\")  # Status message\n",
    "        filename_info = f\"{file_prefix}-{label}\" \n",
    "        filename_hash = hashlib.md5(filename_info.encode()).hexdigest()\n",
    "        saved_sample_path = os.path.join(bootstrap_samples_dir, f\"{filename_hash}.npy\")\n",
    "    \n",
    "        # Check if bootstrap sample file already exists\n",
    "        if os.path.exists(saved_sample_path):\n",
    "            print(f\"Loading bootstrap sample file for: date-range {label} {file_prefix}...\") # Status message\n",
    "            bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "        \n",
    "        # If no bootstrap sample file exists, generate bootstrap sample and save it\n",
    "        else:\n",
    "            print(f\"Calculating bootstrap samples for: date-range {label} {file_prefix}...\") # Status message\n",
    "            bootstrap_summed_probs = np.zeros((n_iterations, int((latest_date - earliest_date) / resolution)))# Bootstrap sampling  <-- New block\n",
    "            for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "                bootstrap_sample = subset_df.sample(n=len(subset_df), replace=True)\n",
    "                bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "                bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "               \n",
    "            # Save the computed bootstrap samples\n",
    "            print(f\"Saving bootstrap sample file for: date-range {label} {file_prefix}...\") # Status message\n",
    "            np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "            \n",
    "        # Calculate 95% confidence intervals\n",
    "        lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "        \n",
    "        # Calculate the summed probability and standard deviations for the subset\n",
    "        summed_prob, _ = compute_summed_probability(subset_df, earliest_date, latest_date, resolution)\n",
    "        \n",
    "        # Generate x-values for plotting\n",
    "        x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "        \n",
    "        # Calculate moving average for summed_prob\n",
    "        moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "        \n",
    "        # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "        moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "        moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "        \n",
    "        # Plot the summed probability distribution\n",
    "        plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    \n",
    "        # Plot the 95% confidence intervals  <-- New line\n",
    "        plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    \n",
    "        # Plot the moving averages\n",
    "        plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "        # Add fill_between for the moving averages of the 95% CI\n",
    "        plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "            \n",
    "        plt.title(f\"Summed Probability Distribution for Date Range {label} (n={n_inscriptions})\")\n",
    "        plt.xlabel('Date')\n",
    "        plt.xlim([X_LIM_LOW, X_LIM_HIGH])\n",
    "        plt.ylabel('Summed Probability')\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "# End function to run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9c48e-16f4-4a88-82b0-745917871c44",
   "metadata": {},
   "source": [
    "## Entire dataset excluding Roma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ad8ef-1c91-432b-bcef-509c1988456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis three times with different settings\n",
    "\n",
    "# (1) for all provinces excluding Roma\n",
    "print(\"Running analysis for all provinces excluding Roma...\")\n",
    "bootstrap_samples_dir_1 = \"bootstrap-samples/spa-ci/date-ranges/no-roma\"\n",
    "file_prefix_1 = \"no-roma\"\n",
    "lire_1 = lire[lire['province'] != 'Roma']\n",
    "run_analysis(lire_1, bootstrap_samples_dir_1, file_prefix_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe78bc3-f898-4697-85d0-bfa1faacb81a",
   "metadata": {},
   "source": [
    "## Latin-speaking provinces only (including Roma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cd7a3-4f26-4adf-8101-55cb4abef5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) for only Latin-speaking provinces including Roma\n",
    "print(\"Running analysis for Latin-speaking provinces including Roma...\")\n",
    "bootstrap_samples_dir_2 = \"bootstrap-samples/spa-ci/date-ranges/latin-speaking\"\n",
    "file_prefix_2 = \"latin-speaking\"\n",
    "lire_2 = lire[lire['province_language'] == 'Latin']\n",
    "run_analysis(lire_2, bootstrap_samples_dir_2, file_prefix_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889f578-0cd1-457c-a7c5-3d32ccc8b33a",
   "metadata": {},
   "source": [
    "## Latin-speaking provinces only (excluding Roma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb0bc1-befa-4903-9d07-3f16a3e59212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) for only Latin-speaking provinces excluding Roma\n",
    "print(\"Running analysis for Latin-speaking provinces excluding Roma...\")\n",
    "bootstrap_samples_dir_3 = \"bootstrap-samples/spa-ci/date-ranges/latin-speaking-no-roma\"\n",
    "file_prefix_3 = \"latin-speaking-no-roma\"\n",
    "lire_3 = lire[(lire['province'] != 'Roma') & (lire['province_language'] == 'Latin')]\n",
    "run_analysis(lire_3, bootstrap_samples_dir_3, file_prefix_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b4d7b-fd73-428c-a527-06f98fd4b39a",
   "metadata": {},
   "source": [
    "# SPA of inscriptions from individual provinces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77393a86-98d2-4db2-b70c-fbc282128a56",
   "metadata": {},
   "source": [
    "## List all the provinces in the LIRE dataset, with number of inscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5c4f1-d205-4434-8a77-a5d056663848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in the 'province' column\n",
    "province_counts = lire['province'].value_counts()\n",
    "\n",
    "# Exclude the 'none' entries if applicable\n",
    "province_counts = province_counts[province_counts.index != 'none']\n",
    "\n",
    "# Filter out provinces with fewer than 100 inscriptions\n",
    "province_counts_filtered = province_counts[province_counts >= 100]\n",
    "\n",
    "# Print inscription counts by province\n",
    "print(\"Count of inscriptions by province (n>=100):\")\n",
    "print(province_counts_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a05b98-e59c-4487-b2f4-1f5d7f23235c",
   "metadata": {},
   "source": [
    "## SPA on all provinces with at least 100 inscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b5497-a738-4a51-8a7e-e7a84824e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "\n",
    "# Function to clean automatically generated filenames\n",
    "\n",
    "def clean_filename(filename):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', filename)\n",
    "\n",
    "# End function\n",
    "\n",
    "# Count the number of inscriptions for each unique province\n",
    "province_counts = lire['province'].value_counts()\n",
    "\n",
    "# Filter to only include provinces with at least 100 inscriptions\n",
    "filtered_provinces = province_counts[province_counts >= 100].index.tolist()\n",
    "\n",
    "# Initialize parameters for SPA\n",
    "n_iterations = 1000  # Number of bootstrap iterations\n",
    "earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "resolution = 5\n",
    "window_size = 5 # Size of the moving window for the moving average\n",
    "\n",
    "# Loop through each province\n",
    "for province in filtered_provinces:\n",
    "    # Create a dynamic file name for the province's bootstrap samples\n",
    "    filename = clean_filename(province)\n",
    "    saved_sample_path = f\"bootstrap-samples/spa-ci/provinces/{filename}.npy\"\n",
    "    \n",
    "    # Filter DataFrame based on the current province\n",
    "    subset_df = lire[lire['province'] == province]\n",
    "\n",
    "    # Get the number of inscriptions for this province\n",
    "    n_inscriptions = len(subset_df)\n",
    "\n",
    "    # Check if saved bootstrap samples exist for this province\n",
    "    if os.path.exists(saved_sample_path):\n",
    "        print(f\"Loading saved bootstrap samples for {province}...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "        # print(f\"Shape of loaded array: {bootstrap_summed_probs.shape}\") # debug ValueError shape mismatch\n",
    "    else:\n",
    "        print(f\"Performing bootstrap operation for {province}...\")\n",
    "        bootstrap_summed_probs = None\n",
    "        for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "            bootstrap_sample = subset_df.sample(n=len(subset_df), replace=True)\n",
    "            bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "            if bootstrap_summed_probs is None:\n",
    "                bootstrap_summed_probs = np.zeros((n_iterations, len(bootstrap_summed_prob)))\n",
    "            bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "\n",
    "        # debug_array = bootstrap_summed_probs # debug\n",
    "        # print(f\"Debug shape before saving: {debug_array.shape}\") # debug ValueError shape mismatch\n",
    "        # print(f\"First element when generated: {debug_array[0]}\") # debug ValueError shape mismatch\n",
    "        print(f\"Saving bootstrap samples for {province}...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "        # debug_array = np.load(saved_sample_path) # debug ValueError shape mismatch\n",
    "        # print(f\"Debug shape after saving: {debug_array.shape}\") # debug ValueError shape mismatch\n",
    "        # print(f\"First element when loaded: {debug_array[0]}\") # debug ValueError shape mismatch\n",
    "\n",
    "    # Calculate 95% confidence intervals\n",
    "    lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "    \n",
    "    # Calculate the summed probability for the subset_df\n",
    "    summed_prob, _ = compute_summed_probability(subset_df, earliest_date, latest_date, resolution)\n",
    "    \n",
    "    # Generate x-values for plotting\n",
    "    x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "    # Calculate moving average for summed_prob\n",
    "    moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "    \n",
    "    # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "    moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "    moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "    plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "    \n",
    "    plt.title(f\"Summed Probability Distribution for Province: {province} (n={n_inscriptions})\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.xlim([-100, 600])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9423e-f95b-4962-ad2e-9dacd5845821",
   "metadata": {},
   "source": [
    "# SPA of inscriptions from individual named cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a279b58-908e-4c2e-b35a-934f76c9d542",
   "metadata": {},
   "source": [
    "### List all cities with at least 100 inscriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5127ba1-3af6-4168-a02e-48940a57d121",
   "metadata": {},
   "source": [
    "Here, we count the records for each unique value (city) in the column 'urban_context_city'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1de55-9848-4474-abbc-2a146491352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in the 'urban_context_city' column\n",
    "city_counts = filtered_lire['urban_context_city'].value_counts()\n",
    "\n",
    "# Exclude the 'none' entries\n",
    "city_counts = city_counts[city_counts.index != 'none']\n",
    "\n",
    "# Filter out cities with fewer than 100 inscriptions\n",
    "city_counts_filtered = city_counts[city_counts >= 100]\n",
    "\n",
    "# Print inscription counts by city\n",
    "print(\"Count of inscriptions by city (with at least 100 inscriptions):\")\n",
    "print(city_counts_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca64423-5515-49a7-b3dc-b10a5fb6a0ab",
   "metadata": {},
   "source": [
    "## SPA on cities with at least 100 inscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3131685-65bd-4507-8f9a-100a7aa3743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "\n",
    "# Function to clean automatically generated filenames\n",
    "\n",
    "def clean_filename(filename):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', filename)\n",
    "\n",
    "# End function\n",
    "\n",
    "\n",
    "# Count the number of inscriptions of each unique city\n",
    "city_counts = lire['urban_context_city'].value_counts()\n",
    "\n",
    "# Filter the cities that have at least 100 inscriptions\n",
    "filtered_cities = city_counts[city_counts >= 100].index.tolist()\n",
    "\n",
    "# Optionally, print out the list and counts for verification\n",
    "# print(\"Filtered cities and their respective counts:\")\n",
    "# print(city_counts[city_counts >= 240])\n",
    "\n",
    "# Optionally, print the list of filtered cities\n",
    "# print(\"List of filtered cities:\")\n",
    "# print(filtered_cities)\n",
    "\n",
    "# Initialize parameters for SPA\n",
    "n_iterations = 1000  # Number of bootstrap iterations\n",
    "earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "resolution = 5\n",
    "window_size = 5 # Size of the moving window for the moving average\n",
    "\n",
    "# Loop through each city\n",
    "for city in filtered_cities:\n",
    "    # Create a dynamic file name for the city's bootstrap samples\n",
    "    filename = clean_filename(city)\n",
    "    saved_sample_path = f\"bootstrap-samples/spa-ci/cities/{filename}.npy\"\n",
    "\n",
    "    \n",
    "    # Filter DataFrame based on the current city\n",
    "    subset_df = lire[lire['urban_context_city'] == city]\n",
    "\n",
    "    # Get the number of inscriptions for this city\n",
    "    n_inscriptions = len(subset_df)\n",
    "\n",
    "    # Check if saved bootstrap samples exist for this province\n",
    "    if os.path.exists(saved_sample_path):\n",
    "        # Load saved bootstrap samples\n",
    "        print(f\"Loading saved bootstrap samples for {city}...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "    else:\n",
    "        print(f\"Performing bootstrap operation for {city}...\")\n",
    "        # Initialize storage for bootstrap results\n",
    "        bootstrap_summed_probs = None\n",
    "        # Bootstrap sampling\n",
    "        for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "            bootstrap_sample = subset_df.sample(n=len(subset_df), replace=True)\n",
    "            bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "            if bootstrap_summed_probs is None:\n",
    "                bootstrap_summed_probs = np.zeros((n_iterations, len(bootstrap_summed_prob)))\n",
    "            bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "        \n",
    "        # Save the bootstrap samples\n",
    "        print(f\"Saving bootstrap samples for {city}...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "        \n",
    "    # Calculate 95% confidence intervals\n",
    "    lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "    \n",
    "    # Calculate the summed probability for the subset_df\n",
    "    summed_prob, _ = compute_summed_probability(subset_df, earliest_date, latest_date, resolution)\n",
    "    \n",
    "    # Generate x-values for plotting\n",
    "    x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "    # Calculate moving average for summed_prob\n",
    "    moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "    \n",
    "    # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "    moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "    moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "    plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "    \n",
    "    plt.title(f\"Summed Probability Distribution for {city} (n={n_inscriptions})\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.xlim([-100, 600])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f0a9d-7c47-443a-9095-1237e3b550d7",
   "metadata": {},
   "source": [
    "## Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ef47f-7e48-4d3a-8b84-46ebcb7ec3b9",
   "metadata": {},
   "source": [
    "I need to:\n",
    "\n",
    "* Rerun empire-wide statistics without Rome - DONE\n",
    "* Eliminate non-Latin-speaking provinces - DONE\n",
    "* Decide what inscriptions to include in analyses (<=200-year date range? 300 year? All?)\n",
    "* Finalise sample-size cut-offs for analysing urban places (250 inscriptions? 500? 750?)\n",
    "* Save bootstrapping data to files throughout so that it all runs faster - DONE\n",
    "* Run on BinderHub\n",
    "* Re-run Hanson's analysis comparing urban populations from other sources (his work, included in LIRE) to the number of inscriptions, with residuals for urban places\n",
    "* Run Hanson's analysis amalgamating to the province level\n",
    "* Run Hanson's anlysis amalgamating to the Latin-speaking-empire level\n",
    "* Consider whether Hanson's analysis can be improved by looking at duration of occupation; it seems to me that he simply presents the (maximum?) size of the settlement, ignoring duration of occuption, which may complicate the relationship between the number of inscriptions and the population - longer-lived settlements will produce more inscriptions at the same population size. Maybe some sort of dirivative metric, like average number of inscriptions per year at the site.\n",
    "* Using whatever coefficients of coreleation that produces, calculate changing population over time using SPA on inscriptions\n",
    "* Extrapolate / amalgamate to province-level and (Latin speaking) empire-level\n",
    "* Do plots showing changing contributions to overall population (itself changing) of cities --> provinces, provinces --> Latin-speaking empire.\n",
    "* Do the analysis based on word count per location instead of inscription count\n",
    "\n",
    "The overal point or hook is the ability not only to get a static (maximum?) urban place size (like Hanson has done), but to plot population over time using inscriptions as a proxy, where enough inscriptions exist, and do it all responsibly with appropriate confidence intervals and qualifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb282734-3530-42ad-aec8-881ce63966d2",
   "metadata": {},
   "source": [
    "# Extending Hanson 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d6c99-7a2b-454d-96cf-3755361c357b",
   "metadata": {},
   "source": [
    "## Reproduce Hanson's correlation between inscription count and his independent population estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7957b-7788-4393-89d6-71cafc3275f8",
   "metadata": {},
   "source": [
    "Hanson (2021, 143) briefly discusses SPA using inscriptions, although he references Wilson (2009) on shipwrecks rather than the extensive literature on radiocarbon dates. He then uses the SPA only to support the broad observation that most of the inscriptions date to the first and second century AD, and therefor correspond to the time when the population was greatest. Before moving on to a diachronic analysis that correlates the total number of inscriptions from each site to the maximum population of the site, he observes that 'using more refined date ranges would be an interesting matter for furture research'. This analysis takes up that challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650aab8-d87f-4879-8d77-73710ee128f2",
   "metadata": {},
   "source": [
    "### Filter named 'urban_context_city' records and display them with population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7fe9d-e32a-483b-9953-59c8b95cd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to exclude rows where 'urban_context_city' is 'None'\n",
    "# or where 'urban_context_pop_est' is NaN\n",
    "filtered_df = lire[(lire['urban_context_city'] != 'None') & (lire['urban_context_pop_est'].notna())]\n",
    "\n",
    "# Get unique cities that have a population estimate\n",
    "unique_cities_with_pop = filtered_df[['urban_context_city', 'urban_context_pop_est']].drop_duplicates()\n",
    "\n",
    "# Sort the DataFrame by population estimate in descending order\n",
    "sorted_unique_cities = unique_cities_with_pop.sort_values(by='urban_context_pop_est', ascending=False)\n",
    "\n",
    "# Display the first 10 records of the sorted DataFrame using to_string() for column spacing\n",
    "print(sorted_unique_cities.head(10).to_string(index=False, col_space=12))\n",
    "\n",
    "# Display the total number of unique cities with population estimates\n",
    "total_records = sorted_unique_cities.shape[0]\n",
    "print(f\"Total number of unique cities with population estimates: {total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df02e0-6d18-4cff-a9a0-2fe78fd20caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create new df including city name, population estimate, and inscription count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2600a-06fa-4fc5-a158-8ca65b5ba0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of unique cities that have population estimates\n",
    "unique_cities_with_pop = lire[(lire['urban_context_city'] != 'None') & (lire['urban_context_pop_est'].notna())]['urban_context_city'].unique()\n",
    "\n",
    "# Group by 'urban_context_city' and 'urban_context_pop_est', then count the number of inscriptions for each city\n",
    "grouped_df = lire[(lire['urban_context_city'] != 'None') & (lire['urban_context_pop_est'].notna())].groupby(['urban_context_city', 'urban_context_pop_est']).size().reset_index(name='inscription_count')\n",
    "\n",
    "# Sort the DataFrame by 'urban_context_pop_est' in descending order\n",
    "grouped_df_sorted = grouped_df.sort_values(by='urban_context_pop_est', ascending=False)\n",
    "\n",
    "# Display the DataFrame in a tight layout\n",
    "print(\"\\nTop 10 cities sorted by population estimate, along with their inscription counts:\")\n",
    "print(grouped_df_sorted.head(10).to_string(index=False, col_space=12))\n",
    "\n",
    "# Display the total number of records in the grouped DataFrame\n",
    "total_records_grouped = grouped_df_sorted.shape[0]\n",
    "print(f\"\\nTotal number of records in the DataFrame: {total_records_grouped}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6ba05-7314-4aca-bebb-fc282ad40998",
   "metadata": {},
   "source": [
    "#### Filter for Latin-speaking only, and exclude Roma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873bf9e-f457-4cb1-80fa-620ee962a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only Latin-speaking provinces and exclude the city 'Roma'\n",
    "latin_provinces_df = lire[(lire['province_language'] == 'Latin') & (lire['urban_context_city'] != 'Roma')]\n",
    "\n",
    "# Generate a list of unique cities in Latin-speaking provinces that have population estimates\n",
    "unique_cities_with_pop = latin_provinces_df[(latin_provinces_df['urban_context_city'] != 'None') & (latin_provinces_df['urban_context_pop_est'].notna())]['urban_context_city'].unique()\n",
    "\n",
    "# Group by 'urban_context_city' and 'urban_context_pop_est', then count the number of inscriptions for each city\n",
    "grouped_df = latin_provinces_df[(latin_provinces_df['urban_context_city'] != 'None') & (latin_provinces_df['urban_context_pop_est'].notna())].groupby(['urban_context_city', 'urban_context_pop_est']).size().reset_index(name='inscription_count')\n",
    "\n",
    "# Sort the DataFrame by 'urban_context_pop_est' in descending order\n",
    "grouped_df_sorted = grouped_df.sort_values(by='urban_context_pop_est', ascending=False)\n",
    "\n",
    "# Display the DataFrame in a tight layout\n",
    "print(\"\\nTop 10 cities in Latin-speaking provinces sorted by population estimate, along with their inscription counts:\")\n",
    "print(grouped_df_sorted.head(10).to_string(index=False, col_space=12))\n",
    "\n",
    "# Display the total number of records in the grouped DataFrame\n",
    "total_records_grouped = grouped_df_sorted.shape[0]\n",
    "print(f\"\\nTotal number of records in the DataFrame: {total_records_grouped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e1024-b0a4-4b72-8bfc-3c919df220fe",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS) regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db45143-5b0f-44f6-b686-832860712f3f",
   "metadata": {},
   "source": [
    "### Reproducing Hanson's appraoch to determining a relationship between city poputlation and inscription count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74965c1a-f821-4276-b994-542cb64ce80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for OLS regression\n",
    "# Note: statsmodels requires us to add a constant term for the intercept\n",
    "X = grouped_df_sorted['urban_context_pop_est']  # Predictor variable (Population)\n",
    "y = grouped_df_sorted['inscription_count']  # Response variable (Number of inscriptions)\n",
    "X = sm.add_constant(X)  # Adding a constant for the intercept term\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the model statistics\n",
    "print(model.summary())\n",
    "\n",
    "# Generate predicted values and standard errors\n",
    "predictions = model.predict(X)\n",
    "pred_std = model.get_prediction(X).se_obs\n",
    "\n",
    "# Calculate upper and lower 95% confidence intervals\n",
    "ci_upper = predictions + 1.96 * pred_std\n",
    "ci_lower = predictions - 1.96 * pred_std\n",
    "\n",
    "# Plot the data and the OLS fit line along with confidence intervals\n",
    "plt.scatter(X['urban_context_pop_est'], y, label='Data', alpha=0.5)\n",
    "plt.xlabel('Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('OLS Fit with 95% Confidence Interval: Relationship between Population and Inscription Count')\n",
    "plt.plot(X['urban_context_pop_est'], predictions, color='red', label='OLS Fit Line')\n",
    "plt.fill_between(X['urban_context_pop_est'], ci_lower, ci_upper, color='gray', alpha=0.2, label='95% CI')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d259b-c13a-40c0-a059-0c6cf81305a3",
   "metadata": {},
   "source": [
    "Hypothesis: cities of higher population produce more inscriptions. Population is the predictor variable and inscriptions are the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba54ad-92bc-4607-8878-56a00b418ded",
   "metadata": {},
   "source": [
    "### Interpreting OLS regression results: ChatGPT help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72ab17-744b-44a4-aabd-7ee8741cfd5c",
   "metadata": {},
   "source": [
    "The \\( R^2 \\) value, also known as the coefficient of determination, is a measure of how well the model's predictions match the actual data. An \\( R^2 \\) value of 1 indicates that the model perfectly predicts the outcome variable, whereas a value of 0 indicates that the model is no better than a model that simply predicts the mean of the target variable for all observations.\n",
    "\n",
    "In your case, an \\( R^2 \\) value of 0.039 suggests that only about 3.9% of the variability in the population estimate can be explained by the number of inscriptions. This is a very low value and indicates a weak relationship between the two variables, according to the model.\n",
    "\n",
    "Here are some considerations for interpreting this \\( R^2 \\) value:\n",
    "\n",
    "1. **Low Predictive Power:** The model has low predictive power for estimating population based on the number of inscriptions.\n",
    "  \n",
    "2. **Other Factors:** Given the low \\( R^2 \\), it's likely that other factors, not included in the model, have a significant impact on the population estimate.\n",
    "\n",
    "3. **Linear Fit Might Not Be Appropriate:** The low \\( R^2 \\) could also mean that the relationship between the variables is not linear, and a linear model is not the best fit for the data.\n",
    "\n",
    "4. **Data Quality:** Always consider the possibility that the data itself might have issues. For example, if the population estimates are not reliable or if the inscription counts are incomplete, that could affect the \\( R^2 \\) value.\n",
    "\n",
    "So, while the \\( R^2 \\) value is a useful statistic, it's important to consider it in the context of your specific study, the quality of your data, and your understanding of the underlying processes that generate that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f7581-f69d-4c81-84f7-c3d14ba9a4b8",
   "metadata": {},
   "source": [
    "The F-statistic is a measure used to assess the significance of the overall regression model. It is used in the context of an F-test where it's compared to a critical value that depends on the degrees of freedom. A larger F-statistic indicates that the model is more likely to be statistically significant, i.e., at least one of the predictor variables in the regression model is statistically significant.\n",
    "\n",
    "In your case, an F-statistic of 32.94 suggests that the regression model is statistically significant at conventional levels (usually \\( p < 0.05 \\)), meaning that there is evidence to reject the null hypothesis that all of the regression coefficients are equal to zero (i.e., that none of the predictors are useful).\n",
    "\n",
    "However, it's essential to interpret the F-statistic in the context of the data and the subject-matter:\n",
    "\n",
    "1. **Significance Doesn't Mean Effect Size:** While the F-statistic tells you whether or not the model as a whole is statistically significant, it doesn't tell you how strong the relationship is. For that, you'll have to look at measures like \\( R^2 \\).\n",
    "\n",
    "2. **Data Quality:** As with \\( R^2 \\), the F-statistic can also be influenced by the quality of your data. If the data are noisy, incomplete, or biased, the F-statistic may be misleading.\n",
    "\n",
    "3. **Multiple Predictors:** In models with multiple predictors, the F-statistic tests whether at least one predictor is useful for predicting the response variable. However, in your case, you have only one predictor, making the F-statistic and the t-test for the predictor's coefficient essentially provide the same information.\n",
    "\n",
    "So, the F-statistic of 32.94 suggests that your model is statistically significant, but given the low \\( R^2 \\), the practical relevance of this finding might be limited. The model accounts for a very small proportion of the variance in the response variable, indicating a weak relationship despite statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd8ece-bbf4-4697-bf9f-7a5d679c4371",
   "metadata": {},
   "source": [
    "Certainly, let's discuss the table line by line.\n",
    "\n",
    "#### Intercept (const)\n",
    "\n",
    "The constant term (`const`) in the regression equation is the value of the dependent variable (the number of inscriptions in this case) when all independent variables are zero. Here, the `coef` (coefficient) value for `const` is 59.7384. This means that if a city had a population of zero (which is not practical in real-world terms), the model predicts that there would be approximately 59.7 inscriptions.\n",
    "\n",
    "#### std err (10.787)\n",
    "This represents the standard error of the coefficient estimate for the intercept. The smaller the standard error, the more precise the estimate. \n",
    "\n",
    "#### t (5.538)\n",
    "This is the t-value, which is calculated by dividing the coefficient by its standard error. The higher the t-value, the stronger the evidence against the null hypothesis, which suggests no effect.\n",
    "\n",
    "#### P>|t| (0.000)\n",
    "This p-value being close to zero indicates that the intercept is statistically significant, meaning that the observed number of inscriptions when all independent variables are zero would not be due to random chance.\n",
    "\n",
    "#### [0.025 0.975] (38.565 80.912)\n",
    "These are the 95% confidence intervals for the intercept. The model is 95% confident that the true intercept value lies between 38.565 and 80.912.\n",
    "\n",
    "#### Coefficient for urban_context_pop_est (0.0055)\n",
    "\n",
    "The coefficient for `urban_context_pop_est` tells us about the size and direction of the relationship between population and the number of inscriptions. A coefficient of 0.0055 means that for each unit increase in population, the number of inscriptions is expected to increase by 0.0055 units.\n",
    "\n",
    "#### std err (0.001)\n",
    "The standard error of 0.001 indicates the accuracy of this coefficient.\n",
    "\n",
    "#### t (5.739)\n",
    "The t-value of 5.739 is high, further supporting the rejection of the null hypothesis that this variable has no effect.\n",
    "\n",
    "#### P>|t| (0.000)\n",
    "The p-value being close to zero suggests that this relationship is statistically significant.\n",
    "\n",
    "#### [0.025 0.975] (0.004 0.007)\n",
    "The 95% confidence interval indicates that we are 95% confident that the true coefficient value lies between 0.004 and 0.007.\n",
    "\n",
    "In summary, the model is statistically significant, and both the intercept and the population variable appear to have a statistically significant relationship with the number of inscriptions. However, as discussed earlier, the R-squared value suggests that population alone does not explain much of the variance in the number of inscriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277ac1a-f567-436f-8f9b-6ea6ab7e1c71",
   "metadata": {},
   "source": [
    "## Considering alternative approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b96872-ce45-4c32-9524-b293e4824686",
   "metadata": {},
   "source": [
    "Certainly! Here's an expanded explanation for each approach:\n",
    "\n",
    "### Count Data Models\n",
    "1. **Poisson Regression**: This approach is useful for count data like the number of inscriptions. It assumes that the mean and variance of the distribution are equal. This is a common starting point for analyzing count data but can be too restrictive if the data is overdispersed.\n",
    "   \n",
    "2. **Negative Binomial Regression**: This is an extension of Poisson regression that introduces an extra parameter to account for overdispersion—when the data exhibits more variability than what the Poisson model can handle.\n",
    "\n",
    "### Linear Models & Extensions\n",
    "3. **Generalized Linear Models (GLM)**: GLMs are a flexible generalization of ordinary linear models, allowing for response variables that have error distribution models other than a normal distribution. The link function specifies the relationship between the linear predictor and the mean of the response variable.\n",
    "   \n",
    "4. **Robust Regression**: This technique is similar to OLS but less sensitive to outliers and high-leverage points. It modifies the loss function to be more forgiving of such data points.\n",
    "   \n",
    "5. **Ridge and Lasso Regression**: These are linear models with a twist. They include a penalty term on the coefficients to prevent them from getting too large, essentially regularizing the model to prevent overfitting.\n",
    "  \n",
    "6. **Polynomial Regression**: This technique fits a nonlinear equation to the data but does it within the framework of linear regression. It's useful when the relationship between variables is clearly not linear.\n",
    "   \n",
    "7. **Mixed-Effects Models**: Useful when there's a grouping variable (e.g., province, time period) that could introduce correlations between observations. This model accounts for both fixed and random effects.\n",
    "\n",
    "### Machine Learning Models\n",
    "8. **Random Forests**: This ensemble method uses multiple decision trees during training and outputs the average prediction of the individual trees for regression problems. It's good for capturing complex relationships but sacrifices some interpretability.\n",
    "\n",
    "9. **Gradient Boosting**: Another ensemble technique that builds trees one at a time, where each one corrects the errors of its predecessor. Like Random Forests, it can model complex relationships but can be harder to interpret.\n",
    "\n",
    "### Bayesian Methods\n",
    "10. **Bayesian Linear Regression**: This approach offers a probabilistic framework that goes beyond point estimates to provide a full distribution for the model's parameters. This accounts for uncertainty and can produce more robust results.\n",
    "  \n",
    "### Non-parametric Models\n",
    "11. **Spearman's Rank Correlation**: This is a non-parametric test that assesses the strength and direction of the relationship between two variables without making any assumptions about the relationship's form.\n",
    "\n",
    "12. **Kernel Regression**: This is a more flexible non-parametric technique that does not assume a specific functional form of the relationship between variables.\n",
    "\n",
    "### Miscellaneous\n",
    "13. **Bootstrapping**: This resampling technique is useful for small datasets or when the underlying distribution is unknown. It can provide more robust estimates by repeatedly sampling with replacement from the data.\n",
    "  \n",
    "14. **Log-Transformation**: If the variables span several orders of magnitude or if the relationship seems multiplicative, applying a log transformation can linearize the relationship, making it easier to model.\n",
    "\n",
    "Would you like to proceed with any of these methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f155a6c-58f3-4551-a5e6-59b03d119657",
   "metadata": {},
   "source": [
    "## Log transformation of OSL regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5b057-c8e2-46f6-b2f4-4a77cf928028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the variables\n",
    "grouped_df_sorted['log_urban_context_pop_est'] = np.log1p(grouped_df_sorted['urban_context_pop_est'])\n",
    "grouped_df_sorted['log_inscription_count'] = np.log1p(grouped_df_sorted['inscription_count'])\n",
    "\n",
    "# Prepare data for OLS regression after log-transformation\n",
    "X_log = grouped_df_sorted['log_urban_context_pop_est']  # Log-transformed predictor variable\n",
    "y_log = grouped_df_sorted['log_inscription_count']  # Log-transformed response variable\n",
    "X_log = sm.add_constant(X_log)  # Adding a constant for the intercept term\n",
    "\n",
    "# Fit the OLS model on log-transformed data\n",
    "model_log = sm.OLS(y_log, X_log).fit()\n",
    "\n",
    "# Print out the model statistics\n",
    "print(model_log.summary())\n",
    "\n",
    "# Get prediction standard errors for 95% confidence interval\n",
    "_, iv_l, iv_u = wls_prediction_std(model_log)\n",
    "\n",
    "# Plot the data and the OLS fit line for log-transformed data\n",
    "plt.scatter(X_log['log_urban_context_pop_est'], y_log, label='Log-Transformed Data', alpha=0.7)\n",
    "plt.xlabel('Log(Population Estimate)')\n",
    "plt.ylabel('Log(Inscription Count)')\n",
    "plt.title('OLS Fit on Log-Transformed Data with 95% Confidence Interval')\n",
    "\n",
    "# Plot OLS fit line\n",
    "plt.plot(X_log['log_urban_context_pop_est'], model_log.predict(X_log), color='red', label='OLS Fit Line')\n",
    "\n",
    "# Plot 95% confidence interval\n",
    "plt.fill_between(X_log['log_urban_context_pop_est'], iv_l, iv_u, color='gray', alpha=0.3, label='95% Conf. Int.')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886a7f3-bb35-4661-8130-ba5e158cff52",
   "metadata": {},
   "source": [
    "## Interpretation of log-transformed OLS results - ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc17668-7b69-4f76-8d70-62c28f367316",
   "metadata": {},
   "source": [
    "Certainly! Let's walk through the output of the log-transformed OLS regression model.\n",
    "\n",
    "### General Information\n",
    "\n",
    "- **Dep. Variable**: This is the dependent (response) variable, which is `log_inscription_count` after log-transformation.\n",
    "- **R-squared**: This statistic indicates the proportion of the variance for the dependent variable that's explained by the independent variable. In this case, the R-squared value of 0.101 indicates that approximately 10.1% of the variation in log-transformed inscription count is explained by the log-transformed population estimate.\n",
    "- **Adj. R-squared**: Similar to R-squared but adjusted for the number of predictors. Since we have only one predictor, this value is almost the same as R-squared.\n",
    "- **F-statistic**: The F-statistic tests whether at least one predictor variable has a non-zero coefficient. An F-statistic of 91.56 is quite high, indicating that the model is likely a good fit.\n",
    "- **Prob (F-statistic)**: This is the probability of observing a test statistic as extreme as the one for your model if the null hypothesis is true. A value close to zero means the model is statistically significant.\n",
    "- **Log-Likelihood**: The log of the likelihood function for the estimated model. Generally, higher values are better, though this metric is mostly useful for comparing different models fitted to the same dataset.\n",
    "- **AIC and BIC**: These are the Akaike and Bayesian Information Criteria, respectively. These metrics offer a balance between goodness-of-fit and model complexity, with lower values generally indicating better models.\n",
    "\n",
    "### Coefficients Table\n",
    "\n",
    "- **coef**: These are the estimated coefficients. For `log_urban_context_pop_est`, it's 0.4725, meaning that for each unit increase in log-transformed population estimate, we expect an increase of 0.4725 in log-transformed inscription count.\n",
    "- **std err**: This is the standard error of the coefficient estimate, useful for hypothesis tests and for constructing confidence intervals.\n",
    "- **t**: This is the t-statistic, calculated by dividing the coefficient by its standard error. Higher absolute values generally indicate greater significance.\n",
    "- **P>|t|**: The p-value associated with the t-statistic. A p-value close to zero indicates that the predictor is a significant predictor of the outcome variable.\n",
    "- **[0.025, 0.975]**: These are the 95% confidence intervals for the coefficient. Since this interval does not contain zero for `log_urban_context_pop_est`, it's a sign that this variable is a significant predictor.\n",
    "\n",
    "### Additional Diagnostics\n",
    "\n",
    "- **Omnibus, Durbin-Watson, Jarque-Bera (JB), Skew, Kurtosis, and Cond. No.**: These are additional tests and metrics that assess the residuals and other model assumptions. They are generally more important when diagnosing issues with the regression model.\n",
    "\n",
    "Based on these results, the log-transformed model seems to provide a better fit compared to the original model, but still, only around 10.1% of the variation in inscription count is explained by population size. The model is statistically significant, and the coefficient for log-transformed population size is also statistically significant.\n",
    "\n",
    "I hope that helps clarify the output! Would you like to move on to trying out some other modeling techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221fa61-87e9-46e1-8034-877a924dae30",
   "metadata": {},
   "source": [
    "Certainly! This table provides detailed statistics for each coefficient (constant and predictor) in the model. Let's look at each row in detail:\n",
    "\n",
    "### Constant (Intercept)\n",
    "- **coef = -0.5180**: This is the y-intercept of the regression line in the log-transformed space. It represents the expected value of the log-transformed inscription count when the log-transformed population estimate is zero. Given that both variables are log-transformed, the interpretation is not as straightforward as in a simple linear regression.\n",
    "  \n",
    "- **std err = 0.391**: This is the standard error of the coefficient. It gives an idea of the uncertainty around the estimate of the intercept.\n",
    "\n",
    "- **t = -1.326**: This is the t-statistic for the hypothesis test that checks whether this coefficient is different from zero. A t-value far from zero indicates that the coefficient is statistically significant, but here the value is relatively close to zero.\n",
    "\n",
    "- **P>|t| = 0.185**: This is the p-value for the test of the null hypothesis that this coefficient is zero. A value greater than 0.05 usually suggests that the coefficient is not statistically significant.\n",
    "\n",
    "- **[0.025, 0.975] = [-1.285, 0.249]**: This is the 95% confidence interval for the coefficient. Since this interval contains zero, it's another sign that the constant is not statistically significant.\n",
    "\n",
    "### log_urban_context_pop_est (Predictor)\n",
    "- **coef = 0.4725**: This is the slope of the regression line in the log-transformed space. It means that for a one-unit increase in the log-transformed population, we expect a 0.4725-unit increase in the log-transformed inscription count.\n",
    "\n",
    "- **std err = 0.049**: This is the standard error of the coefficient. Smaller values indicate more precise estimates.\n",
    "\n",
    "- **t = 9.568**: This t-statistic is far from zero, suggesting that the predictor is statistically significant.\n",
    "\n",
    "- **P>|t| = 0.000**: A p-value close to zero confirms that this predictor is statistically significant.\n",
    "\n",
    "- **[0.025, 0.975] = [0.376, 0.569]**: This is the 95% confidence interval for the predictor coefficient. Since it does not contain zero, we can say with 95% confidence that an increase in log-transformed population is associated with an increase in log-transformed inscription count.\n",
    "\n",
    "I hope this detailed walk-through clarifies each part of the table! Would you like to proceed with the other modeling techniques now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3a440-cd51-498e-a47d-f5408bd468b5",
   "metadata": {},
   "source": [
    "The statistical significance of the constant (intercept) and predictor variables in a regression model have different implications:\n",
    "\n",
    "### Constant (Intercept) Not Statistically Significant\n",
    "- **What it means**: The intercept being not statistically significant means that when the predictor variable is zero (in this case, when the log-transformed population is zero), we can't confidently say that the log-transformed inscription count would differ from zero. Given that both variables are log-transformed, this isn't a straightforward interpretation as it would be in a simple linear regression.\n",
    "  \n",
    "- **Implications**: In most practical situations, especially in this case where both variables are log-transformed, the intercept being non-significant isn't typically a concern. You're generally more interested in the relationship between the variables, not the absolute value of the output when the input is zero. \n",
    "\n",
    "### Predictor Statistically Significant\n",
    "- **What it means**: This is usually what you are most interested in. A statistically significant predictor suggests there's evidence to reject the null hypothesis that the predictor has no effect on the outcome variable. In other words, there's a relationship between the population and inscription count, at least in the dataset you're working with.\n",
    "\n",
    "- **Implications**: Since the predictor is significant, the focus will often be on understanding this relationship further, possibly by considering more complex models, adding additional predictors, or investigating causality (which regression alone cannot establish).\n",
    "\n",
    "### Summary\n",
    "In a nutshell, the constant's lack of significance is usually not a major issue if you are interested in the relationship between variables, especially in cases like this where both the predictor and the response are log-transformed. The statistical significance of the predictor variable is more critical for your analysis, as it suggests that there's a relationship between city population and inscription count that is worth investigating further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d479401-f6f8-40ed-85a7-fc3b6a302f13",
   "metadata": {},
   "source": [
    "## How disperesed is my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c77f46-c4ef-425a-9f60-c6b4b0318985",
   "metadata": {},
   "source": [
    "Choice of regression model partly depends upon how dispersed the data is - here we run a check for dispersal, comparing mean inscription count to the variance of inscription count. If the latter is much higher, the data is 'dispersed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f4a14-de02-4b85-aaa5-740edc0d2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and variance of the inscription count\n",
    "mean_inscription_count = grouped_df_sorted['inscription_count'].mean()\n",
    "var_inscription_count = grouped_df_sorted['inscription_count'].var()\n",
    "\n",
    "print(f\"Mean of Inscription Count: {mean_inscription_count}\")\n",
    "print(f\"Variance of Inscription Count: {var_inscription_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb183bfc-a1ce-4bcd-8964-efd4d92d8ffe",
   "metadata": {},
   "source": [
    "Yeah, that's a high ratio, data is definitely 'dispersed'. Let's use a Negative Binomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542b1df-8dfa-4e3d-a8af-8b47a0846327",
   "metadata": {},
   "source": [
    "## Negative Binomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7b98d-d25b-4f3e-9c16-667eeede4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data and fit the Negative Binomial model (as before)\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Generate predictions and calculate the confidence interval for new observations\n",
    "predictions = model.get_prediction(X)\n",
    "frame = predictions.summary_frame(alpha=0.05)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(X['log_urban_context_pop_est'], frame['mean'], label='Fitted', color='red')\n",
    "\n",
    "# Plot the 95% confidence intervals\n",
    "plt.fill_between(X['log_urban_context_pop_est'], frame['mean_ci_lower'], frame['mean_ci_upper'], color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Negative Binomial Regression Fit')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c3da7-5f36-477c-8739-8ed1defc09a6",
   "metadata": {},
   "source": [
    "## Interpretation from ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa6b04-420f-4954-935e-6762c8e24171",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the important parts of the Negative Binomial Regression output:\n",
    "\n",
    "1. **Dep. Variable**: `inscription_count` - This tells us the dependent variable we're trying to model.\n",
    "  \n",
    "2. **No. Observations**: `816` - This is the number of observations (i.e., data points) used for the model.\n",
    "  \n",
    "3. **Model Family**: `NegativeBinomial` - This confirms that the model uses the Negative Binomial distribution.\n",
    "  \n",
    "4. **Link Function**: `Log` - The log link function means that we're modeling the natural logarithm of the expected count as a linear function of the predictor variables.\n",
    "  \n",
    "5. **Method**: `IRLS` (Iteratively Reweighted Least Squares) - This is the optimization algorithm used to find the best-fitting model.\n",
    "  \n",
    "6. **Log-Likelihood**: `-4256.0` - The log-likelihood measures how well the model fits the data. The closer this value is to 0, the better the model fits the data.\n",
    "  \n",
    "7. **Deviance**: `1781.9` - This is a measure of goodness-of-fit. Lower values suggest that the model fits the data better.\n",
    "  \n",
    "8. **Pearson chi2**: `3.26e+03` - This is another goodness-of-fit measure. Like Deviance, lower values are better.\n",
    "  \n",
    "9. **No. Iterations**: `9` - The number of iterations taken by the IRLS algorithm to converge to a solution.\n",
    "  \n",
    "10. **Pseudo R-squ. (CS)**: `0.4207` - This is a measure of the proportion of variance explained by the model. It's analogous to the R-squared value in OLS regression, but keep in mind that it's not directly comparable. The value suggests that the model explains about 42.07% of the variance in the inscription count.\n",
    "\n",
    "11. **Covariance Type**: `nonrobust` - This specifies the type of covariance estimator used to calculate the standard errors. \"Nonrobust\" here simply means that no additional corrections were applied to the standard errors.\n",
    "\n",
    "The absence of p-values and confidence intervals in the table may vary depending on how the software package reports results. You might need to invoke specific methods to get these values, as they are essential for hypothesis testing.\n",
    "\n",
    "Overall, the Pseudo R-squared value indicates that the model explains a reasonable proportion of the variability in inscription counts, but it's not a perfect model by any means. Given that the model seems to be statistically significant, the predictor (population) is meaningful for predicting the inscription count, at least to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0f16d-27fd-42bb-bae1-7d89bec33abb",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the important parts of the Negative Binomial Regression output:\n",
    "\n",
    "1. **Dep. Variable**: `inscription_count` - This tells us the dependent variable we're trying to model.\n",
    "  \n",
    "2. **No. Observations**: `816` - This is the number of observations (i.e., data points) used for the model.\n",
    "  \n",
    "3. **Model Family**: `NegativeBinomial` - This confirms that the model uses the Negative Binomial distribution.\n",
    "  \n",
    "4. **Link Function**: `Log` - The log link function means that we're modeling the natural logarithm of the expected count as a linear function of the predictor variables.\n",
    "  \n",
    "5. **Method**: `IRLS` (Iteratively Reweighted Least Squares) - This is the optimization algorithm used to find the best-fitting model.\n",
    "  \n",
    "6. **Log-Likelihood**: `-4256.0` - The log-likelihood measures how well the model fits the data. The closer this value is to 0, the better the model fits the data.\n",
    "  \n",
    "7. **Deviance**: `1781.9` - This is a measure of goodness-of-fit. Lower values suggest that the model fits the data better.\n",
    "  \n",
    "8. **Pearson chi2**: `3.26e+03` - This is another goodness-of-fit measure. Like Deviance, lower values are better.\n",
    "  \n",
    "9. **No. Iterations**: `9` - The number of iterations taken by the IRLS algorithm to converge to a solution.\n",
    "  \n",
    "10. **Pseudo R-squ. (CS)**: `0.4207` - This is a measure of the proportion of variance explained by the model. It's analogous to the R-squared value in OLS regression, but keep in mind that it's not directly comparable. The value suggests that the model explains about 42.07% of the variance in the inscription count.\n",
    "\n",
    "11. **Covariance Type**: `nonrobust` - This specifies the type of covariance estimator used to calculate the standard errors. \"Nonrobust\" here simply means that no additional corrections were applied to the standard errors.\n",
    "\n",
    "The absence of p-values and confidence intervals in the table may vary depending on how the software package reports results. You might need to invoke specific methods to get these values, as they are essential for hypothesis testing.\n",
    "\n",
    "Overall, the Pseudo R-squared value indicates that the model explains a reasonable proportion of the variability in inscription counts, but it's not a perfect model by any means. Given that the model seems to be statistically significant, the predictor (population) is meaningful for predicting the inscription count, at least to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a03840-fdbf-4e6d-a139-e4ef6a379980",
   "metadata": {},
   "source": [
    "In a negative binomial regression model with a log link, the exponentiated coefficient represents the multiplicative effect on the dependent variable for a one-unit change in the predictor variable.\n",
    "\n",
    "So, to transform the log coefficient into real-world numbers, you would take the exponent of the coefficient for `log_urban_context_pop_est`, which is \\(e^{0.6866}\\).\n",
    "\n",
    "Calculating, we get:\n",
    "\\[\n",
    "e^{0.6866} \\approx 1.9871\n",
    "\\]\n",
    "\n",
    "Interpreted in the context of your study, this means that for each one-unit increase in the natural logarithm of the population, you can expect the number of inscriptions to be multiplied by approximately 1.9871.\n",
    "\n",
    "To make this even more concrete, let's say you have a city with a population of 1000. If the population increases to \\(e \\times 1000 \\approx 2718.28\\), the number of inscriptions would be expected to increase by a factor of about 1.9871. \n",
    "\n",
    "It's important to note that this is a multiplicative effect on the count, not an additive effect. So, if your city with 1000 people had, say, 50 inscriptions, a population increase to about 2718 would be expected to yield \\(50 \\times 1.9871 \\approx 99.355\\) inscriptions.\n",
    "\n",
    "Do note that this is an estimate and that real-world data often include other factors and variations not captured in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249aa77-e325-459e-b46e-e7392ca23000",
   "metadata": {},
   "source": [
    "The suitability of a Negative Binomial Regression model depends on several factors:\n",
    "\n",
    "1. **Count Data**: Your dependent variable, \"Inscription Count,\" is count data, which fits the assumptions of negative binomial regression.\n",
    "\n",
    "2. **Overdispersion**: Negative binomial regression is especially useful when the variance is greater than the mean, which is a sign of overdispersion. This is common in many types of count data and is a situation where Poisson regression models, for instance, may not be appropriate.\n",
    "\n",
    "3. **Log-Link Function**: The log transformation you applied makes sense given that your data spans several orders of magnitude. A log-link function is typically used for negative binomial regression and is well-suited to capture multiplicative effects on the dependent variable.\n",
    "\n",
    "4. **Interpretability**: Negative binomial models are relatively straightforward to interpret, especially when the link function is logarithmic.\n",
    "\n",
    "5. **Model Fit and Significance**: From the statistical output, it appears that the model fits the data better than the simple OLS regression, given the significance of the predictors and the pseudo R-squared value.\n",
    "\n",
    "6. **Complexity**: Negative binomial regression models are more complex than linear regression models but simpler than some machine learning models, making them a good middle-ground option for capturing complex relationships without becoming too hard to interpret.\n",
    "\n",
    "Given these factors and the nature of your data and research question, a negative binomial regression model seems like a reasonable choice for your analysis. It accounts for the specific statistical properties of count data and provides a more nuanced understanding of the relationship between population size and the number of inscriptions than a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070235af-31f3-47dc-82ce-9082727f13ab",
   "metadata": {},
   "source": [
    "The mean of the Inscription Count being approximately 88.5 and the variance being about 77,374 definitely suggests overdispersion in your data. Overdispersion is a situation where the variance is much greater than the mean, and it's a classic sign that a standard Poisson regression might not be appropriate for the data.\n",
    "\n",
    "Negative Binomial Regression is designed to handle overdispersion, so knowing these statistics actually strengthens the case for using it. It is specifically designed to model count data where the variance is greater than the mean, which appears to be the situation in your dataset.\n",
    "\n",
    "Given this information, I would say that Negative Binomial Regression is not just a good choice, but perhaps even the most appropriate choice given the characteristics of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a413e6-f98e-4e7b-9555-d52fdddb8ac8",
   "metadata": {},
   "source": [
    "## Bootstrap approach to Negative binomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad48506-1d5a-4d06-bee5-fc64b4be4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming grouped_df_sorted is your sorted DataFrame\n",
    "\n",
    "# Initialize directory and filenames for saving bootstrap samples\n",
    "bootstrap_dir = 'bootstrap-samples/modelling/'\n",
    "bootstrap_file = 'negative-binomial-pop-inscriptions.npz'\n",
    "bootstrap_path = os.path.join(bootstrap_dir, bootstrap_file)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "if not os.path.exists(bootstrap_dir):\n",
    "            os.makedirs(bootstrap_dir)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap = 1000\n",
    "\n",
    "# Check if bootstrap samples file exists\n",
    "if os.path.exists(bootstrap_path):\n",
    "    # Load bootstrap coefficients and pseudo R-squared values from the .npz file\n",
    "    loaded_data = np.load(bootstrap_path)\n",
    "    # Extract individual arrays and convert back to DataFrames\n",
    "    bootstrap_coefs_df = pd.DataFrame(loaded_data['bootstrap_coefs'], columns=['const', 'log_urban_context_pop_est'])\n",
    "    bootstrap_pseudo_r2_df = pd.DataFrame(loaded_data['bootstrap_pseudo_r2'], columns=['pseudo_r2'])\n",
    "\n",
    "    print(\"Loaded saved bootstrap samples.\")\n",
    "    \n",
    "else:\n",
    "    # Initialize empty lists to store bootstrap estimates\n",
    "    bootstrap_coefs = []\n",
    "    bootstrap_pseudo_r2 = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for i in range(n_bootstrap):\n",
    "        # Sample with replacement from the original data\n",
    "        bootstrap_sample = grouped_df_sorted.sample(n=len(grouped_df_sorted), replace=True)\n",
    "    \n",
    "        # Prepare the data and fit the Negative Binomial model\n",
    "        X = bootstrap_sample['log_urban_context_pop_est']\n",
    "        y = bootstrap_sample['inscription_count']\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.GLM(y, X, family=sm.families.NegativeBinomial(alpha=1.0)).fit()\n",
    "    \n",
    "        # Store the estimated coefficients and pseudo R-squared value\n",
    "        bootstrap_coefs.append(model.params)\n",
    "        bootstrap_pseudo_r2.append(model.deviance / model.null_deviance)\n",
    "\n",
    "    # Convert bootstrap estimates to DataFrames\n",
    "    bootstrap_coefs_df = pd.DataFrame(bootstrap_coefs, columns=['const', 'log_urban_context_pop_est'])\n",
    "    bootstrap_pseudo_r2_df = pd.DataFrame(bootstrap_pseudo_r2, columns=['pseudo_r2'])\n",
    "    \n",
    "    # Save bootstrap coefficients and pseudo R-squared values to a single .npz file\n",
    "    np.savez(bootstrap_path, bootstrap_coefs=bootstrap_coefs_df.to_numpy(), bootstrap_pseudo_r2=bootstrap_pseudo_r2_df.to_numpy())\n",
    "\n",
    "    print(\"Generated new bootstrap samples and saved.\")\n",
    "\n",
    "# Print summary statistics for bootstrap coefficients\n",
    "print(\"Summary Statistics for Bootstrap Coefficients:\")\n",
    "print(bootstrap_coefs_df.describe(percentiles=[.025, .5, .975]))\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "coef_ci_lower = bootstrap_coefs_df.quantile(0.025)\n",
    "coef_ci_upper = bootstrap_coefs_df.quantile(0.975)\n",
    "pseudo_r2_ci_lower = bootstrap_pseudo_r2_df['pseudo_r2'].quantile(0.025)\n",
    "pseudo_r2_ci_upper = bootstrap_pseudo_r2_df['pseudo_r2'].quantile(0.975)\n",
    "\n",
    "# Print summary statistics for bootstrap pseudo R-squared values\n",
    "print(\"Summary Statistics for Pseudo R-squared values:\")\n",
    "print(f\"Mean: {bootstrap_pseudo_r2_df['pseudo_r2'].mean()}\")\n",
    "print(f\"Standard Deviation: {bootstrap_pseudo_r2_df['pseudo_r2'].std()}\")\n",
    "print(f\"Minimum: {bootstrap_pseudo_r2_df['pseudo_r2'].min()}\")\n",
    "print(f\"Maximum: {bootstrap_pseudo_r2_df['pseudo_r2'].max()}\")\n",
    "\n",
    "# Create a figure with 3 subplots: one for each coefficient and one for pseudo R-squared\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot for Intercept (assuming it's named 'const')\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(bootstrap_coefs_df['const'], bins=30, edgecolor='k')\n",
    "plt.axvline(coef_ci_lower['const'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(coef_ci_upper['const'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Intercept')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for log_urban_context_pop_est\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(bootstrap_coefs_df['log_urban_context_pop_est'], bins=30, edgecolor='k')\n",
    "plt.axvline(coef_ci_lower['log_urban_context_pop_est'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(coef_ci_upper['log_urban_context_pop_est'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - log_urban_context_pop_est')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Pseudo R-squared\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(bootstrap_pseudo_r2_df['pseudo_r2'], bins=30, edgecolor='k')\n",
    "plt.axvline(pseudo_r2_ci_lower, color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(pseudo_r2_ci_upper, color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Pseudo R-squared')\n",
    "plt.xlabel('Pseudo R-squared Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836bcde",
   "metadata": {},
   "source": [
    "Interpretation from Copilot\n",
    "\n",
    "The bootstrap approach to regression provides a distribution of estimated coefficients and pseudo R-squared values from resampling your data, which can give you a sense of the variability and uncertainty in your estimates.\n",
    "\n",
    "Here's how to interpret the results:\n",
    "\n",
    "Bootstrap Coefficients:\n",
    "\n",
    "The mean of the intercept (const) is -1.154253. This is the expected log count of inscriptions when the log of the population estimate is zero, assuming all other predictors are held constant.\n",
    "\n",
    "The mean of the coefficient for log_urban_context_pop_est is 0.683338. This means that for each one-unit increase in the log of the population estimate, the expected log count of inscriptions increases by 0.683338, assuming all other predictors are held constant.\n",
    "\n",
    "The 2.5% and 97.5% percentiles for each coefficient give you a 95% confidence interval for that coefficient. For example, you can be 95% confident that the true value of the coefficient for log_urban_context_pop_est lies between 0.531814 and 0.849219.\n",
    "\n",
    "Bootstrap Pseudo R-squared values:\n",
    "\n",
    "The mean pseudo R-squared value is 0.8013352083668974. This suggests that approximately 80.13% of the variability in the log count of inscriptions is explained by the log of the population estimate. However, pseudo R-squared values are not directly comparable to R-squared values from linear regression and should be interpreted with caution.\n",
    "\n",
    "The 2.5% and 97.5% percentiles for the pseudo R-squared values give you a 95% confidence interval for the pseudo R-squared value. This gives you a sense of the variability in the goodness-of-fit of your model.\n",
    "\n",
    "Remember, the bootstrap approach provides an empirical estimate of the sampling distribution of your statistics, and these results are specific to your data. Different datasets or different bootstrap samples could give different results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda43c58-216e-4e98-9ee3-2e8794923cbb",
   "metadata": {},
   "source": [
    "## Negative binomial regression with both variables log-transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de7cc4-5e4a-43ba-9a5e-d0370bb84f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform both the population estimates and inscription count\n",
    "# This stabilizes the variance and may make the model better fit the data.\n",
    "grouped_df_sorted['log_urban_context_pop_est'] = np.log(grouped_df_sorted['urban_context_pop_est'])\n",
    "grouped_df_sorted['log_inscription_count'] = np.log(grouped_df_sorted['inscription_count'])\n",
    "\n",
    "# Prepare data for Negative Binomial Regression\n",
    "# Both predictor and response variables are log-transformed\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['log_inscription_count']\n",
    "\n",
    "# Add a constant term to the predictor variable\n",
    "# This is required for the intercept term in the model equation\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit a Negative Binomial model to the data\n",
    "# The family argument specifies that we are using a Negative Binomial model\n",
    "# This model is suitable for over-dispersed count data\n",
    "model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Display the model summary\n",
    "# This will show various statistics that help in understanding the fit and significance of the model\n",
    "print(model.summary())\n",
    "\n",
    "# Generate the fitted values and confidence intervals for plotting\n",
    "mu = model.mu  # Fitted values\n",
    "ci = model.conf_int(alpha=0.05)  # 95% Confidence intervals for the coefficients\n",
    "\n",
    "# Generate the frame to hold the fitted values and confidence intervals\n",
    "frame = pd.DataFrame({\n",
    "    'log_urban_context_pop_est': X['log_urban_context_pop_est'],\n",
    "    'mean': mu,\n",
    "})\n",
    "\n",
    "frame = frame.sort_values('log_urban_context_pop_est')\n",
    "\n",
    "# Calculate the confidence intervals for the mean response\n",
    "frame['mean_ci_lower'] = np.exp(frame['log_urban_context_pop_est'] * ci.iloc[1, 0] + ci.iloc[0, 0])\n",
    "frame['mean_ci_upper'] = np.exp(frame['log_urban_context_pop_est'] * ci.iloc[1, 1] + ci.iloc[0, 1])\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(frame['log_urban_context_pop_est'], frame['mean'], label='Fitted', color='red')\n",
    "\n",
    "# Plot the 95% confidence intervals\n",
    "plt.fill_between(frame['log_urban_context_pop_est'], frame['mean_ci_lower'], frame['mean_ci_upper'], color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Log of Inscription Count')\n",
    "plt.title('Negative Binomial Regression Fit (Log-Log)')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd6b91b-a096-4c0b-8a3f-d43644365848",
   "metadata": {},
   "source": [
    "## Interpretatoin of result - ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a93eb-8555-484f-aa00-98f39839c8af",
   "metadata": {},
   "source": [
    "\n",
    "The large change in pseudo R-squared value upon log-transforming the dependent variable (`inscription_count`) indicates that the new model doesn't explain the variance in the data as well as the previous model did. In simpler terms, the log-log model might not be as suitable for your data as the previous model where only the predictor was log-transformed. \n",
    "\n",
    "Here are some reasons why this could happen:\n",
    "\n",
    "### Loss of Information\n",
    "1. **Non-linearity in Original Scale**: The initial model may have captured a nonlinear relationship on the original scale that becomes less meaningful or even distorted when both variables are log-transformed.\n",
    "\n",
    "2. **Zero or Near-zero Values**: If your inscription count contains zeros or near-zero values, log-transforming those could result in undefined or extremely large/small values that distort the model fit.\n",
    "\n",
    "### Model Suitability\n",
    "3. **Different Underlying Process**: By log-transforming both variables, you're essentially hypothesizing that the relationship between the percentage change in `urban_context_pop_est` and the percentage change in `inscription_count` is linear. If this hypothesis doesn't fit the data well, the model's explanatory power will be lower.\n",
    "\n",
    "4. **Overdispersion**: The initial model used Negative Binomial regression to deal with overdispersion. By log-transforming the dependent variable, you may have inadvertently reduced the model's ability to handle overdispersion, affecting the fit.\n",
    "\n",
    "5. **Outliers or Leverage Points**: Log-transforming can sometimes amplify the effect of outliers or high-leverage points. If such points exist in your `inscription_count`, this could have a large effect on model fit.\n",
    "\n",
    "### Metric Sensitivity\n",
    "6. **Sensitivity of Pseudo R-squared**: It's worth noting that pseudo R-squared values are not as straightforward to interpret as R-squared values in linear regression. They are highly sensitive to model specification and are best used for comparing models on the same data, keeping in mind that they do not provide an absolute measure of goodness-of-fit.\n",
    "\n",
    "Given these considerations, if you observe a drastic reduction in model fit after the log transformation of the dependent variable, it would be worthwhile to further explore the data and possibly consider alternative models or transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6a023-5526-4cce-aa47-786340cb8107",
   "metadata": {},
   "source": [
    "### Reflections: Don't log-transform both variables for NBR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730c30d-133f-4452-84d1-6b8d7beb94f4",
   "metadata": {},
   "source": [
    "Log-transforming the response variable dramatically reduced the power of the relationship (pseudo $R^2$ dropped form 0.4207 to 0.02203). 'Normal negative binomial regressoin seems to work better, where only the predictor variable is log-transformed. ChatGPT suggests that pseudo $R^2$ is a good measure for comparing models (rather than an absolute indication of the model's power). Trying a model where both variable were log-transformed was worthwhile, but the 'normal' negative binomial regression works better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e82308-0e4f-43f1-9948-da9a96400f5d",
   "metadata": {},
   "source": [
    "## Bootstrap approach to OLS regression with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe514a70-f312-4ced-b6f4-999966edb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Initialize an empty list to hold the R-squared values\n",
    "bootstrap_r2 = []\n",
    "\n",
    "# Directory and file to save the bootstrap samples\n",
    "bootstrap_dir = 'bootstrap-samples/modelling/'\n",
    "bootstrap_file = 'ols-pop-inscriptions.npy'\n",
    "bootstrap_path = os.path.join(bootstrap_dir, bootstrap_file)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(bootstrap_dir):\n",
    "    os.makedirs(bootstrap_dir)\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(bootstrap_path):\n",
    "    print(\"Generating bootstrap samples...\")\n",
    "    bootstrap_coefs = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Resample the dataset with replacement\n",
    "        resampled_data = grouped_df_sorted.sample(n=len(grouped_df_sorted), replace=True)\n",
    "        \n",
    "        # Extract predictor and response variables\n",
    "        X = resampled_data['urban_context_pop_est']\n",
    "        y = resampled_data['inscription_count']\n",
    "        \n",
    "        # Add constant for the intercept term\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # Fit OLS model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        # Save coefficients\n",
    "        bootstrap_coefs.append(model.params.tolist())\n",
    "\n",
    "        # Save R-squared value\n",
    "        bootstrap_r2.append(model.rsquared)\n",
    "\n",
    "    # Save bootstrap coefficients and R-squared values to a file\n",
    "    np.savez(bootstrap_path.replace('.npy', '.npz'), coefs=bootstrap_coefs, r2=bootstrap_r2)\n",
    "    print(f\"Saved bootstrap samples to {bootstrap_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Bootstrap samples already exist in {bootstrap_path}. No need to regenerate.\")\n",
    "    saved_data = np.load(bootstrap_path.replace('.npy', '.npz'))\n",
    "    bootstrap_coefs = saved_data['coefs']\n",
    "    bootstrap_r2 = saved_data['r2']\n",
    "\n",
    "# If the file was freshly created or read from disk, we can proceed to analyze bootstrap_coefs\n",
    "\n",
    "# Create a DataFrame for easier handling\n",
    "bootstrap_df = pd.DataFrame(bootstrap_coefs, columns=['const', 'pop_est'])\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = bootstrap_df.describe(percentiles=[.025, .5, .975])\n",
    "print(\"Summary Statistics for Bootstrap Coefficients:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Extract 2.5% and 97.5% percentiles for confidence intervals\n",
    "ci_const = summary_stats.loc[['2.5%', '97.5%'], 'const']\n",
    "ci_pop_est = summary_stats.loc[['2.5%', '97.5%'], 'pop_est']\n",
    "\n",
    "# Generate summary statistics for R-squared values\n",
    "bootstrap_r2_array = np.array(bootstrap_r2)\n",
    "print(\"Summary Statistics for R-squared values:\")\n",
    "print(\"Mean:\", np.mean(bootstrap_r2_array))\n",
    "print(\"Standard Deviation:\", np.std(bootstrap_r2_array))\n",
    "print(\"Minimum:\", np.min(bootstrap_r2_array))\n",
    "print(\"Maximum:\", np.max(bootstrap_r2_array))\n",
    "\n",
    "# Calculate 2.5% and 97.5% percentiles for R-squared confidence intervals\n",
    "ci_r2 = np.percentile(bootstrap_r2, [2.5, 97.5])\n",
    "\n",
    "# Plot histograms for each coefficient and R-squared with confidence intervals\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Intercept\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(bootstrap_df['const'], bins=30, edgecolor='k')\n",
    "plt.axvline(ci_const['2.5%'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(ci_const['97.5%'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Intercept')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Population Estimate\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(bootstrap_df['pop_est'], bins=30, edgecolor='k')\n",
    "plt.axvline(ci_pop_est['2.5%'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(ci_pop_est['97.5%'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Population Estimate')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# R-squared\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(bootstrap_r2, bins=30, edgecolor='k')\n",
    "plt.axvline(ci_r2[0], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(ci_r2[1], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - R-squared')\n",
    "plt.xlabel('R-squared Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e06896-a7e0-4f6b-8e27-691212f428bb",
   "metadata": {},
   "source": [
    "### Reflections on bootrapping\n",
    "\n",
    "Bootstrapping an OLS model produced a somewhat higher R-squared result than an OLS model alone, and provided a better indication of uncertainty. Along with log transformation, consider using it as a secondary step considering the small size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b018cb-82ed-4ae9-b456-e11f5bbe367b",
   "metadata": {},
   "source": [
    "## Exploring alternative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0445ebe-4e94-4571-aa7a-468ff0afe4a7",
   "metadata": {},
   "source": [
    "_Output from ChatGPT_\n",
    "\n",
    "---\n",
    "\n",
    "### Preferred Methods for Highly Dispersed and Small Datasets\n",
    "\n",
    "#### 1. Negative Binomial Regression\n",
    "\n",
    "**Explanation**: The Negative Binomial Regression model extends the Poisson Regression model by introducing an extra parameter to handle overdispersion. Overdispersion occurs when your data exhibits more variability than what the Poisson model can accommodate, which is likely given the high dispersion in your dataset. This model is particularly suitable for count data like the number of inscriptions.\n",
    "\n",
    "**Advantages**: \n",
    "- Can handle overdispersion effectively.\n",
    "- Suitable for count data.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Assumes that counts are independent, which might not always be the case.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Robust Regression\n",
    "\n",
    "**Explanation**: Robust Regression aims to fit a regression model in the presence of corrupt or outlier data points. Unlike Ordinary Least Squares (OLS), which is sensitive to outliers, this method uses alternative loss functions to down-weight the influence of outliers, making it a suitable candidate for your dataset with many outliers.\n",
    "\n",
    "**Advantages**: \n",
    "- Less sensitive to outliers.\n",
    "- Similar to OLS but more robust.\n",
    "\n",
    "**Disadvantages**: \n",
    "- May have lower statistical power compared to OLS when the underlying assumptions of OLS are met (which is less likely in your case).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Generalized Linear Models (GLM)\n",
    "\n",
    "**Explanation**: GLMs extend linear models by allowing for response variables with error distribution models other than a normal distribution. Given that your dataset contains count data, a Poisson or Negative Binomial GLM could be appropriate. The link function in GLM specifies the relationship between the predictor and the mean of the response variable.\n",
    "\n",
    "**Advantages**:\n",
    "- Flexible; can handle different types of response variables.\n",
    "- Good for count data and can adjust for overdispersion.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Requires choosing the correct link function and distribution, which can be data-dependent.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Mixed-Effects Models\n",
    "\n",
    "**Explanation**: These models are particularly useful if there is a grouping variable (e.g., region, time period) that could introduce correlations between observations. By including both fixed effects (like a standard linear regression) and random effects (for the groupings), it can account for this kind of structure in the data.\n",
    "\n",
    "**Advantages**: \n",
    "- Can handle both fixed and random effects.\n",
    "- Useful for dealing with hierarchical or grouped data.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Model interpretation can be complex.\n",
    "- Requires sufficient data within each group for accurate random effects estimation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Spearman's Rank Correlation\n",
    "\n",
    "**Explanation**: Spearman's rank correlation is a non-parametric method that assesses the strength and direction of the relationship between two variables. It does this without making any assumptions about the distribution of the data, making it a robust choice for small datasets with outliers.\n",
    "\n",
    "**Advantages**: \n",
    "- Does not assume a specific distribution or relationship form.\n",
    "- Robust to outliers.\n",
    "\n",
    "**Disadvantages**: \n",
    "- Provides only a measure of association, not a full model.\n",
    "- Limited in capturing complex relationships.\n",
    "\n",
    "---\n",
    "\n",
    "Each of these methods has its own set of advantages and disadvantages, and the best choice may depend on specific characteristics of your dataset and what you find most critical: interpretability, robustness to outliers, or flexibility in modeling different forms of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef01862-8749-424c-8ab2-154c9d9972d9",
   "metadata": {},
   "source": [
    "## Robust regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621eff25-0f1b-42ea-82bf-982dd3d6ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data and fit the robust regression model\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "X = sm.add_constant(X)\n",
    "# Here 'HuberT' specifies the Huber T loss function, which is a commonly used function for robust regression\n",
    "model = sm.RLM(y, X, M=sm.robust.norms.HuberT()).fit()\n",
    "\n",
    "# Generate predictions and calculate the confidence interval for new observations\n",
    "# Note that for robust models, the confidence intervals can be less straightforward to compute\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(X['log_urban_context_pop_est'], predictions, label='Fitted', color='red')\n",
    "\n",
    "# Note: Unlike GLM models, getting the confidence interval for the predictions is less straightforward in robust regression\n",
    "# Therefore, this example doesn't include the confidence interval plotting\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Robust Regression Fit')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d511c1-47b5-467b-8b41-ec40abc39126",
   "metadata": {},
   "source": [
    "### Reflections on Robust regression\n",
    "\n",
    "Low p-values, low standard error compared to coefficient size. After running other models, perhaps return to this one and undertake one of the suggested approaches to measure the strength and quality of the relationship between variables.\n",
    "\n",
    "AIC/BIC: The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are often used for model selection and can be useful for comparing different types of models, though they're not measures of \"fit\" in the way that $R^2$ is. Lower values indicate a model that explains the data better, subject to a penalty for complexity.\n",
    "\n",
    "Residual Analysis: Plotting the residuals versus the fitted values can also provide valuable diagnostic information. In a good model, you'd like these to be randomly scattered with no obvious pattern.\n",
    "\n",
    "Cross-Validation: If your dataset is sufficiently large, you can use k-fold cross-validation to assess how well your model generalizes to new data. This is a robust way to assess model quality, but it may not be suitable for very small datasets.\n",
    "\n",
    "Bootstrapping: Since you've already been working with bootstrapping, you could apply it here as well to get more robust estimates of your coefficients and their confidence intervals.\n",
    "\n",
    "Out-of-Sample Prediction: If you have another dataset, or if you can partition your existing dataset, you can assess how well the model predicts the dependent variable in this out-of-sample data. This can be a very practical way of assessing model quality.\n",
    "\n",
    "Effect Size Measures: In the realm of statistical hypothesis testing, the effect size complements the p-value and gives you an idea of how \"big\" the observed relationship is. Cohen's \n",
    "$f^2$ is a commonly used effect size measure for regression analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae896ec-7b26-4884-b6c7-c925f5da35d2",
   "metadata": {},
   "source": [
    "## Generalised Linear Model assuming a Negative Binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85034e4e-bf53-418c-a868-0b32c4b5eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "\n",
    "# Add a constant term to the independent variable for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit a Generalized Linear Model assuming a Negative Binomial distribution\n",
    "nb_model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Print the summary statistics of the model fit\n",
    "print(nb_model.summary())\n",
    "\n",
    "# Generate predictions and calculate the confidence interval for new observations\n",
    "predictions = nb_model.get_prediction(X)\n",
    "frame = predictions.summary_frame(alpha=0.05)\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(X['log_urban_context_pop_est'], frame['mean'], label='Fitted', color='red')\n",
    "\n",
    "# Plot the 95% confidence intervals\n",
    "plt.fill_between(X['log_urban_context_pop_est'], frame['mean_ci_lower'], frame['mean_ci_upper'], color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Generalized Linear Model (Negative Binomial) Fit')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a367d-5e63-45cf-afb9-fa2d3ae10028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# GLM with Negative Binomial distribution\n",
    "nb_model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "print(\"Negative Binomial Model Summary:\")\n",
    "print(nb_model.summary())\n",
    "\n",
    "# GLM with Poisson distribution\n",
    "poisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n",
    "print(\"\\nPoisson Model Summary:\")\n",
    "print(poisson_model.summary())\n",
    "\n",
    "# GLM with Tweedie distribution\n",
    "# Note: The 'var_power' parameter can be adjusted; 1=Poisson, 2=Gamma, between 1 and 2 allows for Tweedie\n",
    "tweedie_model = sm.GLM(y, X, family=sm.families.Tweedie(var_power=1.5)).fit()\n",
    "print(\"\\nTweedie Model Summary:\")\n",
    "print(tweedie_model.summary())\n",
    "\n",
    "# You can extend this to plot and analyze the residuals and fitted values for each model if you wish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00120f-a51f-4ff1-9eaa-44744ea5f092",
   "metadata": {},
   "source": [
    "## Comparison from ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67facdc3-6fc5-4d8c-ae36-3fb4d3a8c0a0",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the relevant statistics for each model to compare their performance:\n",
    "\n",
    "### Negative Binomial Model\n",
    "- Log-Likelihood: -4256.0\n",
    "- Pseudo R-squared: 0.4207\n",
    "- Deviance: 1781.9\n",
    "- Pearson chi2: 3.26e+03\n",
    "\n",
    "### Poisson Model\n",
    "- Log-Likelihood: -79607.0\n",
    "- Pseudo R-squared: 1.000 (Note: This is likely incorrect, possibly a result of over-dispersion or other issues)\n",
    "- Deviance: 1.5520e+05\n",
    "- Pearson chi2: 3.85e+05\n",
    "\n",
    "### Tweedie Model\n",
    "- Log-Likelihood: -5562.3\n",
    "- Pseudo R-squared: 0.1144\n",
    "- Deviance: 13846.0\n",
    "- Pearson chi2: 3.37e+04\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "1. **Log-Likelihood**: Higher (closer to 0) is better. The Negative Binomial model has the highest log-likelihood, suggesting it fits the data better than the other models.\n",
    "  \n",
    "2. **Pseudo R-squared**: Closer to 1 is better, but take this with caution as it's not directly comparable to R-squared in OLS regression. The Negative Binomial model has a much more reasonable value compared to the Poisson model, whose Pseudo R-squared of 1 is likely not accurate.\n",
    "\n",
    "3. **Deviance and Pearson Chi-squared**: Lower values are generally better. Here, the Negative Binomial model has much lower values compared to the Poisson and Tweedie models, further supporting its superior fit.\n",
    "\n",
    "4. **Convergence & Iterations**: All models have converged, but the number of iterations for the Negative Binomial model is moderate, indicating that the model didn't have trouble fitting the data.\n",
    "\n",
    "5. **Coefficients & Standard Errors**: In all models, the coefficients are significant with a p-value of 0.000. This suggests that `log_urban_context_pop_est` is a significant predictor for `inscription_count` across all models.\n",
    "\n",
    "### Recommendation:\n",
    "Based on these statistics, the **Negative Binomial model** seems to be the best fit for your data among the three. It has the highest log-likelihood, a reasonable pseudo R-squared, and the lowest deviance and Pearson chi-squared statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7b0fc-e16b-49cc-a057-4199bdc331f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have two NumPy arrays:\n",
    "# log_urban_context_pop_est: the log of population estimates in urban context\n",
    "# inscription_count: the count of inscriptions\n",
    "log_urban_context_pop_est = np.array(grouped_df_sorted['log_urban_context_pop_est'])\n",
    "inscription_count = np.array(grouped_df_sorted['inscription_count'])\n",
    "\n",
    "# Perform Spearman's Rank Correlation\n",
    "spearman_corr, p_value = stats.spearmanr(log_urban_context_pop_est, inscription_count)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Spearman's Rank Correlation Coefficient: {spearman_corr}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Optional: Create a scatter plot for visualization\n",
    "plt.scatter(log_urban_context_pop_est, inscription_count)\n",
    "plt.xlabel('Log of Population Estimate in Urban Context')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Scatter Plot of Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbc7d0-6f8a-454b-9a26-11c004b449cb",
   "metadata": {},
   "source": [
    "## To-do: residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c4c76",
   "metadata": {},
   "source": [
    "Hanson (2021, 147-149; figure 7.5 and 7.6) analyses residuals to compare the expected value from the observed value at each site. Reproduce. '[A]n interesting avenue for future work might be to investigate the distribution of the residuals of this and similar relationships in more detail' (148).Along with the &Delta-T of the inscriptions dates from a locale, residuals could offer insights into the nature of the epigraphic record and help place it in the larger context of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ba0a9-25f3-478a-952b-7284e95b0200",
   "metadata": {},
   "source": [
    "# SPA using letter counts\n",
    "\n",
    "The LIRE dataset, imported as the variable 'lire', contains a column called 'clearn_text_conservative'. We will use that column and count the number of letters in it, excluding spaces and special characters. The letter count will serve as a proxy for the amount of information contained in the inscription. This additional analysis is suggested in Hanson's article about inscription counts and populations (citation needed - see above).\n",
    "\n",
    "## First, view 50 rows of 'clean_text_conservative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e7655-8892-4e55-b222-2195421ecf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum column width to None for unrestricted display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Now display the first 50 rows of 'clean_text_conservative'\n",
    "print(lire['clean_text_conservative'].head(100))\n",
    "\n",
    "# Reset the maximum column width to 50 characters\n",
    "# pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d1cd6-a486-4cd0-a19a-d3ece66093d3",
   "metadata": {},
   "source": [
    "## Count letters in 'clean_text_conservative'\n",
    "\n",
    "Append counts in new column called 'letter_count_ctc'\n",
    "\n",
    "### Latin-speaking empire, excluding Roma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785a33c-d757-43f2-a210-f07a5b6bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dataset\n",
    "# lire = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Define the alphabets for Latin and a more comprehensive set for Ancient Greek\n",
    "latin_alphabet = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "greek_alphabet = set(\"αβγδεζηθικλμνξοπρστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩάέήίόύώϊϋΐΰἀἁἂἃἄἅἆἇἈἉἊἋἌἍἎἏἐἑἒἓἔἕἘἙἚἛἜἝἠἡἢἣἤἥἦἧἨἩἪἫἬἭἮἯἰἱἲἳἴἵἶἷἸἹἺἻἼἽἾἿὀὁὂὃὄὅὈὉὊὋὌὍὐὑὒὓὔὕὖὗὙὛὝὟὠὡὢὣὤὥὦὧὨὩὪὫὬὭὮὯὰὲὴὶὸὺὼᾀᾁᾂᾃᾄᾅᾆᾇᾈᾉᾊᾋᾌᾍᾎᾏᾐᾑᾒᾓᾔᾕᾖᾗᾘᾙᾚᾛᾜᾝᾞᾟᾠᾡᾢᾣᾤᾥᾦᾧᾨᾩᾪᾫᾬᾭᾮᾯ\")\n",
    "\n",
    "def count_letters(text):\n",
    "    if text is None:\n",
    "        return 0\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if char in latin_alphabet or char in greek_alphabet:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Apply the function to the 'clean_text_conservative' column\n",
    "lire['letter_count_ctc'] = lire['clean_text_conservative'].apply(count_letters)\n",
    "\n",
    "# Show the first few rows to verify\n",
    "# print(lire.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bebad44-2d36-45d9-8d60-9e2cf4693e74",
   "metadata": {},
   "source": [
    "## Letter count descriptive statistics and other characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a3eb5-c238-4456-a057-95fac33cb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'lire' is your DataFrame and 'letter_count_ctc' is the column you are interested in\n",
    "desc_stats = lire['letter_count_ctc'].describe()\n",
    "\n",
    "# Descriptive statistics\n",
    "\n",
    "print(\"Descriptive Statistics for 'letter_count_ctc':\")\n",
    "print(desc_stats)\n",
    "\n",
    "# Debugging \n",
    "\n",
    "# Count the number of 0-letter inscriptions\n",
    "zero_letter_count = lire['letter_count_ctc'].value_counts().get(0, 0)\n",
    "print(f\"Number of rows where 'letter_count_ctc' is 0: {zero_letter_count}\")\n",
    "\n",
    "# Count the number of null values in 'letter_count_ctc'\n",
    "num_null_values = lire['letter_count_ctc'].isna().sum()\n",
    "print(f\"The number of null values in 'letter_count_ctc' is: {num_null_values}\")\n",
    "\n",
    "# Count the number of negative values in 'letter_count_ctc'\n",
    "negative_count = len(lire[lire['letter_count_ctc'] < 0])\n",
    "print(f\"Number of negative values in 'letter_count_ctc': {negative_count}\")\n",
    "\n",
    "# Create a histogram\n",
    "\n",
    "# Filter out rows where 'letter_count_ctc' is zero if needed\n",
    "filtered_lire = lire[lire['letter_count_ctc'] > 0]\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean_letter_count = np.mean(filtered_lire['letter_count_ctc'])\n",
    "std_dev_letter_count = np.std(filtered_lire['letter_count_ctc'])\n",
    "\n",
    "# Compute the median letter count\n",
    "median_letter_count = filtered_lire['letter_count_ctc'].median()\n",
    "\n",
    "# Find the maximum value in 'letter_count_ctc'\n",
    "max_count = lire['letter_count_ctc'].max()\n",
    "print(f\"The highest count in 'letter_count_ctc' is: {max_count}\")\n",
    "\n",
    "# Sum the values in 'letter_count_ctc'\n",
    "total_letters = lire['letter_count_ctc'].sum()\n",
    "print(f\"The total number of letters in the dataset is: {total_letters}\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(filtered_lire['letter_count_ctc'], bins=range(0, int(filtered_lire['letter_count_ctc'].max()) + 3, 3), edgecolor='black')\n",
    "plt.xlabel('Letter Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Letter Counts in 3-character bins')\n",
    "\n",
    "# Set x-axis limits\n",
    "plt.xlim([0, 250])  # Change these numbers as you see fit\n",
    "\n",
    "# Add lines for the mean and standard deviation\n",
    "plt.axvline(mean_letter_count, color='r', linestyle='dashed', linewidth=1, label=f\"Mean: {mean_letter_count:.2f}\")\n",
    "plt.axvline(mean_letter_count - std_dev_letter_count, color='g', linestyle='dashed', linewidth=1, label=f\"1 SD Below Mean\")\n",
    "plt.axvline(mean_letter_count + std_dev_letter_count, color='g', linestyle='dashed', linewidth=1, label=f\"1 SD Above Mean\")\n",
    "\n",
    "# Add line for the median\n",
    "plt.axvline(median_letter_count, color='b', linestyle='dashed', linewidth=1, label=f\"Median: {median_letter_count:.2f}\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f103b0-0225-4e29-8014-20f159121f6a",
   "metadata": {},
   "source": [
    "## SPA weighted using letter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b3d4d-aa27-49e2-a40e-7ac4432aff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins,\n",
    "    weighted by the number of letters in each inscription.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges and letter counts for each inscription.\n",
    "          The DataFrame is expected to have columns 'not_before', 'not_after', and 'letter_count_ctc'.\n",
    "          \n",
    "    - earliest_date, latest_date: The date limits for the distribution, \n",
    "                                  used for initializing the summed probability array.\n",
    "                                  \n",
    "    - resolution: The bin size for the histogram (in years). \n",
    "                  This determines the granularity of the summed probability array.\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: The summed probability distribution, \n",
    "                   where each element represents the summed probability for a bin.\n",
    "                   \n",
    "    - std_devs: The standard deviations for each bin, \n",
    "                calculated from the variances of the probabilities that contribute to each bin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an array of zeros for storing summed probabilities\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    \n",
    "    # Initialize an array of zeros for storing the sum of the variances\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    # Loop through each row (inscription) in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the date range and the letter count of the inscription\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        letter_count = row['letter_count_ctc']\n",
    "        \n",
    "        # Convert the date range to index range in the summed_prob array\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Ensure the indices are within bounds\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Compute the uniform probability across the date range for the inscription\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute the probability across the relevant bins, weighted by the letter count\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            weighted_prob = uniform_prob * letter_count\n",
    "            summed_prob[i] += weighted_prob\n",
    "            \n",
    "    # Compute variance for each bin based on the final summed_prob\n",
    "    variance = summed_prob * (1 - summed_prob / np.sum(summed_prob))\n",
    "    \n",
    "    # Make sure no negative values exist due to numerical errors\n",
    "    # variance[variance < 0] = 0  \n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_devs = np.sqrt(variance)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "# Function to run the analysis\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... [compute_summed_probability stays the same] ...\n",
    "\n",
    "def run_analysis(df, bootstrap_samples_dir, file_name):\n",
    "    \"\"\"\n",
    "    Analyzes a dataset of inscriptions to calculate and visualize a summed probability distribution.\n",
    "    Bootstrapping is used to calculate the 95% confidence interval. The results are displayed as a bar graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing date ranges and letter counts for each inscription.\n",
    "          Expected columns: 'not_before', 'not_after', 'letter_count_ctc'\n",
    "    - bootstrap_samples_dir: Directory where bootstrap samples will be saved.\n",
    "    - file_name: A unique name to identify and save the bootstrap samples file.\n",
    "    \n",
    "    Returns:\n",
    "    None. The function generates a bar graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the lower and upper limits for the x-axis in the plot\n",
    "    X_LIM_LOW = -100\n",
    "    X_LIM_HIGH = 600\n",
    "    \n",
    "    # Initialize parameters\n",
    "    n_iterations = 1000  # Number of bootstrap iterations\n",
    "    earliest_date, latest_date = df['not_before'].min(), df['not_after'].max()  # Get the earliest and latest dates from the DataFrame\n",
    "    resolution = 5  # Bin size for histogram in years\n",
    "    window_size = 5  # Size of moving window for moving average\n",
    "    \n",
    "    # Create new directory for storing bootstrap samples if it doesn't already exist\n",
    "    os.makedirs(bootstrap_samples_dir, exist_ok=True)\n",
    "\n",
    "    # Define the saved_sample_path based on the directory and file name\n",
    "    saved_sample_path = os.path.join(bootstrap_samples_dir, f\"{file_name}.npy\")\n",
    "\n",
    "    # Calculate the number of inscriptions in the DataFrame\n",
    "    n_inscriptions = df.shape[0]\n",
    "    \n",
    "# Check if a bootstrap sample file already exists for this file_name\n",
    "    if os.path.exists(saved_sample_path):\n",
    "        print(f\"Loading bootstrap sample file for: {file_name}...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "    else:\n",
    "        # If no bootstrap sample exists, generate a new one\n",
    "        print(f\"Calculating bootstrap samples for: {file_name}...\")\n",
    "        bootstrap_summed_probs = np.zeros((n_iterations, int((latest_date - earliest_date) / resolution)))\n",
    "        for i in tqdm(range(n_iterations)):\n",
    "            bootstrap_sample = df.sample(n=len(df), replace=True)\n",
    "            bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "            bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "        \n",
    "        # Save the newly generated bootstrap samples\n",
    "        print(f\"Saving bootstrap sample file for: {file_name}...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "        \n",
    "    # Calculate the lower and upper bounds for the 95% confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "    \n",
    "    # Calculate the summed probability for the DataFrame\n",
    "    summed_prob, _ = compute_summed_probability(df, earliest_date, latest_date, resolution)\n",
    "    \n",
    "    # Generate x-values for plotting\n",
    "    x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "\n",
    "    # Calculate moving averages\n",
    "    moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "    moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "    moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "    \n",
    "    # Plotting\n",
    "    plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "    plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "        \n",
    "    plt.title(f\"Summed Probability Distribution (n={n_inscriptions})\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.xlim([X_LIM_LOW, X_LIM_HIGH])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# End function to run the analysis\n",
    "\n",
    "# For only Latin-speaking provinces excluding Roma\n",
    "print(\"Running analysis for letter counts, clearn text conservative, Latin-speaking provinces excluding Roma...\")\n",
    "bootstrap_samples_dir_3 = \"bootstrap-samples/spa-letter-count/conservative/\"\n",
    "file_name_3 = \"latin-speaking-no-roma\"\n",
    "lire_3 = lire[(lire['province'] != 'Roma') & (lire['province_language'] == 'Latin')]\n",
    "filtered_lire_3 = lire_3[lire_3['letter_count_ctc'] > 0]\n",
    "min_value = filtered_lire_3['letter_count_ctc'].min()\n",
    "print(f\"The minimum value in the 'letter_count_ctc' column is {min_value}\") # debugging negative sq root problem\n",
    "run_analysis(filtered_lire_3, bootstrap_samples_dir_3, file_name_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8974b",
   "metadata": {},
   "source": [
    "## SPA on word counts by municipality\n",
    "\n",
    "Restrict to Latin speaking provinces excluding Roma\n",
    "Run by urban_context_city \n",
    "Use only records where urban_context_pop_est is not NaN \n",
    "Rename each bootstrap file (file_name_3) to [municipality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins,\n",
    "    weighted by the number of letters in each inscription.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges and letter counts for each inscription.\n",
    "          The DataFrame is expected to have columns 'not_before', 'not_after', and 'letter_count_ctc'.\n",
    "          \n",
    "    - earliest_date, latest_date: The date limits for the distribution, \n",
    "                                  used for initializing the summed probability array.\n",
    "                                  \n",
    "    - resolution: The bin size for the histogram (in years). \n",
    "                  This determines the granularity of the summed probability array.\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: The summed probability distribution, \n",
    "                   where each element represents the summed probability for a bin.\n",
    "                   \n",
    "    - std_devs: The standard deviations for each bin, \n",
    "                calculated from the variances of the probabilities that contribute to each bin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an array of zeros for storing summed probabilities\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    \n",
    "    # Initialize an array of zeros for storing the sum of the variances\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    # Loop through each row (inscription) in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the date range and the letter count of the inscription\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        letter_count = row['letter_count_ctc']\n",
    "        \n",
    "        # Convert the date range to index range in the summed_prob array\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Ensure the indices are within bounds\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Compute the uniform probability across the date range for the inscription\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute the probability across the relevant bins, weighted by the letter count\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            weighted_prob = uniform_prob * letter_count\n",
    "            summed_prob[i] += weighted_prob\n",
    "            \n",
    "    # Compute variance for each bin based on the final summed_prob\n",
    "    variance = summed_prob * (1 - summed_prob / np.sum(summed_prob))\n",
    "    \n",
    "    # Make sure no negative values exist due to numerical errors\n",
    "    # variance[variance < 0] = 0  \n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_devs = np.sqrt(variance)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "# Function to run the analysis\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... [compute_summed_probability stays the same] ...\n",
    "\n",
    "# Function to loop over each unique city\n",
    "def run_analysis(df, bootstrap_samples_dir):\n",
    "    # Get the unique cities in the DataFrame, remove None and sort them alphabetically\n",
    "    cities = sorted([city for city in df['urban_context_city'].unique() if city is not None])\n",
    "\n",
    "    # Loop over each city\n",
    "    for city in cities:\n",
    "        # Filter the DataFrame to only include rows where 'urban_context_city' equals the current city\n",
    "        # and 'urban_context_pop_est' is not null\n",
    "        df_city = df[(df['urban_context_city'] == city) & (df['urban_context_pop_est'].notnull())]\n",
    "\n",
    "        # Skip this city if the DataFrame is empty or has fewer than 100 inscriptions\n",
    "        if df_city.empty or len(df_city) < 100:\n",
    "            continue\n",
    "\n",
    "        # Call compute_summed_probability and run_analysis for the filtered DataFrame\n",
    "        file_name = city.replace(' ', '_')  # Replace spaces with underscores in the file name\n",
    "        print(f\"Running analysis for city: {city}...\")\n",
    "        run_analysis_single_city(df_city, bootstrap_samples_dir, file_name, city)\n",
    "\n",
    "def run_analysis_single_city(df, bootstrap_samples_dir, file_name, city_name):\n",
    "    \"\"\"\n",
    "    Analyzes a dataset of inscriptions to calculate and visualize a summed probability distribution.\n",
    "    Bootstrapping is used to calculate the 95% confidence interval. The results are displayed as a bar graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing date ranges and letter counts for each inscription.\n",
    "          Expected columns: 'not_before', 'not_after', 'letter_count_ctc'\n",
    "    - bootstrap_samples_dir: Directory where bootstrap samples will be saved.\n",
    "    - file_name: A unique name to identify and save the bootstrap samples file.\n",
    "    \n",
    "    Returns:\n",
    "    None. The function generates a bar graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the lower and upper limits for the x-axis in the plot\n",
    "    X_LIM_LOW = -100\n",
    "    X_LIM_HIGH = 600\n",
    "    \n",
    "    # Initialize parameters\n",
    "    n_iterations = 1000  # Number of bootstrap iterations\n",
    "    earliest_date, latest_date = df['not_before'].min(), df['not_after'].max()  # Get the earliest and latest dates from the DataFrame\n",
    "    resolution = 5  # Bin size for histogram in years\n",
    "    window_size = 5  # Size of moving window for moving average\n",
    "    \n",
    "    # Get the unique cities in the DataFrame\n",
    "    cities = df['urban_context_city'].unique()\n",
    "\n",
    "    # Create new directory for storing bootstrap samples if it doesn't already exist\n",
    "    os.makedirs(bootstrap_samples_dir, exist_ok=True)\n",
    "\n",
    "    # Define the saved_sample_path based on the directory and file name\n",
    "    saved_sample_path = os.path.join(bootstrap_samples_dir, f\"{file_name}.npy\")\n",
    "\n",
    "    # Calculate the total number of inscriptions and letters\n",
    "    n_inscriptions = len(df)\n",
    "    n_letters = df['letter_count_ctc'].sum()\n",
    "    \n",
    "# Check if a bootstrap sample file already exists for this file_name\n",
    "    if os.path.exists(saved_sample_path):\n",
    "        print(f\"Loading bootstrap sample file for city: {city_name}...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "    else:\n",
    "        # If no bootstrap sample exists, generate a new one\n",
    "        print(f\"Calculating bootstrap samples for city: {city_name}...\")\n",
    "        bootstrap_summed_probs = np.zeros((n_iterations, int((latest_date - earliest_date) / resolution)))\n",
    "        for i in tqdm(range(n_iterations)):\n",
    "            bootstrap_sample = df.sample(n=len(df), replace=True)\n",
    "            bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "            bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "        \n",
    "        # Save the newly generated bootstrap samples\n",
    "        print(f\"Saving bootstrap sample file for: {file_name}...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "        \n",
    "    # Calculate the lower and upper bounds for the 95% confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "    \n",
    "    # Calculate the summed probability for the DataFrame\n",
    "    summed_prob, _ = compute_summed_probability(df, earliest_date, latest_date, resolution)\n",
    "    \n",
    "    # Generate x-values for plotting\n",
    "    x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "\n",
    "    # Calculate moving averages\n",
    "    moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "    moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "    moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "    \n",
    "    # Plotting\n",
    "    plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "    plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "        \n",
    "    plt.title(f\"Summed Probability Distribution for {city_name} ({n_inscriptions} inscriptions | {n_letters} letters)\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.xlim([X_LIM_LOW, X_LIM_HIGH])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# End function to run the analysis\n",
    "\n",
    "# For only Latin-speaking provinces excluding Roma, now by city\n",
    "print(\"Running analysis for letter counts, clearn text conservative, Latin-speaking provinces excluding Roma, by city...\")\n",
    "bootstrap_samples_dir_3 = \"bootstrap-samples/spa-letter-count/conservative/\"\n",
    "lire_3 = lire[(lire['province'] != 'Roma') & (lire['province_language'] == 'Latin')]\n",
    "filtered_lire_3 = lire_3[lire_3['letter_count_ctc'] > 0]\n",
    "min_value = filtered_lire_3['letter_count_ctc'].min()\n",
    "print(f\"The minimum value in the 'letter_count_ctc' column is {min_value}\") # debugging negative sq root problem\n",
    "run_analysis(filtered_lire_3, bootstrap_samples_dir_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85d4b3",
   "metadata": {},
   "source": [
    "I think that city names may be misaligned with analysis. Outputting a filtered list of city names and inscription / letter counts to check.\n",
    "\n",
    "OK, city names match (almost) with inscription and (exactly) with word count, so I think we are good, except that soem of the inscription counts are off by one or two. I think it is likely from the way that we are filtering / dealing with inscriptions with zero words or maybe zero date range.\n",
    "\n",
    "To do: follow up on minor discrepencies in inscription count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for Latin-speaking provinces, cities with population estimates\n",
    "df_filtered = lire[(lire['province_language'] == 'Latin') & \n",
    "                 (lire['urban_context_pop_est'].notnull())]\n",
    "\n",
    "# Group the filtered DataFrame by 'urban_context_city' and calculate the count and sum of 'letter_count_ctc' for each city\n",
    "city_counts = df_filtered.groupby('urban_context_city').agg({\n",
    "    'letter_count_ctc': ['count', 'sum']\n",
    "})\n",
    "\n",
    "# Filter for cities with over 100 inscriptions\n",
    "city_counts = city_counts[city_counts[('letter_count_ctc', 'count')] > 100]\n",
    "\n",
    "# Print the city names, inscription counts, and letter counts\n",
    "print(city_counts)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "# 'index=True' will include the index ('urban_context_city') in the CSV\n",
    "city_counts.to_csv('city_counts.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1d245",
   "metadata": {},
   "source": [
    "## Negative Binomial Regression (city population vs. letter counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f72aea",
   "metadata": {},
   "source": [
    "Extending Hanson 2021 again, running the same negative binomial regression as above, but this time using letter counts instead of inscription counts versus city population estimates from independent proxies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557709e0",
   "metadata": {},
   "source": [
    "Repeating the filtering of LIRE, grouping, and ordering. Already performed once above, will need to be turned into a function for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only Latin-speaking provinces and exclude the city 'Roma'\n",
    "latin_provinces_df = lire[(lire['province_language'] == 'Latin') & (lire['urban_context_city'] != 'Roma')]\n",
    "\n",
    "# Generate a list of unique cities in Latin-speaking provinces that have population estimates\n",
    "unique_cities_with_pop = latin_provinces_df[(latin_provinces_df['urban_context_city'] != 'None') & (latin_provinces_df['urban_context_pop_est'].notna())]['urban_context_city'].unique()\n",
    "\n",
    "# Group by 'urban_context_city' and 'urban_context_pop_est', then count the number of inscriptions and sum the letter counts for each city\n",
    "grouped_df = latin_provinces_df[(latin_provinces_df['urban_context_city'] != 'None') & (latin_provinces_df['urban_context_pop_est'].notna())].groupby(['urban_context_city', 'urban_context_pop_est']).agg({'your_letter_count_column': 'sum', 'your_inscription_id_column': 'count'}).reset_index().rename(columns={'your_letter_count_column': 'letter_count', 'your_inscription_id_column': 'inscription_count'})\n",
    "\n",
    "# Sort the DataFrame by 'urban_context_pop_est' in descending order\n",
    "grouped_df_sorted = grouped_df.sort_values(by='urban_context_pop_est', ascending=False)\n",
    "\n",
    "# Display the DataFrame in a tight layout\n",
    "print(\"\\nTop 10 cities in Latin-speaking provinces sorted by population estimate, along with their inscription counts and total letter counts:\")\n",
    "print(grouped_df_sorted.head(10).to_string(index=False, col_space=12))\n",
    "\n",
    "# Display the total number of records in the grouped DataFrame\n",
    "total_records_grouped = grouped_df_sorted.shape[0]\n",
    "print(f\"\\nTotal number of records in the DataFrame: {total_records_grouped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c90bf",
   "metadata": {},
   "source": [
    "Calculating the negative binomial regression of population estimate versus letter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb39e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['letter_count_ctc']  # Use 'letter_count_ctc' instead of 'inscription_count'\n",
    "\n",
    "# Add a constant term to the independent variable for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit a Generalized Linear Model assuming a Negative Binomial distribution\n",
    "nb_model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Print the summary statistics of the model fit\n",
    "print(nb_model.summary())\n",
    "\n",
    "# Generate predictions and calculate the confidence interval for new observations\n",
    "predictions = nb_model.get_prediction(X)\n",
    "frame = predictions.summary_frame(alpha=0.05)\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(X['log_urban_context_pop_est'], frame['mean'], label='Fitted', color='red')\n",
    "\n",
    "# Plot the 95% confidence intervals\n",
    "plt.fill_between(X['log_urban_context_pop_est'], frame['mean_ci_lower'], frame['mean_ci_upper'], color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Letter Count')  # Change the y-axis label to 'Letter Count'\n",
    "plt.title('Generalized Linear Model (Negative Binomial) Fit')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8796e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
