{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918911a7-e881-464f-80a8-08cb1dde88e0",
   "metadata": {},
   "source": [
    "# A Summed Probability Analysis of inscriptions from the Latin-speaking Roman Empire\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The availability of large, machine-readable epigraphic datasets opens new avenues of research to historians and archaeologists. \n",
    "\n",
    "This project aims to apply summed probability analysis (SPA) to Latin inscriptions. For decades, SPA has been applied to radiocarbon dates in archaeology. The idea behind SPA is that each date represtents data about human occupation, and so tallying them provides some insights into demographic changes (i.e., more radiocarbon dates equals more people). Problems with SPA using radiocarbon dates are addressed through use of the large datasets and thoughtful statistical approaches. \n",
    "\n",
    "Like radiocarbon dates, inscriptions provide a data-point: a place and a date of human activity. Also like radiocarbon dates, inscription dates come with temporal uncertainty. They are also subject to the vagaries of preservation. Inscriptions have the advantage, however, of directly representing a human act in the past (the placing of an inscription), not just the taking of a sample by a modern researcher that may or may not be related to past human activity. \n",
    "\n",
    "## Purpose\n",
    "\n",
    "To test the use of inscriptions counts as a proxy for population (when compared to other proxies). If the approaches looks feasible, estimate relative populations of various cities, provinces, and the empire as a whole over time.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used for this analysis is Latin Inscriptions of the Roman Empire (LIRE; https://doi.org/10.5281/zenodo.8147298), which has been compiled and cleaned from various digital inscription corpora by the SDAM group at Aarhus University, Denmark. It contains some 182,852 inscriptions dating from 50 BC - AD 350. \n",
    "\n",
    "If this analysis proves successful, I will consider expanding it to the Latin Inscriptions in Space and Time (LIST) dataset, which requires additional cleaning but extends the data forward past AD 350, perhaps helping to clarify 4th-century trends.\n",
    "\n",
    "## Approach\n",
    "\n",
    "After some initial (minor) cleaning and the addition of a 'date_range' column (subtracting 'not_before' from 'not_after' for each inscription) and a 'province_language' column (to identify primarily Latin-speaking provinces), a series of statistical analyses are undertaken, informed by the following considerations.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* Like radiocarbon dates, inscriptions are associated with an occupation event of some kind - they commemorate human action in the past.\n",
    "* 'The suite of dates from a site constitute a representative sample of occupation events at that site' (Williams 2011, 579)\n",
    "\n",
    "### Key elements of the approach\n",
    "\n",
    "Key elements of the approach are borrowed from SPA on radiocarbon dates\n",
    "\n",
    "* Determining necessary sample size. As a starting point, Williams (2011) suggests 500 radiocarbon dates minimum, based on subsampling the full dataset and then calculating RMSE.\n",
    "* Determining and expressing error margins. Williams suggests using mean of the standard deviations of date-ranges.\n",
    "* Use moving average trendlines to accommodate errors and uncertainties. Williams suggests 500-800 years for long-term prehistoric analysis.\n",
    "* Use all dates that are not anamolous (regardless of uncertainty - but confirm this approach through exploration of the dataset first).\n",
    "* Instead of dividing by bioregion, devide by political entity (city, province). Mitigates need to examine range of archaeological site types, although other subdivisions need to be considered (e.g., Latin- versus Greek-speaking provinces; Rome as an anomoly).\n",
    "* Characterise patters - rather than most common dates and site types, look at relative population over time in various provinces, cities, and the Latin-speaking empire as a whole (with and without Rome).\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Note that some of the limitations of SPA for radiocarbon dates apply to inscriptions and some do not. \n",
    "\n",
    "* Calibration effects do not apply (although some thought needs to be given to what sort of distribution - uniform or trapezoidal? - should be used).\n",
    "* Taphonomic loss may be present in inscriptions, but given the restricted analysis range (ca. 100 BC - AD 400), I am discounting such loss.\n",
    "* Intra-site sampling biases should be considered, but are probably less acute. Radiocarbon dates are created by archaeologists, and bracket particular phenomena rather than forming a representative sample of occupation at a site. Inscriptions, on the other hand, commemorate human activities of various kinds directly, and while survival is not assured, when they exist, the represent something initiated in antiquity, not a subjective assertion by a researcher of something happening in the past. In this sense, inscriptions are one step less removed from the reality of the past. Radiocarbon dates also depend on the survival of suitable materials for dating. Inscriptions on stone are durable and survive in large numbers, without the bias introduced by requiring the survival of organic materials.\n",
    "* Comparison with other proxies still needed; Hansen provides them.\n",
    "\n",
    "### Statistical analyses\n",
    "\n",
    "#### Dataset exploration\n",
    "\n",
    "* Generate descriptive statistics of date ranges, including inscription counts under certain date-range thresholds, to get an idea of how much termporal uncertainty exists in the dataset.\n",
    "* Explore the minimum sample size needed for SPA when examining spatially, temporally, or otherwise limited subsets of the data.\n",
    "* Run SPA of the entire dataset, including under various date-range thresholds and sample sizes, to shed light on the trade-offs between sample size and temporal uncertainty.\n",
    "\n",
    "#### SPA on inscription and letter counts for various subsets of data\n",
    "* Latin-speaking empire with and without Roma\n",
    "* Each province\n",
    "* Each urban area\n",
    "* Reproduce Hanson's results showing statistically significant relationship between independent calculation of urban area population and inscription count.\n",
    "* Re-run the SPA using letter count, not just incsription count, to test Hanson's information content theory explaining the relationship between population and inscription count.\n",
    "* Check to see if the letter count approach correlates more or less than the inscription count approach to population.\n",
    "* Calculate urban area, province, and empire population changes between AD 1 - 300.\n",
    "\n",
    "## Results\n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad1030-a1a4-4475-a2b4-d844cebdc602",
   "metadata": {},
   "source": [
    "*ChatGPT-4 used throughout to generate code snippets*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60cd32-d760-4749-936b-833ef8d8f7cd",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799f33b-81f6-4e78-88e2-d68d2d417dca",
   "metadata": {},
   "source": [
    "## Install libraries\n",
    "\n",
    "geopandas, fastparquet, numpy, others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849fcaf-00e4-4382-93d5-1ec9f6c9bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in jupyter notebooks, normal install using !pip install doesn't work, use the solution from:\n",
    "# https://www.geeksforgeeks.org/install-python-package-using-jupyter-notebook/\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install geopandas\n",
    "!{sys.executable} -m pip install fastparquet\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install tempun\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc95b8-b28f-4feb-a48e-2ccc3edad2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fastparquet\n",
    "import tempun # Vojtek's code to model temporal uncertainty\n",
    "import random\n",
    "import os # for saving and loading bootstrap samples\n",
    "import re # for regular expressions to fix bootsrtap filenames\n",
    "import hashlib # for generating hashes for bootstrap sample filenames\n",
    "import statsmodels.api as sm # For OLS regression and other statistics\n",
    "import scipy.stats as stats # For Spearman's Rank Correlation and non-normal distributions\n",
    "import pickle  # Importing pickle for serialization\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.ndimage import uniform_filter1d  # For more efficient moving average calculation\n",
    "from tqdm.auto import tqdm # For adding a progress bar to lengthy operations; tqdm.aut automatically adapts to the environment it is run it to provide a GUI-based progress bar where possible\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std # to compute the prediction standard errors \n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial # for bootstrapped Negative Binomial Regresson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf5df7-697e-4c3c-9886-ff16fca504e9",
   "metadata": {},
   "source": [
    "## Import LIRE_v2.3 dataset \n",
    "\n",
    "**Source**: https://zenodo.org/record/8147298\n",
    "\n",
    "**DOI**: https://doi.org/10.5281/zenodo.8147298 \n",
    "\n",
    "**Version**: 2.3\n",
    "\n",
    "**Format**: parquet\n",
    "\n",
    "**License**: CC-BY-4.\n",
    "\n",
    "**Cite as**: Kaše, Vojtěch, Heřmánková, Petra, & Sobotková, Adéla. (2023). LIRE (v2.3) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.8147298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21834c-d8f9-4d11-bcdd-ef64baf5e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lire = gpd.read_parquet('data/LIRE_v2-3.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b156d-4233-4168-9770-a8014ac5fb74",
   "metadata": {},
   "source": [
    "### Display first five rows of LIRE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823a26c-2bdb-4bdd-ba9e-d70a033cb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lire.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111e418-5c89-4eaa-b4c6-bb14c4ce3660",
   "metadata": {},
   "source": [
    "### Print column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffc928-29ef-4fbb-b753-4f860dfc5888",
   "metadata": {},
   "source": [
    "Also set column names as a variable so that it can be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8994a-0e2b-434f-952f-213febd2a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lire_columns = lire.columns\n",
    "print(\"\\n\".join(lire_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1aa61a-da0d-4b16-92cd-1443cf078c93",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831d9e1-f996-4804-a4d5-66d493a83964",
   "metadata": {},
   "source": [
    "The quality of the dataset is good, but I found some errors where the date range is negative. Vojtech suggests reversing the dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba67b65-5461-487b-a5a4-b162b838f80d",
   "metadata": {},
   "source": [
    "### Identify records with a date range less than zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2c502-46ca-4565-b0e1-a7157fb1ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where date_range is less than 0\n",
    "error_rows = lire[lire['not_before'] > lire['not_after']]\n",
    "\n",
    "# Display only specific columns for the filtered records\n",
    "print(error_rows[['LIST-ID', 'raw_dating', 'not_before', 'not_after']])\n",
    "\n",
    "error_rows.to_csv(\"error_rows\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e5e80-f54b-44db-a7ab-e42b11d0f02e",
   "metadata": {},
   "source": [
    "### Swap the 'not_before' and 'not_after' values to resolve error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0853a69-18b3-4eae-873f-7e8e5b4845f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to swap 'not_before' and 'not_after' when 'not_before' > 'not_after'\n",
    "def swap_dates(row):\n",
    "    if row['not_before'] > row['not_after']:\n",
    "        row['not_before'], row['not_after'] = row['not_after'], row['not_before']\n",
    "    return row\n",
    "\n",
    "# Apply the function to swap the dates\n",
    "lire = lire.apply(swap_dates, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c60795-5846-4f48-9e6f-1301665ae97d",
   "metadata": {},
   "source": [
    "## Add ``date_range`` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206b6d3-eb76-4ae7-9283-d8808cedcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'date_range' column already exists\n",
    "if 'date_range' not in lire.columns:\n",
    "    # If it doesn't exist, calculate date range for each inscription and add it to the LIRE dataframe\n",
    "    lire['date_range'] = lire['not_after'] - lire['not_before']\n",
    "    # Show the updated DataFrame to check that the new column has been added successfully\n",
    "    print(\"date_range column added:\")\n",
    "    # print(lire.head()) # Option to print the first few records of the dataset to confirm\n",
    "else:\n",
    "    print(\"date_range column already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e11a8-0266-42ce-a44e-3950a69eff5c",
   "metadata": {},
   "source": [
    "## Add a ``province_language`` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a397c-952b-4750-b461-6f69728dd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each province to its primary language\n",
    "province_language_map = {\n",
    "    'Roma': 'Latin',\n",
    "    'Latium et Campania / Regio I': 'Latin',\n",
    "    'Dalmatia': 'Latin',\n",
    "    'Hispania citerior': 'Latin',\n",
    "    'Germania superior': 'Latin',\n",
    "    'Venetia et Histria / Regio X': 'Latin',\n",
    "    'Dacia': 'Latin',\n",
    "    'Britannia': 'Latin',\n",
    "    'Pannonia superior': 'Latin',\n",
    "    'Samnium / Regio IV': 'Latin',\n",
    "    'Africa proconsularis': 'Latin',\n",
    "    'Germania inferior': 'Latin',\n",
    "    'Apulia et Calabria / Regio II': 'Latin',\n",
    "    'Pannonia inferior': 'Latin',\n",
    "    'Numidia': 'Latin',\n",
    "    'Etruria / Regio VII': 'Latin',\n",
    "    'Umbria / Regio VI': 'Latin',\n",
    "    'Noricum': 'Latin',\n",
    "    'Baetica': 'Latin',\n",
    "    'Transpadana / Regio XI': 'Latin',\n",
    "    'Moesia inferior': 'Latin',\n",
    "    'Lusitania': 'Latin',\n",
    "    'Moesia superior': 'Latin',\n",
    "    'Sardinia': 'Latin',\n",
    "    'Belgica': 'Latin',\n",
    "    'Gallia Narbonensis': 'Latin',\n",
    "    'Aemilia / Regio VIII': 'Latin',\n",
    "    'Picenum / Regio V': 'Latin',\n",
    "    'Raetia': 'Latin',\n",
    "    'Macedonia': 'Greek',\n",
    "    'Aquitani(c)a': 'Latin',\n",
    "    'Bruttium et Lucania / Regio III': 'Latin',\n",
    "    'Liguria / Regio IX': 'Latin',\n",
    "    'Lugudunensis': 'Latin',\n",
    "    'Mauretania Caesariensis': 'Latin',\n",
    "    'Asia': 'Greek',\n",
    "    'Belgica | Germania superior': 'Latin',\n",
    "    'Sicilia': 'Latin',\n",
    "    'Syria': 'Greek',\n",
    "    'Achaia': 'Greek',\n",
    "    'Alpes Cottiae': 'Latin',\n",
    "    'Alpes Maritimae': 'Latin',\n",
    "    'Galatia': 'Greek',\n",
    "    'Thracia': 'Greek',\n",
    "    'Aegyptus': 'Greek'\n",
    "}\n",
    "\n",
    "# Check if the 'province_language' column already exists\n",
    "if 'province_language' not in lire.columns:\n",
    "    # If it doesn't exist, add a new column to the DataFrame, 'province_language', \n",
    "    # mapping the province to its primary language\n",
    "    lire['province_language'] = lire['province'].map(province_language_map)\n",
    "    # Show the updated DataFrame to check that the new column has been added successfully\n",
    "    print(\"province_language column added:\")\n",
    "    # print(lire.head()) # Option to print the first few records of the dataset to confirm\n",
    "else:\n",
    "    print(\"province_language column already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80feda7a-ab88-48c1-b394-c374d5afc8d8",
   "metadata": {},
   "source": [
    "# Exploring the LIRE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fe3c3-ad23-4fb3-90fb-d3a101d034f8",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics for date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bb647-1c16-4494-9d11-b31f34423e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate date range for each inscription and add it to the DF\n",
    "lire['date_range'] = lire['not_after'] - lire['not_before']\n",
    "\n",
    "# Return the total number of records\n",
    "total_records = lire.shape[0]\n",
    "\n",
    "# Calculate the average date range\n",
    "average_range = lire['date_range'].mean()\n",
    "\n",
    "# Calculate the median date range\n",
    "median_range = lire['date_range'].median()\n",
    "\n",
    "# Calculate the standard deviation of the date range\n",
    "std_dev_range = lire['date_range'].std()\n",
    "\n",
    "print(f\"Total number of records: {total_records}\")\n",
    "print(f\"Average Date Range: {average_range}\")\n",
    "print(f\"Median Date Range: {median_range}\")\n",
    "print(f\"Standard Deviation of Date Range: {std_dev_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6917903-8b11-4d65-91a5-c24a12947d84",
   "metadata": {},
   "source": [
    "## Create a histogram of date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017dc15c-0bce-4b8b-8ff9-d76f62a14540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(lire['date_range'], bins=np.arange(0, lire['date_range'].max() + 10, 10), edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date Range (Years)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Date Ranges for Roman Inscriptions (10-year bins)')\n",
    "\n",
    "# set axis limits and grid\n",
    "plt.xlim([0, 600])\n",
    "plt.ylim([0, 50000])\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(0, 600, 50), rotation='vertical')  # Change 10 to the desired tick interval for the x-axis\n",
    "plt.yticks(np.arange(0, 50000, 10000))  # Change 500 to the desired tick interval for the y-axis\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbad81-48c0-45bc-a548-9db73be308a1",
   "metadata": {},
   "source": [
    "## Count number of inscriptions *between* various date-range thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831f739-271a-4a2a-ac0b-f1d8b594ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "\n",
    "thresholds = [1, 10, 25, 50, 100, 200, 300] \n",
    "\n",
    "# Create bins using the thresholds\n",
    "bins = [0] + thresholds + [lire['date_range'].max() + 1]\n",
    "\n",
    "# Categorize the date ranges based on the bins\n",
    "lire['date_range_bins'] = pd.cut(lire['date_range'], bins, right=False) # Count is right-exclusive\n",
    "\n",
    "# Count the number of inscriptions in each bin\n",
    "inscription_counts = lire['date_range_bins'].value_counts().sort_index()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Counts of inscriptions with date ranges between certain thresholds:\")\n",
    "print(inscription_counts)\n",
    "\n",
    "# As a check, count the number of inscriptions with a '0' date range \n",
    "# (date reanges should not be less that one year)\n",
    "\n",
    "print(\"Specific counts for cross-checking\")\n",
    "zero_date_range_count = len(lire[lire['date_range'] == 0])\n",
    "print(f\"Number of records with a date range of 0: {zero_date_range_count}\")\n",
    "lessthanone_date_range_count = len(lire[lire['date_range'] < 1])\n",
    "print(f\"Number of records with a date range of < 1: {lessthanone_date_range_count}\")\n",
    "oneorless_date_range_count = len(lire[lire['date_range'] <= 1])\n",
    "print(f\"Number of records with a date range of <= 1: {oneorless_date_range_count}\")\n",
    "lessthanzero_date_range_count = len(lire[lire['date_range'] < 0])\n",
    "print(f\"Number of records with a date range less than 0: {lessthanzero_date_range_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56fb9bc-9274-4386-9fd0-2316f0cd84cd",
   "metadata": {},
   "source": [
    "## Count number of inscriptions with date ranges *under* various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccba6d-151c-4110-bcc1-49e993cdc078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "thresholds = [1, 10, 25, 50, 100, 200, 300]\n",
    "\n",
    "# Initialize an empty dictionary to store counts\n",
    "cumulative_counts = {}\n",
    "\n",
    "# Calculate counts for each threshold\n",
    "for threshold in thresholds:\n",
    "    count = len(lire[lire['date_range'] < threshold]) # counts are right-exclusive\n",
    "    cumulative_counts[threshold] = count\n",
    "\n",
    "# Display the counts\n",
    "print(\"Cumulative counts of inscriptions with date ranges under various thresholds:\")\n",
    "for threshold, count in cumulative_counts.items():\n",
    "    print(f\"0-{threshold} years: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f1744-2860-4ab4-bdfc-a4febea111a0",
   "metadata": {},
   "source": [
    "## Preliminary SPA calculation, all inscriptions, uniform distribution, 10-year bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698a738-3744-4188-9229-95e967f8815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the summed probability distribution\n",
    "earliest_date = lire['not_before'].min()\n",
    "latest_date = lire['not_after'].max()\n",
    "resolution = 10 # Resolution defines the size of bins or segments in years\n",
    "\n",
    "# Creat an array of zeros to hold the summed probabilities\n",
    "num_bins = int((latest_date - earliest_date) / resolution) + 1\n",
    "summed_prob = np.zeros(num_bins)\n",
    "\n",
    "# Loop through each inscription to update the summed probability distribution\n",
    "for index, row in lire.iterrows():\n",
    "    start = row['not_before']\n",
    "    end = row['not_after']\n",
    "    \n",
    "    start_idx = int((start - earliest_date) / resolution)\n",
    "    end_idx = int((end - earliest_date) / resolution)\n",
    "   \n",
    "    # Calculate uniform distribution for this inscription,\n",
    "    # accounting for date combinations that add up to zero\n",
    "    try:\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1)\n",
    "    except ZeroDivisionError:\n",
    "        uniform_prob = 1  # set default value here\n",
    "\n",
    "    # Add this distribution to the overall summed distribution\n",
    "    summed_prob[start_idx : end_idx + 1] += uniform_prob\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(np.arange(earliest_date, latest_date + resolution, resolution), summed_prob, width=resolution, color='blue', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Summed Probability')\n",
    "plt.title('Summed Probability Analysis of Roman Inscriptions')\n",
    "\n",
    "# set axis limits and grid\n",
    "plt.xlim([-100, 600])\n",
    "plt.ylim([0, 7000])\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(-100, 600, 50), rotation='vertical')  # Change 10 to the desired tick interval for the x-axis\n",
    "plt.yticks(np.arange(0, 7000, 500))  # Change 500 to the desired tick interval for the y-axis\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c961e2f-1cce-416a-ad89-922d166a283b",
   "metadata": {},
   "source": [
    "### Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6bfa5-291a-4fb8-afc5-314441c6fa21",
   "metadata": {},
   "source": [
    "Looking at this overview plot:\n",
    "\n",
    "* Several of the peaks seem to correspond to dynasty changes.\n",
    "* Start date should probably be 1 AD\n",
    "* End date should probably be 400 AD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871114c8-b57b-4def-9256-b6ed7085f468",
   "metadata": {},
   "source": [
    "# Minimum required sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f365f39-92d0-4339-8242-bec708adec4e",
   "metadata": {},
   "source": [
    "## Calculate $\\Delta$T: mean of date-range standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca26a7-ed73-4ab8-9d84-fa8c1f17768d",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2be0a-86b4-4757-b36d-77c338e13466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the date-range standard deviations for each record\n",
    "lire['date_range_std_dev'] = (lire['not_after'] - lire['not_before']) / np.sqrt(12)  # Assuming uniform distribution\n",
    "\n",
    "# Calculate the mean of these standard deviations\n",
    "mean_std_dev = lire['date_range_std_dev'].mean()\n",
    "\n",
    "print(f\"The mean of the date-range standard deviations is: {mean_std_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497892b-d070-47ff-bcec-c036dc13a286",
   "metadata": {},
   "source": [
    "### Various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c2805-9667-4669-bc18-a3a2805a405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range thresholds for filtering\n",
    "thresholds = [25, 50, 100, 200, 300, float('inf')]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Filter the data for records with a date range less than or equal to the current threshold\n",
    "    filtered_lire = lire[lire['date_range'] <= threshold]\n",
    "    \n",
    "    # Calculate the date-range standard deviations for each record in the filtered DataFrame\n",
    "    filtered_lire['date_range_std_dev'] = (filtered_lire['not_after'] - filtered_lire['not_before']) / np.sqrt(12)\n",
    "    \n",
    "    # Calculate the mean of these standard deviations in the filtered DataFrame\n",
    "    mean_std_dev_filtered = filtered_lire['date_range_std_dev'].mean()\n",
    "    \n",
    "    # Count the number of records in the filtered DataFrame\n",
    "    num_records_filtered = len(filtered_lire)\n",
    "    \n",
    "    print(f\"For date range cut-off less than or equal to {threshold} years:\")\n",
    "    print(f\"The mean of the date-range standard deviations is: {mean_std_dev_filtered}\")\n",
    "    print(f\"The number of records in the filtered dataset is: {num_records_filtered}\")\n",
    "    print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57474ff2-2c6e-43b8-a2f4-a94bd7ade9a4",
   "metadata": {},
   "source": [
    "### Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a77865-3d8b-49b3-baf8-7e4365c99560",
   "metadata": {},
   "source": [
    "Calculating the mean of date-range standard devations worked, and gives a rough idea of uncertainty on particular sample sizes. \n",
    "\n",
    "Now attempting to apply the approach suggested by Williams 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980da0f-8e65-4d54-9bab-fb72d11b1e4e",
   "metadata": {},
   "source": [
    "## SE of the date-range mean via bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ec2b7-588d-405f-aa6a-cca4ba39a240",
   "metadata": {},
   "source": [
    "None of my attempts to calculate MSE according to Willaims 2012 worked. Instead used the bootstrapping sampling distribution assessment approach described at https://ealizadeh.com/blog/statistics-data-vs-sampling-distribution/. Compared the mean and standard deviation of date ranges in the sample to the mean of date ranges in the population, them compute standard error, with confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fbee5-7df2-4941-9f9f-29e7f158dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming lire is your DataFrame\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "n_iterations = 1000\n",
    "\n",
    "# Sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Initialize empty list to store standard error values for plotting\n",
    "standard_errors = []\n",
    "\n",
    "# New list to store the means for each sample size\n",
    "sample_means = []\n",
    "\n",
    "# Initialize empty dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Calculate mean and standard deviation of full dataset for comparison\n",
    "mean_date_range_full = lire['date_range'].mean()\n",
    "std_date_range_full = lire['date_range'].std()\n",
    "\n",
    "# Make sure the directory for saving the bootstrap samples exists\n",
    "if not os.path.exists(\"bootstrap-samples/standard-error/means_sd\"):\n",
    "    os.makedirs(\"bootstrap-samples/standard-error/means_sd\", exist_ok=True)\n",
    "    \n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "    \n",
    "    # Create a dynamic file name\n",
    "    saved_sample_path = f\"bootstrap-samples/standard-error/means_sd/n_{sample_size}.npy\"\n",
    "    \n",
    "    if os.path.exists(saved_sample_path): # Check if saved bootstrap sample exists\n",
    "        # Load saved sample\n",
    "        print(\"Loading saved bootstrap sample...\")\n",
    "        saved_data = np.load(saved_sample_path, allow_pickle=True).item()\n",
    "        bootstrap_sample_means = saved_data['means']\n",
    "        bootstrap_sample_stds = saved_data['stds']\n",
    "    else:\n",
    "        # Create bootstrap sample\n",
    "        print(f\"Performing bootstrap operation for sample size {sample_size}...\")\n",
    "        bootstrap_sample_means = []\n",
    "        bootstrap_sample_stds = []  \n",
    "    \n",
    "        # Bootstrap sampling\n",
    "        for i in range(n_iterations):\n",
    "            # Sample with replacement from the dataset\n",
    "            bootstrap_sample = lire['date_range'].sample(n=sample_size, replace=True)\n",
    "            # Calculate the mean date-range of the bootstrap sample\n",
    "            bootstrap_sample_mean = bootstrap_sample.mean()\n",
    "            # Calculate the standard deviation of date-range in the bootstrap sample\n",
    "            bootstrap_sample_std = bootstrap_sample.std()\n",
    "            # Store the mean and standard deviation in their respective lists\n",
    "            bootstrap_sample_means.append(bootstrap_sample_mean)\n",
    "            bootstrap_sample_stds.append(bootstrap_sample_std)\n",
    "            \n",
    "        # Save the sample date-range means and standard deviations\n",
    "        print(\"Saving bootstrap sample means and standard deviations...\")\n",
    "        saved_data = {\n",
    "            'means': bootstrap_sample_means,\n",
    "            'stds': bootstrap_sample_stds\n",
    "        }\n",
    "        np.save(saved_sample_path, saved_data)\n",
    "    \n",
    "    # Calculate the standard error for the sample date-range mean\n",
    "    standard_error = np.std(bootstrap_sample_means)\n",
    "    \n",
    "    # Store the standard error for plotting\n",
    "    standard_errors.append(standard_error)\n",
    "    \n",
    "    # Store the sample date-range means for plotting\n",
    "    sample_means.append(np.mean(bootstrap_sample_means))\n",
    "\n",
    "    # Calculate the date-range mean and standard deviation for the bootstrap samples\n",
    "    mean_bootstrap_samples = np.mean(bootstrap_sample_means)\n",
    "    std_bootstrap_samples = np.mean(bootstrap_sample_stds)\n",
    "\n",
    "    # Store results in the dictionary\n",
    "    results[sample_size] = {\n",
    "        'mean_bootstrap_samples': mean_bootstrap_samples,\n",
    "        'std_bootstrap_samples': std_bootstrap_samples,\n",
    "        'standard_error': standard_error\n",
    "    }\n",
    "\n",
    "print(f\"Mean date range for full dataset: {mean_date_range_full}\")\n",
    "print(f\"Standard Deviation of date range for full dataset: {std_date_range_full}\")\n",
    "\n",
    "# Display the results\n",
    "for sample_size, metrics in results.items():\n",
    "    print(f\"\\nFor a sample size of {sample_size}:\")\n",
    "    print(f\"  Mean date range for bootstrap samples: {metrics['mean_bootstrap_samples']}\")\n",
    "    print(f\"  Standard Deviation of date range for bootstrap samples: {metrics['std_bootstrap_samples']}\")\n",
    "    print(f\"  Standard Error of the mean date range: {metrics['standard_error']}\")\n",
    "\n",
    "# Plotting the standard errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, standard_errors, marker='o')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Standard Error of the Mean Date Range')\n",
    "plt.title('Standard Error vs. Sample Size')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the means with confidence interval\n",
    "\n",
    "# Calculate the 95% confidence intervals\n",
    "lower_bound = np.array(sample_means) - 1.96 * np.array(standard_errors)\n",
    "upper_bound = np.array(sample_means) + 1.96 * np.array(standard_errors)\n",
    "\n",
    "# Plotting the mean and 95% CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(sample_sizes, sample_means, yerr=[sample_means - lower_bound, upper_bound - sample_means], fmt='o', label='Sample Means with 95% CI')\n",
    "plt.axhline(y=mean_date_range_full, color='r', linestyle='-', label='Full Dataset Mean') \n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Mean Date Range')\n",
    "plt.title('Mean Date Range with 95% Confidence Intervals for Various Sample Sizes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b9cae-398b-4a03-a774-76a75915df92",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test of 'date_range' subsets\n",
    "\n",
    "Another view on minimum sample size for SPA\n",
    "\n",
    "The D-statistic is the maximum difference between the cumulative distribution functions (CDFs) of the two samples, and the P-value is the significance level. A smaller P-value would indicate that it is unlikely that the sample and the dataset come from the same distribution, assuming the null hypothesis is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3afc33-e245-4eff-8b3e-64582cce21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming lire is your DataFrame\n",
    "\n",
    "# Sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Create a dictionary to hold Kolmogorov-Smirnov test results for each sample size\n",
    "ks_results = {}\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    # Sample without replacement from the dataset\n",
    "    sample = lire['date_range'].sample(n=sample_size)\n",
    "    \n",
    "    # Perform the Kolmogorov-Smirnov test\n",
    "    d_stat, p_value = stats.ks_2samp(sample, lire['date_range'])\n",
    "    \n",
    "    # Store the result in the dictionary\n",
    "    ks_results[sample_size] = {\n",
    "        'D-statistic': d_stat,\n",
    "        'P-value': p_value\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for sample_size, metrics in ks_results.items():\n",
    "    print(f\"\\nFor a sample size of {sample_size}:\")\n",
    "    print(f\"  D-statistic: {metrics['D-statistic']}\")\n",
    "    print(f\"  P-value: {metrics['P-value']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e48a5-79d8-4ff2-b018-7b27db93812c",
   "metadata": {},
   "source": [
    "#### K-S with bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061c67e-2ceb-4e44-ae90-ad7bcabc6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle  # Importing pickle for serialization\n",
    "from scipy.stats import ks_2samp\n",
    "from tqdm.auto import tqdm  # Using the tqdm.auto for universal progress bar compatibility\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load or generate bootstrap samples\n",
    "def load_or_generate_bootstrap_samples(sample_size, num_iterations, full_data):\n",
    "    \"\"\"\n",
    "    Load or generate bootstrap samples for a given sample size and number of iterations.\n",
    "\n",
    "    Parameters:\n",
    "    - sample_size (int): The size of each bootstrap sample.\n",
    "    - num_iterations (int): The number of bootstrap samples to generate.\n",
    "    - full_data (DataFrame): The original dataset to sample from.\n",
    "\n",
    "    Returns:\n",
    "    - List of DataFrames: A list containing the bootstrap samples.\n",
    "    \"\"\"\n",
    "\n",
    "    # Directory and file paths for saving/loading samples\n",
    "    directory = f'bootstrap-samples/statistics/k_s-date_range'\n",
    "    file_path = os.path.join(directory, f'n_{sample_size}.pkl')\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Try to load existing samples, otherwise generate new ones\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            bootstrap_samples = pickle.load(f)\n",
    "        print(f\"Loaded bootstrap samples from {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        # Generating bootstrap samples with tqdm, added a description to the progress bar\n",
    "        bootstrap_samples = [full_data.sample(n=sample_size, replace=True).copy().reset_index(drop=True) for _ in tqdm(range(num_iterations), desc=f\"Generating bootstrap samples for n={sample_size}\")]\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(bootstrap_samples, f)\n",
    "        print(f\"Saved bootstrap samples to {file_path}\")\n",
    "\n",
    "    return bootstrap_samples\n",
    "\n",
    "# Load the LIRE dataset into 'full_data'\n",
    "full_data = lire\n",
    "\n",
    "# Calculate the 'mean_date' for the full dataset\n",
    "full_data['mean_date'] = (full_data['not_before'] + full_data['not_after']) / 2.0\n",
    "\n",
    "# Define various sample sizes to test\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Number of iterations for bootstrapping\n",
    "num_iterations = 1000\n",
    "\n",
    "# Dictionary to hold the K-S test results\n",
    "ks_test_results = {}\n",
    "\n",
    "# Initialize dictionaries to store standard errors and sample means\n",
    "# This is done for both 'Date Range' and 'Mean Date' analyses.\n",
    "standard_errors = {}\n",
    "standard_errors_date = {}\n",
    "sample_means = {}\n",
    "sample_means_date = {}\n",
    "\n",
    "# Loop over each sample size to perform K-S tests with tqdm progress bar and description\n",
    "for sample_size in tqdm(sample_sizes, desc=\"Performing K-S tests across sample sizes\"):\n",
    "    bootstrap_samples = load_or_generate_bootstrap_samples(sample_size, num_iterations, full_data)\n",
    "\n",
    "    # Lists to store D-statistics and P-values for 'date_range' and 'mean_date'\n",
    "    d_statistics = []\n",
    "    p_values = []\n",
    "    d_statistics_date = []\n",
    "    p_values_date = []\n",
    "\n",
    "    # Run K-S test for each bootstrap sample\n",
    "    for bootstrap_sample in tqdm(bootstrap_samples, desc=f'KS Test for n={sample_size}'):\n",
    "\n",
    "        # Compute 'mean_date' for each bootstrap sample\n",
    "        bootstrap_sample['mean_date'] = (bootstrap_sample['not_before'] + bootstrap_sample['not_after']) / 2.0\n",
    "\n",
    "        # Perform K-S Test for 'date_range' and 'mean_date'\n",
    "        d_stat, p_value = ks_2samp(full_data['date_range'], bootstrap_sample['date_range'])\n",
    "        d_stat_date, p_value_date = ks_2samp(full_data['mean_date'], bootstrap_sample['mean_date'])\n",
    "\n",
    "        # Append the results to the lists\n",
    "        d_statistics.append(d_stat)\n",
    "        p_values.append(p_value)\n",
    "        d_statistics_date.append(d_stat_date)\n",
    "        p_values_date.append(p_value_date)\n",
    "\n",
    "    # Store the K-S test results in the dictionary\n",
    "    ks_test_results[sample_size] = {'d_statistics': d_statistics, 'p_values': p_values,\n",
    "                                    'd_statistics_date': d_statistics_date, 'p_values_date': p_values_date}\n",
    "\n",
    "# Calculate means and standard deviations for each metric and sample size\n",
    "means = {}\n",
    "std_devs = {}\n",
    "\n",
    "for metric in ['d_statistics', 'p_values', 'd_statistics_date', 'p_values_date']:\n",
    "    means[metric] = []\n",
    "    std_devs[metric] = []\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        means[metric].append(np.mean(ks_test_results[sample_size][metric]))\n",
    "        std_devs[metric].append(np.std(ks_test_results[sample_size][metric]))\n",
    "\n",
    "# Plot the K-S test results with trend lines and confidence intervals\n",
    "for metric in ['d_statistics', 'p_values', 'd_statistics_date', 'p_values_date']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the trend line (mean)\n",
    "    plt.plot(sample_sizes, means[metric], label='Mean')\n",
    "    \n",
    "    # Plot the confidence interval (mean ± std_dev)\n",
    "    plt.fill_between(sample_sizes, np.array(means[metric]) - np.array(std_devs[metric]),\n",
    "                     np.array(means[metric]) + np.array(std_devs[metric]), color='gray', alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    \n",
    "    # Clarify titles to indicate what each plot represents\n",
    "    if 'date' in metric:\n",
    "        plt.title(f\"{metric.replace('_', ' ').title().replace('Date', 'Mean Date')} Across Different Sample Sizes\")\n",
    "    else:\n",
    "        plt.title(f\"{metric.replace('_', ' ').title().replace('Statistics', 'Statistics for Date Range')} Across Different Sample Sizes\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store the summary statistics\n",
    "summary_stats = pd.DataFrame(columns=['Sample Size', 'Metric', 'Mean', 'Std Dev'])\n",
    "\n",
    "# Loop through each metric and sample size to populate the DataFrame\n",
    "for metric in ['d_statistics', 'p_values', 'd_statistics_date', 'p_values_date']:\n",
    "    for sample_size in sample_sizes:\n",
    "        mean_value = np.mean(ks_test_results[sample_size][metric])\n",
    "        std_dev_value = np.std(ks_test_results[sample_size][metric])\n",
    "\n",
    "        # Clarify the metric names to make them more descriptive\n",
    "        if 'date' in metric:\n",
    "            descriptive_metric = metric.replace('_', ' ').title().replace('Date', 'Mean Date')\n",
    "        else:\n",
    "            descriptive_metric = metric.replace('_', ' ').title().replace('Statistics', 'Statistics for Date Range')\n",
    "\n",
    "        summary_stats = summary_stats.append({'Sample Size': sample_size, 'Metric': descriptive_metric, \n",
    "                                              'Mean': mean_value, 'Std Dev': std_dev_value}, ignore_index=True)\n",
    "\n",
    "# Display the summary table\n",
    "print(\"Summary Statistics for K-S Tests\")\n",
    "print(\"=\"*50)\n",
    "print(summary_stats)\n",
    "\n",
    "# Calculate standard errors and means for each metric and sample size.\n",
    "# Standard error is calculated as the standard deviation divided by the square root of the sample size.\n",
    "for sample_size in sample_sizes:\n",
    "    standard_errors[sample_size] = np.std(ks_test_results[sample_size]['d_statistics']) / np.sqrt(sample_size)\n",
    "    standard_errors_date[sample_size] = np.std(ks_test_results[sample_size]['d_statistics_date']) / np.sqrt(sample_size)\n",
    "    \n",
    "    sample_means[sample_size] = np.mean(ks_test_results[sample_size]['d_statistics'])\n",
    "    sample_means_date[sample_size] = np.mean(ks_test_results[sample_size]['d_statistics_date'])\n",
    "\n",
    "# Convert dictionaries to lists for ease of plotting\n",
    "standard_errors_list = [standard_errors[s] for s in sample_sizes]\n",
    "standard_errors_date_list = [standard_errors_date[s] for s in sample_sizes]\n",
    "sample_means_list = [sample_means[s] for s in sample_sizes]\n",
    "sample_means_date_list = [sample_means_date[s] for s in sample_sizes]\n",
    "\n",
    "# Plot standard errors for each metric across various sample sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, standard_errors_list, marker='o', label='Date Range')\n",
    "plt.plot(sample_sizes, standard_errors_date_list, marker='x', label='Mean Date')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Standard Error of the D-Statistic')\n",
    "plt.title('Standard Error of the D-Statistic Across Different Sample Sizes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the 95% confidence intervals for the mean D-statistics\n",
    "lower_bound = np.array(sample_means_list) - 1.96 * np.array(standard_errors_list)\n",
    "upper_bound = np.array(sample_means_list) + 1.96 * np.array(standard_errors_list)\n",
    "\n",
    "lower_bound_date = np.array(sample_means_date_list) - 1.96 * np.array(standard_errors_date_list)\n",
    "upper_bound_date = np.array(sample_means_date_list) + 1.96 * np.array(standard_errors_date_list)\n",
    "\n",
    "# Plot the mean D-statistics with 95% confidence intervals for each metric across various sample sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(sample_sizes, sample_means_list, yerr=[sample_means_list - lower_bound, upper_bound - sample_means_list], fmt='o', label='D-Statistics for Date Range with 95% CI')\n",
    "plt.errorbar(sample_sizes, sample_means_date_list, yerr=[sample_means_date_list - lower_bound_date, upper_bound_date - sample_means_date_list], fmt='x', label='D-Statistics for Mean Date with 95% CI')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Mean D-Statistic')\n",
    "plt.title('Mean D-Statistic with 95% Confidence Intervals Across Different Sample Sizes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics for each sample size and metric, mimicking the format of your previous model\n",
    "for sample_size in sample_sizes:\n",
    "    print(f\"\\nFor a sample size of {sample_size}:\")\n",
    "    print(f\"  Mean D-Statistic for Date Range: {sample_means[sample_size]}\")\n",
    "    print(f\"  Standard Error of D-Statistic for Date Range: {standard_errors[sample_size]}\")\n",
    "    print(f\"  Mean D-Statistic for Mean Date: {sample_means_date[sample_size]}\")\n",
    "    print(f\"  Standard Error of D-Statistic for Mean Date: {standard_errors_date[sample_size]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ec7db-cb78-45f5-9594-7e0d8b5b8ea9",
   "metadata": {},
   "source": [
    "### Standard Error (SE) of the date-range summed probability values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671f886-f507-4029-855d-53263fe74342",
   "metadata": {},
   "source": [
    "Using the same approach taken for means above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9ef16-c36c-4ecc-818a-03271a0facfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming lire is your DataFrame\n",
    "# lire = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "n_iterations = 1000\n",
    "\n",
    "# Sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Create 5-year bins for full dataset\n",
    "bins = np.arange(lire['date_range'].min(), lire['date_range'].max() + 5, 5)\n",
    "full_data_summed_prob = np.histogram(lire['date_range'], bins=bins, density=True)[0]\n",
    "\n",
    "# Initialize a dictionary to hold results for each sample size\n",
    "results = {}\n",
    "\n",
    "# Initialize empty list to store standard error values for plotting\n",
    "standard_errors = []\n",
    "\n",
    "# Make sure the directory for saving the bootstrap samples exists\n",
    "if not os.path.exists(\"bootstrap-samples/standard-error/summed-probabilities\"):\n",
    "    os.makedirs(\"bootstrap-samples/standard-error/summed-probabilities\", exist_ok=True)\n",
    "\n",
    "# Loop through each sample size\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    # Create a dynamic file name\n",
    "    saved_sample_path = f\"bootstrap-samples/standard-error/summed-probabilities/n_{sample_size}.npy\"\n",
    "    \n",
    "    if os.path.exists(saved_sample_path): # Check if saved bootstrap sample exists\n",
    "        print(\"Loading saved bootstrap sample...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path, allow_pickle=True)\n",
    "    else:\n",
    "        # Initialize a list to hold the summed probabilities for each bootstrap sample\n",
    "        bootstrap_summed_probs = []\n",
    "    \n",
    "        # Bootstrap sampling\n",
    "        for i in range(n_iterations):\n",
    "            # Sample with replacement from the dataset\n",
    "            bootstrap_sample = lire['date_range'].sample(n=sample_size, replace=True)\n",
    "            \n",
    "            # Calculate the summed probability for this bootstrap sample\n",
    "            bootstrap_summed_prob = np.histogram(bootstrap_sample, bins=bins, density=True)[0]\n",
    "            \n",
    "            # Store in the list\n",
    "            bootstrap_summed_probs.append(bootstrap_summed_prob)\n",
    "\n",
    "        # Save the sample\n",
    "        print(\"Saving bootstrap sample...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "\n",
    "    # Convert to NumPy array for easier calculations\n",
    "    bootstrap_summed_probs = np.array(bootstrap_summed_probs)\n",
    "    \n",
    "    # Calculate the standard error for each bin\n",
    "    standard_errors_for_bins = np.std(bootstrap_summed_probs, axis=0)\n",
    "    \n",
    "    # Calculate the mean standard error for this sample size\n",
    "    mean_standard_error = np.mean(standard_errors_for_bins)\n",
    "    \n",
    "    # Store the standard error for plotting\n",
    "    standard_errors.append(mean_standard_error)\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    results[sample_size] = {\n",
    "        'standard_errors_for_bins': standard_errors_for_bins,\n",
    "        'mean_standard_error': mean_standard_error,\n",
    "        'bootstrap_summed_probs': bootstrap_summed_probs  # <-- Add this line\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for sample_size, metrics in results.items():\n",
    "    print(f\"\\nFor a sample size of {sample_size}:\")\n",
    "    print(f\"  Mean Standard Error for Summed Probabilities: {metrics['mean_standard_error']}\")\n",
    "\n",
    "# Plotting the standard errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sample_sizes, standard_errors, marker='o')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Mean Standard Error of Summed Probability')\n",
    "plt.title('Standard Error vs. Sample Size')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting summed probabilities with 95% confidence intervals for each sample size\n",
    "for sample_size, metrics in results.items():\n",
    "    # Retrieve the bootstrap summed probabilities for this sample size\n",
    "    bootstrap_summed_probs = metrics['bootstrap_summed_probs']\n",
    "    \n",
    "    # Calculate the mean summed probability across all bootstrap samples for this sample size\n",
    "    mean_summed_prob = np.mean(bootstrap_summed_probs, axis=0) \n",
    "    \n",
    "    # Calculate the 95% confidence intervals\n",
    "    lower_bound = mean_summed_prob - 1.96 * metrics['standard_errors_for_bins']\n",
    "    upper_bound = mean_summed_prob + 1.96 * metrics['standard_errors_for_bins']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the mean summed probabilities\n",
    "    plt.plot(bins[:-1], mean_summed_prob, marker='o', label='Mean Summed Probability')\n",
    "    \n",
    "    # Fill between the upper and lower confidence intervals\n",
    "    plt.fill_between(bins[:-1], lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    \n",
    "    # Plot the summed probabilities for the full dataset for comparison\n",
    "    plt.plot(bins[:-1], full_data_summed_prob, marker='x', linestyle='--', label='Full Dataset')\n",
    "    \n",
    "    plt.xlabel('Date Range')\n",
    "    plt.xlim([0, 300])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.title(f'Summed Probability with 95% Confidence Intervals for Sample Size = {sample_size}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaca631-eb56-44f0-8b18-d664dbbffc6e",
   "metadata": {},
   "source": [
    "#### Explanation of summed probability in this graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f2622-45e9-4431-8948-c8cc11201124",
   "metadata": {},
   "source": [
    "The y-axis labeled \"Summed Probability\" represents the probability density of the data falling into each bin along the x-axis (in this case, each 5-year range). To clarify, it's not the probability of a single event occurring in that bin, but rather a density measure that gives you an idea of how many events fall into that time frame relative to the entire dataset.\n",
    "\n",
    "In a histogram, if you just count the number of occurrences and plot them, you get a frequency distribution. If you divide each count by the total number of observations and further divide by the width of the bin, you get a probability density.\n",
    "\n",
    "The \"summed probability\" term can be a bit misleading. In a probability density plot or histogram, the area under the curve sums to 1, but the individual y-values are density measures rather than probabilities in the usual sense.\n",
    "\n",
    "So, if the y-value for a particular 5-year range is, say, 0.2, this doesn't mean there's a 20% chance of an event happening in that 5-year range. Rather, it means that the density of events in that 5-year range is 0.2 per year. To get the actual probability of an event happening within that 5-year bin, you would multiply this density by the width of the bin (in this case, 5 years)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a66bf-a7bf-41ff-980e-988961cd2ad1",
   "metadata": {},
   "source": [
    "# Summed Probability Analysis of entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f7560-fd01-4bba-bda0-c7ecca9dd4a1",
   "metadata": {},
   "source": [
    "## Various date-ranges and sample sizes; error based on date-range SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e27de-195f-4ddb-8e68-29b524f4bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# Filter your DataFrame to only include records with a date range <= thresholds\n",
    "\n",
    "# Define date range thresholds for filtering; note that calculations are now *below* rather than *between* thresholds\n",
    "thresholds = [(0, 0), (0, 10), (0, 25), (0, 50), (0, 100), (0, 200), (0, 300), (0, float('inf'))]\n",
    "\n",
    "# Define overall earliest and latest date in the dataset, and bin resolution\n",
    "earliest_date, latest_date = filtered_lire['not_before'].min(), filtered_lire['not_after'].max()\n",
    "resolution = 5\n",
    "\n",
    "# Define sample sizes to investigate\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "\n",
    "# Add special value to signify using all records\n",
    "sample_sizes.append('all')\n",
    "\n",
    "# Loop through each date range threshold\n",
    "for start, end in thresholds:\n",
    "    \n",
    "    # Filter DataFrame based on the current threshold\n",
    "    if end == float('inf'):\n",
    "        subset_df = filtered_lire[filtered_lire['date_range'] >= start]\n",
    "        label = f\"{start}+\"\n",
    "    elif start == 0 and end == 0:\n",
    "        subset_df = filtered_lire[filtered_lire['date_range'] == 0]\n",
    "        label = f\"{start}\"\n",
    "    else:\n",
    "        subset_df = filtered_lire[(filtered_lire['date_range'] >= start) & (filtered_lire['date_range'] <= end)]\n",
    "        label = f\"{start}-{end}\"\n",
    "    \n",
    "    # Loop through each sample size\n",
    "    for sample_size in sample_sizes:\n",
    "\n",
    "         # Special case to use all records in filtered DataFrame\n",
    "        if sample_size == 'all':\n",
    "            subsample = filtered_lire\n",
    "            size_label = \"all\"\n",
    "        # Or, create a random subsample from the filtered DataFrame\n",
    "        else:\n",
    "            subsample = subset_df.sample(n=min(sample_size, len(subset_df)), random_state=42)\n",
    "            size_label = str(sample_size)\n",
    "\n",
    "        # Calculate the summed probability and standard deviations for the subsample\n",
    "        summed_prob, std_devs = compute_summed_probability(subsample, earliest_date, latest_date, resolution=5)\n",
    "        \n",
    "        # Generate x-values for plotting, making sure they align in shape with summed_prob\n",
    "        x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "        \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        \n",
    "        # Plot the summed probability distribution\n",
    "        plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "        \n",
    "        # Calculate and plot the moving average\n",
    "        window_size = 5\n",
    "        moving_avg = np.convolve(summed_prob, np.ones(window_size)/window_size, mode='valid')\n",
    "        \n",
    "        # Calculate and plot the error margins\n",
    "        moving_std_dev = np.convolve(std_devs, np.ones(window_size)/window_size, mode='valid')\n",
    "        \n",
    "        # Generate x-values for the moving average and error margins\n",
    "        moving_avg_x = x_values[int(window_size/2):-int(window_size/2)+1][:len(moving_avg)]\n",
    "        \n",
    "        # Plot the moving average and fill between for the error margin\n",
    "        plt.plot(moving_avg_x, moving_avg, color='red', label=f\"{window_size}-bin Moving Average\")\n",
    "        plt.fill_between(moving_avg_x, moving_avg - moving_std_dev, moving_avg + moving_std_dev, color='red', alpha=0.2, label='Error Margin')\n",
    "        \n",
    "        plt.title(f\"Summed Probability Distribution for Date Range {label}, Sample Size = {size_label}\")\n",
    "        plt.xlabel('Date')\n",
    "        plt.xlim([-100, 600])\n",
    "        plt.ylabel('Summed Probability')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab588f-234d-4ba0-a6b0-9d94735f3602",
   "metadata": {},
   "source": [
    "## Various date-ranges and sample sizes; bootstrapped confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc03c33-1cfd-4f85-a4bc-e6bb95a280b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "# Initialize parameters\n",
    "n_iterations = 1000  # Number of bootstrap iterations\n",
    "thresholds = [(0, 0), (0, 10), (0, 25), (0, 50), (0, 100), (0, 200), (0, 300), (0, float('inf'))]\n",
    "earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "resolution = 5\n",
    "sample_sizes = [25, 50, 100, 250, 500, 1000, 2000]\n",
    "sample_sizes.append('all') # Add special value to signify using all records\n",
    "window_size = 5 # Define the size of the moving window for the moving average\n",
    "\n",
    "# Define directory to store bootstrap samples and create it if it doesn't exist\n",
    "bootstrap_samples_dir = \"bootstrap-samples/spa-ci/date-ranges/subsamples\"\n",
    "os.makedirs(bootstrap_samples_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each date range threshold\n",
    "for start, end in thresholds:\n",
    "    # Filter DataFrame based on the current threshold\n",
    "    if end == float('inf'):\n",
    "        subset_df = lire[lire['date_range'] >= start]\n",
    "        label = f\"{start}+\"\n",
    "    elif start == 0 and end == 0:\n",
    "        subset_df = lire[lire['date_range'] == 0]\n",
    "        label = f\"{start}\"\n",
    "    else:\n",
    "        subset_df = lire[(lire['date_range'] >= start) & (lire['date_range'] <= end)]\n",
    "        label = f\"{start}-{end}\"\n",
    "    # Save the number of inscriptions to a variable\n",
    "    n_inscriptions = subset_df.shape[0]\n",
    "\n",
    "    # Loop through each sample size\n",
    "    for sample_size in sample_sizes:\n",
    "        # Check for special case of sample_size = 'all'\n",
    "        if sample_size == 'all':\n",
    "            subsample = subset_df\n",
    "            size_label = \"all\"\n",
    "        # Or, create a random subsample from the filtered DataFrame\n",
    "        else:\n",
    "            subsample = subset_df.sample(n=min(sample_size, len(subset_df)), random_state=42)\n",
    "            size_label = str(sample_size)\n",
    "        # Generate filename for the bootstrap sample\n",
    "        print(f\"Checking for bootstrap sample file for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "        filename_info = f\"date-range_{label}_sample-size_{size_label}\"\n",
    "        filename_hash = hashlib.md5(filename_info.encode()).hexdigest()\n",
    "        saved_sample_path = os.path.join(bootstrap_samples_dir, f\"{filename_hash}.npy\")\n",
    "\n",
    "        # Check if bootstrap sample file already exists\n",
    "        if os.path.exists(saved_sample_path):\n",
    "            print(f\"Loading bootstrap sample file for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "            bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "        # If no bootstrap sample file exists, generate bootstrap sample and save it\n",
    "        else:\n",
    "            print(f\"Calculating bootstrap samples for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "            bootstrap_summed_probs = np.zeros((n_iterations, int((latest_date - earliest_date) / resolution)))# Bootstrap sampling  <-- New block\n",
    "            for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "                bootstrap_sample = subsample.sample(n=len(subsample), replace=True)\n",
    "                bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "                bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "            \n",
    "            # Save the computed bootstrap samples\n",
    "            print(f\"Saving bootstrap sample file for date-range: {label}, sample size: {size_label}...\") # Status message\n",
    "            np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "\n",
    "        # Calculate 95% confidence intervals\n",
    "        lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "\n",
    "        # Calculate the summed probability and standard deviations for the subset\n",
    "        summed_prob, _ = compute_summed_probability(subsample, earliest_date, latest_date, resolution)\n",
    "\n",
    "        # Generate x-values for plotting\n",
    "        x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "        # Calculate moving average for summed_prob\n",
    "        moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "\n",
    "        # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "        moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "        moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "\n",
    "        # Plot the summed probability distribution\n",
    "        plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "        \n",
    "        # Plot the 95% confidence intervals  <-- New line\n",
    "        plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "        \n",
    "        # Plot the moving averages\n",
    "        plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "\n",
    "        # Add fill_between for the moving averages of the 95% CI\n",
    "        plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "\n",
    "        # Plot title; include number of inscriptions if subsample = 'all'\n",
    "        if size_label == \"all\":\n",
    "            plt.title(f\"Summed Probability Distribution for Date Range {label}, all records (n={n_inscriptions})\")\n",
    "        else:\n",
    "            plt.title(f\"Summed Probability Distribution for Date Range {label}, Sample Size {size_label}\")\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.xlim([-100, 600])\n",
    "        plt.ylabel('Summed Probability')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87dda20-2bca-4d08-b3aa-15298cbf0a7a",
   "metadata": {},
   "source": [
    "## Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e57d99-2f7c-46d9-aaa2-0d8645516d41",
   "metadata": {},
   "source": [
    "The output of the SPA appears, at first glance, plausible as a proxy for either the population of the Latin-speaking regions of the Roman Empire, or perhaps the broader sociopolitical complexity, between about AD 1 - 400. Is so, it would indicate a high and slowly growing population (with some ups and downs) between ca. 1 - 200 AD, then a major downturn during the 'Crisis of the 3rd Century', followed by a partial rebound in the late 3rd and early 4th centuries AD, concluding with decline betwwen about the second quarter and the end of the 4th century. \n",
    "\n",
    "I now need to:\n",
    "\n",
    "* compare this pattern with population estimates from Hansen and others\n",
    "* refresh my lit review from 2020, looking especially to see if anyone has done SPA on inscriptions since then (I could find no indication of this approach at that time)\n",
    "* apply this approach to subsets of the data: provinces, cities, etc., and see what those patterns look like\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e5c3a-7326-4877-9d53-86b0a2d2edd0",
   "metadata": {},
   "source": [
    "# SPA on major subsets of LIRE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3e55c-1eab-40a4-bedb-da5d96397067",
   "metadata": {},
   "source": [
    "Use code from above, except:\n",
    "\n",
    "1. Remove the sample_sizes loop and list to eliminate subsampling.\n",
    "2. Exclude entries from the province of Roma by filtering out such rows from lire dataframe.\n",
    "3. Update the folder path to save/load the bootstrap samples\n",
    "\n",
    "No need for subsamples, that was exploratory. Will, however, still generate SPA for various date-ranges.\n",
    "\n",
    "Changes in the code were so slight that I've attempted to run the three variations as part of one code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b476dd0-e7aa-4fa7-824f-43f67edf5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "# Function to run the analysis\n",
    "\n",
    "def run_analysis(lire, bootstrap_samples_dir, file_prefix):\n",
    "        \n",
    "    # Constants\n",
    "    X_LIM_LOW = -100\n",
    "    X_LIM_HIGH = 600\n",
    "    \n",
    "    # Initialize parameters\n",
    "    n_iterations = 1000  # Number of bootstrap iterations\n",
    "    thresholds = [(0, 0), (0, 10), (0, 25), (0, 50), (0, 100), (0, 200), (0, 300), (0, float('inf'))]\n",
    "    earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "    resolution = 5\n",
    "    window_size = 5 # Define the size of the moving window for the moving average\n",
    "    \n",
    "    # Creat new bootstrap samples directory if it doesn't exist (parameter passed to this function)\n",
    "    os.makedirs(bootstrap_samples_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through each date range threshold\n",
    "    for start, end in thresholds:\n",
    "        # Filter DataFrame based on the current threshold\n",
    "        if end == float('inf'):\n",
    "            subset_df = lire[lire['date_range'] >= start]\n",
    "            label = f\"{start}+\"\n",
    "        elif start == 0 and end == 0:\n",
    "            subset_df = lire[lire['date_range'] == 0]\n",
    "            label = f\"{start}\"\n",
    "        else:\n",
    "            subset_df = lire[(lire['date_range'] >= start) & (lire['date_range'] <= end)]\n",
    "            label = f\"{start}-{end}\"\n",
    "    \n",
    "        # Save the number of inscriptions to a variable\n",
    "        n_inscriptions = subset_df.shape[0]\n",
    "    \n",
    "        # Generate filename for the bootstrap sample\n",
    "        print(f\"Checking for bootstrap sample file for: date-range {label} {file_prefix}...\")  # Status message\n",
    "        filename_info = f\"{file_prefix}-{label}\" \n",
    "        filename_hash = hashlib.md5(filename_info.encode()).hexdigest()\n",
    "        saved_sample_path = os.path.join(bootstrap_samples_dir, f\"{filename_hash}.npy\")\n",
    "    \n",
    "        # Check if bootstrap sample file already exists\n",
    "        if os.path.exists(saved_sample_path):\n",
    "            print(f\"Loading bootstrap sample file for: date-range {label} {file_prefix}...\") # Status message\n",
    "            bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "        \n",
    "        # If no bootstrap sample file exists, generate bootstrap sample and save it\n",
    "        else:\n",
    "            print(f\"Calculating bootstrap samples for: date-range {label} {file_prefix}...\") # Status message\n",
    "            bootstrap_summed_probs = np.zeros((n_iterations, int((latest_date - earliest_date) / resolution)))# Bootstrap sampling  <-- New block\n",
    "            for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "                bootstrap_sample = subset_df.sample(n=len(subset_df), replace=True)\n",
    "                bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "                bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "               \n",
    "            # Save the computed bootstrap samples\n",
    "            print(f\"Saving bootstrap sample file for: date-range {label} {file_prefix}...\") # Status message\n",
    "            np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "            \n",
    "        # Calculate 95% confidence intervals\n",
    "        lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "        upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "        \n",
    "        # Calculate the summed probability and standard deviations for the subset\n",
    "        summed_prob, _ = compute_summed_probability(subset_df, earliest_date, latest_date, resolution)\n",
    "        \n",
    "        # Generate x-values for plotting\n",
    "        x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "        \n",
    "        # Calculate moving average for summed_prob\n",
    "        moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "        \n",
    "        # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "        moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "        moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "        \n",
    "        # Plot the summed probability distribution\n",
    "        plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    \n",
    "        # Plot the 95% confidence intervals  <-- New line\n",
    "        plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    \n",
    "        # Plot the moving averages\n",
    "        plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "        # Add fill_between for the moving averages of the 95% CI\n",
    "        plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "            \n",
    "        plt.title(f\"Summed Probability Distribution for Date Range {label} (n={n_inscriptions})\")\n",
    "        plt.xlabel('Date')\n",
    "        plt.xlim([X_LIM_LOW, X_LIM_HIGH])\n",
    "        plt.ylabel('Summed Probability')\n",
    "        plt.legend()\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "# End function to run the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9c48e-16f4-4a88-82b0-745917871c44",
   "metadata": {},
   "source": [
    "## Entire dataset excluding Roma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ad8ef-1c91-432b-bcef-509c1988456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis three times with different settings\n",
    "\n",
    "# (1) for all provinces excluding Roma\n",
    "print(\"Running analysis for all provinces excluding Roma...\")\n",
    "bootstrap_samples_dir_1 = \"bootstrap-samples/spa-ci/date-ranges/no-roma\"\n",
    "file_prefix_1 = \"no-roma\"\n",
    "lire_1 = lire[lire['province'] != 'Roma']\n",
    "run_analysis(lire_1, bootstrap_samples_dir_1, file_prefix_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe78bc3-f898-4697-85d0-bfa1faacb81a",
   "metadata": {},
   "source": [
    "## Latin-speaking provinces only (including Roma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7cd7a3-4f26-4adf-8101-55cb4abef5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) for only Latin-speaking provinces including Roma\n",
    "print(\"Running analysis for Latin-speaking provinces including Roma...\")\n",
    "bootstrap_samples_dir_2 = \"bootstrap-samples/spa-ci/date-ranges/latin-speaking\"\n",
    "file_prefix_2 = \"latin-speaking\"\n",
    "lire_2 = lire[lire['province_language'] == 'Latin']\n",
    "run_analysis(lire_2, bootstrap_samples_dir_2, file_prefix_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889f578-0cd1-457c-a7c5-3d32ccc8b33a",
   "metadata": {},
   "source": [
    "## Latin-speaking provinces only (excluding Roma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb0bc1-befa-4903-9d07-3f16a3e59212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) for only Latin-speaking provinces excluding Roma\n",
    "print(\"Running analysis for Latin-speaking provinces excluding Roma...\")\n",
    "bootstrap_samples_dir_3 = \"bootstrap-samples/spa-ci/date-ranges/latin-speaking-no-roma\"\n",
    "file_prefix_3 = \"latin-speaking-no-roma\"\n",
    "lire_3 = lire[(lire['province'] != 'Roma') & (lire['province_language'] == 'Latin')]\n",
    "run_analysis(lire_3, bootstrap_samples_dir_3, file_prefix_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b4d7b-fd73-428c-a527-06f98fd4b39a",
   "metadata": {},
   "source": [
    "# SPA of inscriptions from individual provinces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77393a86-98d2-4db2-b70c-fbc282128a56",
   "metadata": {},
   "source": [
    "## List all the provinces in the LIRE dataset, with number of inscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5c4f1-d205-4434-8a77-a5d056663848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in the 'province' column\n",
    "province_counts = lire['province'].value_counts()\n",
    "\n",
    "# Exclude the 'none' entries if applicable\n",
    "province_counts = province_counts[province_counts.index != 'none']\n",
    "\n",
    "# Filter out provinces with fewer than 100 inscriptions\n",
    "province_counts_filtered = province_counts[province_counts >= 100]\n",
    "\n",
    "# Print inscription counts by province\n",
    "print(\"Count of inscriptions by province (n>=100):\")\n",
    "print(province_counts_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a05b98-e59c-4487-b2f4-1f5d7f23235c",
   "metadata": {},
   "source": [
    "## SPA on all provinces with at least 100 inscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b5497-a738-4a51-8a7e-e7a84824e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "\n",
    "# Function to clean automatically generated filenames\n",
    "\n",
    "def clean_filename(filename):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', filename)\n",
    "\n",
    "# End function\n",
    "\n",
    "# Count the number of inscriptions for each unique province\n",
    "province_counts = lire['province'].value_counts()\n",
    "\n",
    "# Filter to only include provinces with at least 100 inscriptions\n",
    "filtered_provinces = province_counts[province_counts >= 100].index.tolist()\n",
    "\n",
    "# Initialize parameters for SPA\n",
    "n_iterations = 1000  # Number of bootstrap iterations\n",
    "earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "resolution = 5\n",
    "window_size = 5 # Size of the moving window for the moving average\n",
    "\n",
    "# Loop through each province\n",
    "for province in filtered_provinces:\n",
    "    # Create a dynamic file name for the province's bootstrap samples\n",
    "    filename = clean_filename(province)\n",
    "    saved_sample_path = f\"bootstrap-samples/spa-ci/provinces/{filename}.npy\"\n",
    "    \n",
    "    # Filter DataFrame based on the current province\n",
    "    subset_df = lire[lire['province'] == province]\n",
    "\n",
    "    # Get the number of inscriptions for this province\n",
    "    n_inscriptions = len(subset_df)\n",
    "\n",
    "    # Check if saved bootstrap samples exist for this province\n",
    "    if os.path.exists(saved_sample_path):\n",
    "        print(f\"Loading saved bootstrap samples for {province}...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "        # print(f\"Shape of loaded array: {bootstrap_summed_probs.shape}\") # debug ValueError shape mismatch\n",
    "    else:\n",
    "        print(f\"Performing bootstrap operation for {province}...\")\n",
    "        bootstrap_summed_probs = None\n",
    "        for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "            bootstrap_sample = subset_df.sample(n=len(subset_df), replace=True)\n",
    "            bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "            if bootstrap_summed_probs is None:\n",
    "                bootstrap_summed_probs = np.zeros((n_iterations, len(bootstrap_summed_prob)))\n",
    "            bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "\n",
    "        # debug_array = bootstrap_summed_probs # debug\n",
    "        # print(f\"Debug shape before saving: {debug_array.shape}\") # debug ValueError shape mismatch\n",
    "        # print(f\"First element when generated: {debug_array[0]}\") # debug ValueError shape mismatch\n",
    "        print(f\"Saving bootstrap samples for {province}...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "        # debug_array = np.load(saved_sample_path) # debug ValueError shape mismatch\n",
    "        # print(f\"Debug shape after saving: {debug_array.shape}\") # debug ValueError shape mismatch\n",
    "        # print(f\"First element when loaded: {debug_array[0]}\") # debug ValueError shape mismatch\n",
    "\n",
    "    # Calculate 95% confidence intervals\n",
    "    lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "    \n",
    "    # Calculate the summed probability for the subset_df\n",
    "    summed_prob, _ = compute_summed_probability(subset_df, earliest_date, latest_date, resolution)\n",
    "    \n",
    "    # Generate x-values for plotting\n",
    "    x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "    # Calculate moving average for summed_prob\n",
    "    moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "    \n",
    "    # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "    moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "    moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "    plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "    \n",
    "    plt.title(f\"Summed Probability Distribution for Province: {province} (n={n_inscriptions})\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.xlim([-100, 600])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9423e-f95b-4962-ad2e-9dacd5845821",
   "metadata": {},
   "source": [
    "# SPA of inscriptions from individual named cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a279b58-908e-4c2e-b35a-934f76c9d542",
   "metadata": {},
   "source": [
    "### List all cities with at least 100 inscriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5127ba1-3af6-4168-a02e-48940a57d121",
   "metadata": {},
   "source": [
    "Here, we count the records for each unique value (city) in the column 'urban_context_city'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1de55-9848-4474-abbc-2a146491352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique values in the 'urban_context_city' column\n",
    "city_counts = filtered_lire['urban_context_city'].value_counts()\n",
    "\n",
    "# Exclude the 'none' entries\n",
    "city_counts = city_counts[city_counts.index != 'none']\n",
    "\n",
    "# Filter out cities with fewer than 100 inscriptions\n",
    "city_counts_filtered = city_counts[city_counts >= 100]\n",
    "\n",
    "# Print inscription counts by city\n",
    "print(\"Count of inscriptions by city (with at least 100 inscriptions):\")\n",
    "print(city_counts_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca64423-5515-49a7-b3dc-b10a5fb6a0ab",
   "metadata": {},
   "source": [
    "## SPA on cities with at least 100 inscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3131685-65bd-4507-8f9a-100a7aa3743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges\n",
    "    - earliest_date, latest_date: the date limits for the distribution\n",
    "    - resolution: bin size for the histogram (in years)\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: summed probability distribution\n",
    "    - std_devs: standard deviations for each bin\n",
    "    \"\"\"\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Avoid negative or out-of-bounds indices\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Calculate uniform probability for the given date range\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute this uniform probability across the bins for the date range\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            summed_prob[i] += uniform_prob\n",
    "            std_devs[i] += (uniform_prob * (1 - uniform_prob))\n",
    "            \n",
    "    # Convert variances to standard deviations\n",
    "    std_devs = np.sqrt(std_devs)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "\n",
    "# Function to clean automatically generated filenames\n",
    "\n",
    "def clean_filename(filename):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', filename)\n",
    "\n",
    "# End function\n",
    "\n",
    "\n",
    "# Count the number of inscriptions of each unique city\n",
    "city_counts = lire['urban_context_city'].value_counts()\n",
    "\n",
    "# Filter the cities that have at least 100 inscriptions\n",
    "filtered_cities = city_counts[city_counts >= 100].index.tolist()\n",
    "\n",
    "# Optionally, print out the list and counts for verification\n",
    "# print(\"Filtered cities and their respective counts:\")\n",
    "# print(city_counts[city_counts >= 240])\n",
    "\n",
    "# Optionally, print the list of filtered cities\n",
    "# print(\"List of filtered cities:\")\n",
    "# print(filtered_cities)\n",
    "\n",
    "# Initialize parameters for SPA\n",
    "n_iterations = 1000  # Number of bootstrap iterations\n",
    "earliest_date, latest_date = lire['not_before'].min(), lire['not_after'].max()\n",
    "resolution = 5\n",
    "window_size = 5 # Size of the moving window for the moving average\n",
    "\n",
    "# Loop through each city\n",
    "for city in filtered_cities:\n",
    "    # Create a dynamic file name for the city's bootstrap samples\n",
    "    filename = clean_filename(city)\n",
    "    saved_sample_path = f\"bootstrap-samples/spa-ci/cities/{filename}.npy\"\n",
    "\n",
    "    \n",
    "    # Filter DataFrame based on the current city\n",
    "    subset_df = lire[lire['urban_context_city'] == city]\n",
    "\n",
    "    # Get the number of inscriptions for this city\n",
    "    n_inscriptions = len(subset_df)\n",
    "\n",
    "    # Check if saved bootstrap samples exist for this province\n",
    "    if os.path.exists(saved_sample_path):\n",
    "        # Load saved bootstrap samples\n",
    "        print(f\"Loading saved bootstrap samples for {city}...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "    else:\n",
    "        print(f\"Performing bootstrap operation for {city}...\")\n",
    "        # Initialize storage for bootstrap results\n",
    "        bootstrap_summed_probs = None\n",
    "        # Bootstrap sampling\n",
    "        for i in tqdm(range(n_iterations)):  # Loop wrapped with tqdm to display a progress bar\n",
    "            bootstrap_sample = subset_df.sample(n=len(subset_df), replace=True)\n",
    "            bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "            if bootstrap_summed_probs is None:\n",
    "                bootstrap_summed_probs = np.zeros((n_iterations, len(bootstrap_summed_prob)))\n",
    "            bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "        \n",
    "        # Save the bootstrap samples\n",
    "        print(f\"Saving bootstrap samples for {city}...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "        \n",
    "    # Calculate 95% confidence intervals\n",
    "    lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "    \n",
    "    # Calculate the summed probability for the subset_df\n",
    "    summed_prob, _ = compute_summed_probability(subset_df, earliest_date, latest_date, resolution)\n",
    "    \n",
    "    # Generate x-values for plotting\n",
    "    x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "    \n",
    "    # Calculate moving average for summed_prob\n",
    "    moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "    \n",
    "    # Calculate moving average for the lower and upper bounds of the 95% CI\n",
    "    moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "    moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "    plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "    \n",
    "    plt.title(f\"Summed Probability Distribution for {city} (n={n_inscriptions})\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.xlim([-100, 600])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f0a9d-7c47-443a-9095-1237e3b550d7",
   "metadata": {},
   "source": [
    "## Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ef47f-7e48-4d3a-8b84-46ebcb7ec3b9",
   "metadata": {},
   "source": [
    "I need to:\n",
    "\n",
    "* Rerun empire-wide statistics without Rome - DONE\n",
    "* Eliminate non-Latin-speaking provinces - DONE\n",
    "* Decide what inscriptions to include in analyses (<=200-year date range? 300 year? All?)\n",
    "* Finalise sample-size cut-offs for analysing urban places (250 inscriptions? 500? 750?)\n",
    "* Save bootstrapping data to files throughout so that it all runs faster - DONE\n",
    "* Run on BinderHub\n",
    "* Re-run Hanson's analysis comparing urban populations from other sources (his work, included in LIRE) to the number of inscriptions, with residuals for urban places\n",
    "* Run Hanson's analysis amalgamating to the province level\n",
    "* Run Hanson's anlysis amalgamating to the Latin-speaking-empire level\n",
    "* Consider whether Hanson's analysis can be improved by looking at duration of occupation; it seems to me that he simply presents the (maximum?) size of the settlement, ignoring duration of occuption, which may complicate the relationship between the number of inscriptions and the population - longer-lived settlements will produce more inscriptions at the same population size. Maybe some sort of dirivative metric, like average number of inscriptions per year at the site.\n",
    "* Using whatever coefficients of coreleation that produces, calculate changing population over time using SPA on inscriptions\n",
    "* Extrapolate / amalgamate to province-level and (Latin speaking) empire-level\n",
    "* Do plots showing changing contributions to overall population (itself changing) of cities --> provinces, provinces --> Latin-speaking empire.\n",
    "* Do the analysis based on word count per location instead of inscription count\n",
    "\n",
    "The overal point or hook is the ability not only to get a static (maximum?) urban place size (like Hanson has done), but to plot population over time using inscriptions as a proxy, where enough inscriptions exist, and do it all responsibly with appropriate confidence intervals and qualifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb282734-3530-42ad-aec8-881ce63966d2",
   "metadata": {},
   "source": [
    "# Extending Hanson 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d6c99-7a2b-454d-96cf-3755361c357b",
   "metadata": {},
   "source": [
    "## Reproduce Hanson's correlation between inscription count and his independent population estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7957b-7788-4393-89d6-71cafc3275f8",
   "metadata": {},
   "source": [
    "Hanson (2021, 143) briefly discusses SPA using inscriptions, although he references Wilson (2009) on shipwrecks rather than the extensive literature on radiocarbon dates. He then uses the SPA only to support the broad observation that most of the inscriptions date to the first and second century AD, and therefor correspond to the time when the population was greatest. Before moving on to a diachronic analysis that correlates the total number of inscriptions from each site to the maximum population of the site, he observes that 'using more refined date ranges would be an interesting matter for furture research'. This analysis takes up that challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650aab8-d87f-4879-8d77-73710ee128f2",
   "metadata": {},
   "source": [
    "### Filter named 'urban_context_city' records and display them with population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7fe9d-e32a-483b-9953-59c8b95cd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to exclude rows where 'urban_context_city' is 'None'\n",
    "# or where 'urban_context_pop_est' is NaN\n",
    "filtered_df = lire[(lire['urban_context_city'] != 'None') & (lire['urban_context_pop_est'].notna())]\n",
    "\n",
    "# Get unique cities that have a population estimate\n",
    "unique_cities_with_pop = filtered_df[['urban_context_city', 'urban_context_pop_est']].drop_duplicates()\n",
    "\n",
    "# Sort the DataFrame by population estimate in descending order\n",
    "sorted_unique_cities = unique_cities_with_pop.sort_values(by='urban_context_pop_est', ascending=False)\n",
    "\n",
    "# Display the first 10 records of the sorted DataFrame using to_string() for column spacing\n",
    "print(sorted_unique_cities.head(10).to_string(index=False, col_space=12))\n",
    "\n",
    "# Display the total number of unique cities with population estimates\n",
    "total_records = sorted_unique_cities.shape[0]\n",
    "print(f\"Total number of unique cities with population estimates: {total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df02e0-6d18-4cff-a9a0-2fe78fd20caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create new df including city name, population estimate, and inscription count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2600a-06fa-4fc5-a158-8ca65b5ba0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of unique cities that have population estimates\n",
    "unique_cities_with_pop = lire[(lire['urban_context_city'] != 'None') & (lire['urban_context_pop_est'].notna())]['urban_context_city'].unique()\n",
    "\n",
    "# Group by 'urban_context_city' and 'urban_context_pop_est', then count the number of inscriptions for each city\n",
    "grouped_df = lire[(lire['urban_context_city'] != 'None') & (lire['urban_context_pop_est'].notna())].groupby(['urban_context_city', 'urban_context_pop_est']).size().reset_index(name='inscription_count')\n",
    "\n",
    "# Sort the DataFrame by 'urban_context_pop_est' in descending order\n",
    "grouped_df_sorted = grouped_df.sort_values(by='urban_context_pop_est', ascending=False)\n",
    "\n",
    "# Display the DataFrame in a tight layout\n",
    "print(\"\\nTop 10 cities sorted by population estimate, along with their inscription counts:\")\n",
    "print(grouped_df_sorted.head(10).to_string(index=False, col_space=12))\n",
    "\n",
    "# Display the total number of records in the grouped DataFrame\n",
    "total_records_grouped = grouped_df_sorted.shape[0]\n",
    "print(f\"\\nTotal number of records in the DataFrame: {total_records_grouped}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6ba05-7314-4aca-bebb-fc282ad40998",
   "metadata": {},
   "source": [
    "#### Filter for Latin-speaking only, and exclude Roma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873bf9e-f457-4cb1-80fa-620ee962a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only Latin-speaking provinces and exclude the city 'Roma'\n",
    "latin_provinces_df = lire[(lire['province_language'] == 'Latin') & (lire['urban_context_city'] != 'Roma')]\n",
    "\n",
    "# Generate a list of unique cities in Latin-speaking provinces that have population estimates\n",
    "unique_cities_with_pop = latin_provinces_df[(latin_provinces_df['urban_context_city'] != 'None') & (latin_provinces_df['urban_context_pop_est'].notna())]['urban_context_city'].unique()\n",
    "\n",
    "# Group by 'urban_context_city' and 'urban_context_pop_est', then count the number of inscriptions for each city\n",
    "grouped_df = latin_provinces_df[(latin_provinces_df['urban_context_city'] != 'None') & (latin_provinces_df['urban_context_pop_est'].notna())].groupby(['urban_context_city', 'urban_context_pop_est']).size().reset_index(name='inscription_count')\n",
    "\n",
    "# Sort the DataFrame by 'urban_context_pop_est' in descending order\n",
    "grouped_df_sorted = grouped_df.sort_values(by='urban_context_pop_est', ascending=False)\n",
    "\n",
    "# Display the DataFrame in a tight layout\n",
    "print(\"\\nTop 10 cities in Latin-speaking provinces sorted by population estimate, along with their inscription counts:\")\n",
    "print(grouped_df_sorted.head(10).to_string(index=False, col_space=12))\n",
    "\n",
    "# Display the total number of records in the grouped DataFrame\n",
    "total_records_grouped = grouped_df_sorted.shape[0]\n",
    "print(f\"\\nTotal number of records in the DataFrame: {total_records_grouped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e1024-b0a4-4b72-8bfc-3c919df220fe",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS) regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db45143-5b0f-44f6-b686-832860712f3f",
   "metadata": {},
   "source": [
    "### Reproducing Hanson's appraoch to determining a relationship between city poputlation and inscription count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74965c1a-f821-4276-b994-542cb64ce80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for OLS regression\n",
    "# Note: statsmodels requires us to add a constant term for the intercept\n",
    "X = grouped_df_sorted['urban_context_pop_est']  # Predictor variable (Population)\n",
    "y = grouped_df_sorted['inscription_count']  # Response variable (Number of inscriptions)\n",
    "X = sm.add_constant(X)  # Adding a constant for the intercept term\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the model statistics\n",
    "print(model.summary())\n",
    "\n",
    "# Generate predicted values and standard errors\n",
    "predictions = model.predict(X)\n",
    "pred_std = model.get_prediction(X).se_obs\n",
    "\n",
    "# Calculate upper and lower 95% confidence intervals\n",
    "ci_upper = predictions + 1.96 * pred_std\n",
    "ci_lower = predictions - 1.96 * pred_std\n",
    "\n",
    "# Plot the data and the OLS fit line along with confidence intervals\n",
    "plt.scatter(X['urban_context_pop_est'], y, label='Data', alpha=0.5)\n",
    "plt.xlabel('Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('OLS Fit with 95% Confidence Interval: Relationship between Population and Inscription Count')\n",
    "plt.plot(X['urban_context_pop_est'], predictions, color='red', label='OLS Fit Line')\n",
    "plt.fill_between(X['urban_context_pop_est'], ci_lower, ci_upper, color='gray', alpha=0.2, label='95% CI')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d259b-c13a-40c0-a059-0c6cf81305a3",
   "metadata": {},
   "source": [
    "Hypothesis: cities of higher population produce more inscriptions. Population is the predictor variable and inscriptions are the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba54ad-92bc-4607-8878-56a00b418ded",
   "metadata": {},
   "source": [
    "### Interpreting OLS regression results: ChatGPT help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72ab17-744b-44a4-aabd-7ee8741cfd5c",
   "metadata": {},
   "source": [
    "The \\( R^2 \\) value, also known as the coefficient of determination, is a measure of how well the model's predictions match the actual data. An \\( R^2 \\) value of 1 indicates that the model perfectly predicts the outcome variable, whereas a value of 0 indicates that the model is no better than a model that simply predicts the mean of the target variable for all observations.\n",
    "\n",
    "In your case, an \\( R^2 \\) value of 0.039 suggests that only about 3.9% of the variability in the population estimate can be explained by the number of inscriptions. This is a very low value and indicates a weak relationship between the two variables, according to the model.\n",
    "\n",
    "Here are some considerations for interpreting this \\( R^2 \\) value:\n",
    "\n",
    "1. **Low Predictive Power:** The model has low predictive power for estimating population based on the number of inscriptions.\n",
    "  \n",
    "2. **Other Factors:** Given the low \\( R^2 \\), it's likely that other factors, not included in the model, have a significant impact on the population estimate.\n",
    "\n",
    "3. **Linear Fit Might Not Be Appropriate:** The low \\( R^2 \\) could also mean that the relationship between the variables is not linear, and a linear model is not the best fit for the data.\n",
    "\n",
    "4. **Data Quality:** Always consider the possibility that the data itself might have issues. For example, if the population estimates are not reliable or if the inscription counts are incomplete, that could affect the \\( R^2 \\) value.\n",
    "\n",
    "So, while the \\( R^2 \\) value is a useful statistic, it's important to consider it in the context of your specific study, the quality of your data, and your understanding of the underlying processes that generate that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f7581-f69d-4c81-84f7-c3d14ba9a4b8",
   "metadata": {},
   "source": [
    "The F-statistic is a measure used to assess the significance of the overall regression model. It is used in the context of an F-test where it's compared to a critical value that depends on the degrees of freedom. A larger F-statistic indicates that the model is more likely to be statistically significant, i.e., at least one of the predictor variables in the regression model is statistically significant.\n",
    "\n",
    "In your case, an F-statistic of 32.94 suggests that the regression model is statistically significant at conventional levels (usually \\( p < 0.05 \\)), meaning that there is evidence to reject the null hypothesis that all of the regression coefficients are equal to zero (i.e., that none of the predictors are useful).\n",
    "\n",
    "However, it's essential to interpret the F-statistic in the context of the data and the subject-matter:\n",
    "\n",
    "1. **Significance Doesn't Mean Effect Size:** While the F-statistic tells you whether or not the model as a whole is statistically significant, it doesn't tell you how strong the relationship is. For that, you'll have to look at measures like \\( R^2 \\).\n",
    "\n",
    "2. **Data Quality:** As with \\( R^2 \\), the F-statistic can also be influenced by the quality of your data. If the data are noisy, incomplete, or biased, the F-statistic may be misleading.\n",
    "\n",
    "3. **Multiple Predictors:** In models with multiple predictors, the F-statistic tests whether at least one predictor is useful for predicting the response variable. However, in your case, you have only one predictor, making the F-statistic and the t-test for the predictor's coefficient essentially provide the same information.\n",
    "\n",
    "So, the F-statistic of 32.94 suggests that your model is statistically significant, but given the low \\( R^2 \\), the practical relevance of this finding might be limited. The model accounts for a very small proportion of the variance in the response variable, indicating a weak relationship despite statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd8ece-bbf4-4697-bf9f-7a5d679c4371",
   "metadata": {},
   "source": [
    "Certainly, let's discuss the table line by line.\n",
    "\n",
    "#### Intercept (const)\n",
    "\n",
    "The constant term (`const`) in the regression equation is the value of the dependent variable (the number of inscriptions in this case) when all independent variables are zero. Here, the `coef` (coefficient) value for `const` is 59.7384. This means that if a city had a population of zero (which is not practical in real-world terms), the model predicts that there would be approximately 59.7 inscriptions.\n",
    "\n",
    "#### std err (10.787)\n",
    "This represents the standard error of the coefficient estimate for the intercept. The smaller the standard error, the more precise the estimate. \n",
    "\n",
    "#### t (5.538)\n",
    "This is the t-value, which is calculated by dividing the coefficient by its standard error. The higher the t-value, the stronger the evidence against the null hypothesis, which suggests no effect.\n",
    "\n",
    "#### P>|t| (0.000)\n",
    "This p-value being close to zero indicates that the intercept is statistically significant, meaning that the observed number of inscriptions when all independent variables are zero would not be due to random chance.\n",
    "\n",
    "#### [0.025 0.975] (38.565 80.912)\n",
    "These are the 95% confidence intervals for the intercept. The model is 95% confident that the true intercept value lies between 38.565 and 80.912.\n",
    "\n",
    "#### Coefficient for urban_context_pop_est (0.0055)\n",
    "\n",
    "The coefficient for `urban_context_pop_est` tells us about the size and direction of the relationship between population and the number of inscriptions. A coefficient of 0.0055 means that for each unit increase in population, the number of inscriptions is expected to increase by 0.0055 units.\n",
    "\n",
    "#### std err (0.001)\n",
    "The standard error of 0.001 indicates the accuracy of this coefficient.\n",
    "\n",
    "#### t (5.739)\n",
    "The t-value of 5.739 is high, further supporting the rejection of the null hypothesis that this variable has no effect.\n",
    "\n",
    "#### P>|t| (0.000)\n",
    "The p-value being close to zero suggests that this relationship is statistically significant.\n",
    "\n",
    "#### [0.025 0.975] (0.004 0.007)\n",
    "The 95% confidence interval indicates that we are 95% confident that the true coefficient value lies between 0.004 and 0.007.\n",
    "\n",
    "In summary, the model is statistically significant, and both the intercept and the population variable appear to have a statistically significant relationship with the number of inscriptions. However, as discussed earlier, the R-squared value suggests that population alone does not explain much of the variance in the number of inscriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277ac1a-f567-436f-8f9b-6ea6ab7e1c71",
   "metadata": {},
   "source": [
    "## Considering alternative approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b96872-ce45-4c32-9524-b293e4824686",
   "metadata": {},
   "source": [
    "Certainly! Here's an expanded explanation for each approach:\n",
    "\n",
    "### Count Data Models\n",
    "1. **Poisson Regression**: This approach is useful for count data like the number of inscriptions. It assumes that the mean and variance of the distribution are equal. This is a common starting point for analyzing count data but can be too restrictive if the data is overdispersed.\n",
    "   \n",
    "2. **Negative Binomial Regression**: This is an extension of Poisson regression that introduces an extra parameter to account for overdispersion—when the data exhibits more variability than what the Poisson model can handle.\n",
    "\n",
    "### Linear Models & Extensions\n",
    "3. **Generalized Linear Models (GLM)**: GLMs are a flexible generalization of ordinary linear models, allowing for response variables that have error distribution models other than a normal distribution. The link function specifies the relationship between the linear predictor and the mean of the response variable.\n",
    "   \n",
    "4. **Robust Regression**: This technique is similar to OLS but less sensitive to outliers and high-leverage points. It modifies the loss function to be more forgiving of such data points.\n",
    "   \n",
    "5. **Ridge and Lasso Regression**: These are linear models with a twist. They include a penalty term on the coefficients to prevent them from getting too large, essentially regularizing the model to prevent overfitting.\n",
    "  \n",
    "6. **Polynomial Regression**: This technique fits a nonlinear equation to the data but does it within the framework of linear regression. It's useful when the relationship between variables is clearly not linear.\n",
    "   \n",
    "7. **Mixed-Effects Models**: Useful when there's a grouping variable (e.g., province, time period) that could introduce correlations between observations. This model accounts for both fixed and random effects.\n",
    "\n",
    "### Machine Learning Models\n",
    "8. **Random Forests**: This ensemble method uses multiple decision trees during training and outputs the average prediction of the individual trees for regression problems. It's good for capturing complex relationships but sacrifices some interpretability.\n",
    "\n",
    "9. **Gradient Boosting**: Another ensemble technique that builds trees one at a time, where each one corrects the errors of its predecessor. Like Random Forests, it can model complex relationships but can be harder to interpret.\n",
    "\n",
    "### Bayesian Methods\n",
    "10. **Bayesian Linear Regression**: This approach offers a probabilistic framework that goes beyond point estimates to provide a full distribution for the model's parameters. This accounts for uncertainty and can produce more robust results.\n",
    "  \n",
    "### Non-parametric Models\n",
    "11. **Spearman's Rank Correlation**: This is a non-parametric test that assesses the strength and direction of the relationship between two variables without making any assumptions about the relationship's form.\n",
    "\n",
    "12. **Kernel Regression**: This is a more flexible non-parametric technique that does not assume a specific functional form of the relationship between variables.\n",
    "\n",
    "### Miscellaneous\n",
    "13. **Bootstrapping**: This resampling technique is useful for small datasets or when the underlying distribution is unknown. It can provide more robust estimates by repeatedly sampling with replacement from the data.\n",
    "  \n",
    "14. **Log-Transformation**: If the variables span several orders of magnitude or if the relationship seems multiplicative, applying a log transformation can linearize the relationship, making it easier to model.\n",
    "\n",
    "Would you like to proceed with any of these methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f155a6c-58f3-4551-a5e6-59b03d119657",
   "metadata": {},
   "source": [
    "## Log transformation of OSL regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5b057-c8e2-46f6-b2f4-4a77cf928028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the variables\n",
    "grouped_df_sorted['log_urban_context_pop_est'] = np.log1p(grouped_df_sorted['urban_context_pop_est'])\n",
    "grouped_df_sorted['log_inscription_count'] = np.log1p(grouped_df_sorted['inscription_count'])\n",
    "\n",
    "# Prepare data for OLS regression after log-transformation\n",
    "X_log = grouped_df_sorted['log_urban_context_pop_est']  # Log-transformed predictor variable\n",
    "y_log = grouped_df_sorted['log_inscription_count']  # Log-transformed response variable\n",
    "X_log = sm.add_constant(X_log)  # Adding a constant for the intercept term\n",
    "\n",
    "# Fit the OLS model on log-transformed data\n",
    "model_log = sm.OLS(y_log, X_log).fit()\n",
    "\n",
    "# Print out the model statistics\n",
    "print(model_log.summary())\n",
    "\n",
    "# Get prediction standard errors for 95% confidence interval\n",
    "_, iv_l, iv_u = wls_prediction_std(model_log)\n",
    "\n",
    "# Plot the data and the OLS fit line for log-transformed data\n",
    "plt.scatter(X_log['log_urban_context_pop_est'], y_log, label='Log-Transformed Data', alpha=0.7)\n",
    "plt.xlabel('Log(Population Estimate)')\n",
    "plt.ylabel('Log(Inscription Count)')\n",
    "plt.title('OLS Fit on Log-Transformed Data with 95% Confidence Interval')\n",
    "\n",
    "# Plot OLS fit line\n",
    "plt.plot(X_log['log_urban_context_pop_est'], model_log.predict(X_log), color='red', label='OLS Fit Line')\n",
    "\n",
    "# Plot 95% confidence interval\n",
    "plt.fill_between(X_log['log_urban_context_pop_est'], iv_l, iv_u, color='gray', alpha=0.3, label='95% Conf. Int.')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886a7f3-bb35-4661-8130-ba5e158cff52",
   "metadata": {},
   "source": [
    "## Interpretation of log-transformed OLS results - ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc17668-7b69-4f76-8d70-62c28f367316",
   "metadata": {},
   "source": [
    "Certainly! Let's walk through the output of the log-transformed OLS regression model.\n",
    "\n",
    "### General Information\n",
    "\n",
    "- **Dep. Variable**: This is the dependent (response) variable, which is `log_inscription_count` after log-transformation.\n",
    "- **R-squared**: This statistic indicates the proportion of the variance for the dependent variable that's explained by the independent variable. In this case, the R-squared value of 0.101 indicates that approximately 10.1% of the variation in log-transformed inscription count is explained by the log-transformed population estimate.\n",
    "- **Adj. R-squared**: Similar to R-squared but adjusted for the number of predictors. Since we have only one predictor, this value is almost the same as R-squared.\n",
    "- **F-statistic**: The F-statistic tests whether at least one predictor variable has a non-zero coefficient. An F-statistic of 91.56 is quite high, indicating that the model is likely a good fit.\n",
    "- **Prob (F-statistic)**: This is the probability of observing a test statistic as extreme as the one for your model if the null hypothesis is true. A value close to zero means the model is statistically significant.\n",
    "- **Log-Likelihood**: The log of the likelihood function for the estimated model. Generally, higher values are better, though this metric is mostly useful for comparing different models fitted to the same dataset.\n",
    "- **AIC and BIC**: These are the Akaike and Bayesian Information Criteria, respectively. These metrics offer a balance between goodness-of-fit and model complexity, with lower values generally indicating better models.\n",
    "\n",
    "### Coefficients Table\n",
    "\n",
    "- **coef**: These are the estimated coefficients. For `log_urban_context_pop_est`, it's 0.4725, meaning that for each unit increase in log-transformed population estimate, we expect an increase of 0.4725 in log-transformed inscription count.\n",
    "- **std err**: This is the standard error of the coefficient estimate, useful for hypothesis tests and for constructing confidence intervals.\n",
    "- **t**: This is the t-statistic, calculated by dividing the coefficient by its standard error. Higher absolute values generally indicate greater significance.\n",
    "- **P>|t|**: The p-value associated with the t-statistic. A p-value close to zero indicates that the predictor is a significant predictor of the outcome variable.\n",
    "- **[0.025, 0.975]**: These are the 95% confidence intervals for the coefficient. Since this interval does not contain zero for `log_urban_context_pop_est`, it's a sign that this variable is a significant predictor.\n",
    "\n",
    "### Additional Diagnostics\n",
    "\n",
    "- **Omnibus, Durbin-Watson, Jarque-Bera (JB), Skew, Kurtosis, and Cond. No.**: These are additional tests and metrics that assess the residuals and other model assumptions. They are generally more important when diagnosing issues with the regression model.\n",
    "\n",
    "Based on these results, the log-transformed model seems to provide a better fit compared to the original model, but still, only around 10.1% of the variation in inscription count is explained by population size. The model is statistically significant, and the coefficient for log-transformed population size is also statistically significant.\n",
    "\n",
    "I hope that helps clarify the output! Would you like to move on to trying out some other modeling techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221fa61-87e9-46e1-8034-877a924dae30",
   "metadata": {},
   "source": [
    "Certainly! This table provides detailed statistics for each coefficient (constant and predictor) in the model. Let's look at each row in detail:\n",
    "\n",
    "### Constant (Intercept)\n",
    "- **coef = -0.5180**: This is the y-intercept of the regression line in the log-transformed space. It represents the expected value of the log-transformed inscription count when the log-transformed population estimate is zero. Given that both variables are log-transformed, the interpretation is not as straightforward as in a simple linear regression.\n",
    "  \n",
    "- **std err = 0.391**: This is the standard error of the coefficient. It gives an idea of the uncertainty around the estimate of the intercept.\n",
    "\n",
    "- **t = -1.326**: This is the t-statistic for the hypothesis test that checks whether this coefficient is different from zero. A t-value far from zero indicates that the coefficient is statistically significant, but here the value is relatively close to zero.\n",
    "\n",
    "- **P>|t| = 0.185**: This is the p-value for the test of the null hypothesis that this coefficient is zero. A value greater than 0.05 usually suggests that the coefficient is not statistically significant.\n",
    "\n",
    "- **[0.025, 0.975] = [-1.285, 0.249]**: This is the 95% confidence interval for the coefficient. Since this interval contains zero, it's another sign that the constant is not statistically significant.\n",
    "\n",
    "### log_urban_context_pop_est (Predictor)\n",
    "- **coef = 0.4725**: This is the slope of the regression line in the log-transformed space. It means that for a one-unit increase in the log-transformed population, we expect a 0.4725-unit increase in the log-transformed inscription count.\n",
    "\n",
    "- **std err = 0.049**: This is the standard error of the coefficient. Smaller values indicate more precise estimates.\n",
    "\n",
    "- **t = 9.568**: This t-statistic is far from zero, suggesting that the predictor is statistically significant.\n",
    "\n",
    "- **P>|t| = 0.000**: A p-value close to zero confirms that this predictor is statistically significant.\n",
    "\n",
    "- **[0.025, 0.975] = [0.376, 0.569]**: This is the 95% confidence interval for the predictor coefficient. Since it does not contain zero, we can say with 95% confidence that an increase in log-transformed population is associated with an increase in log-transformed inscription count.\n",
    "\n",
    "I hope this detailed walk-through clarifies each part of the table! Would you like to proceed with the other modeling techniques now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3a440-cd51-498e-a47d-f5408bd468b5",
   "metadata": {},
   "source": [
    "The statistical significance of the constant (intercept) and predictor variables in a regression model have different implications:\n",
    "\n",
    "### Constant (Intercept) Not Statistically Significant\n",
    "- **What it means**: The intercept being not statistically significant means that when the predictor variable is zero (in this case, when the log-transformed population is zero), we can't confidently say that the log-transformed inscription count would differ from zero. Given that both variables are log-transformed, this isn't a straightforward interpretation as it would be in a simple linear regression.\n",
    "  \n",
    "- **Implications**: In most practical situations, especially in this case where both variables are log-transformed, the intercept being non-significant isn't typically a concern. You're generally more interested in the relationship between the variables, not the absolute value of the output when the input is zero. \n",
    "\n",
    "### Predictor Statistically Significant\n",
    "- **What it means**: This is usually what you are most interested in. A statistically significant predictor suggests there's evidence to reject the null hypothesis that the predictor has no effect on the outcome variable. In other words, there's a relationship between the population and inscription count, at least in the dataset you're working with.\n",
    "\n",
    "- **Implications**: Since the predictor is significant, the focus will often be on understanding this relationship further, possibly by considering more complex models, adding additional predictors, or investigating causality (which regression alone cannot establish).\n",
    "\n",
    "### Summary\n",
    "In a nutshell, the constant's lack of significance is usually not a major issue if you are interested in the relationship between variables, especially in cases like this where both the predictor and the response are log-transformed. The statistical significance of the predictor variable is more critical for your analysis, as it suggests that there's a relationship between city population and inscription count that is worth investigating further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d479401-f6f8-40ed-85a7-fc3b6a302f13",
   "metadata": {},
   "source": [
    "## How disperesed is my data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c77f46-c4ef-425a-9f60-c6b4b0318985",
   "metadata": {},
   "source": [
    "Choice of regression model partly depends upon how dispersed the data is - here we run a check for dispersal, comparing mean inscription count to the variance of inscription count. If the latter is much higher, the data is 'dispersed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f4a14-de02-4b85-aaa5-740edc0d2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and variance of the inscription count\n",
    "mean_inscription_count = grouped_df_sorted['inscription_count'].mean()\n",
    "var_inscription_count = grouped_df_sorted['inscription_count'].var()\n",
    "\n",
    "print(f\"Mean of Inscription Count: {mean_inscription_count}\")\n",
    "print(f\"Variance of Inscription Count: {var_inscription_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb183bfc-a1ce-4bcd-8964-efd4d92d8ffe",
   "metadata": {},
   "source": [
    "Yeah, that's a high ratio, data is definitely 'dispersed'. Let's use a Negative Binomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542b1df-8dfa-4e3d-a8af-8b47a0846327",
   "metadata": {},
   "source": [
    "## Negative Binomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7b98d-d25b-4f3e-9c16-667eeede4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data and fit the Negative Binomial model (as before)\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Generate predictions and calculate the confidence interval for new observations\n",
    "predictions = model.get_prediction(X)\n",
    "frame = predictions.summary_frame(alpha=0.05)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(X['log_urban_context_pop_est'], frame['mean'], label='Fitted', color='red')\n",
    "\n",
    "# Plot the 95% confidence intervals\n",
    "plt.fill_between(X['log_urban_context_pop_est'], frame['mean_ci_lower'], frame['mean_ci_upper'], color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Negative Binomial Regression Fit')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c3da7-5f36-477c-8739-8ed1defc09a6",
   "metadata": {},
   "source": [
    "## Interpretation from ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa6b04-420f-4954-935e-6762c8e24171",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the important parts of the Negative Binomial Regression output:\n",
    "\n",
    "1. **Dep. Variable**: `inscription_count` - This tells us the dependent variable we're trying to model.\n",
    "  \n",
    "2. **No. Observations**: `816` - This is the number of observations (i.e., data points) used for the model.\n",
    "  \n",
    "3. **Model Family**: `NegativeBinomial` - This confirms that the model uses the Negative Binomial distribution.\n",
    "  \n",
    "4. **Link Function**: `Log` - The log link function means that we're modeling the natural logarithm of the expected count as a linear function of the predictor variables.\n",
    "  \n",
    "5. **Method**: `IRLS` (Iteratively Reweighted Least Squares) - This is the optimization algorithm used to find the best-fitting model.\n",
    "  \n",
    "6. **Log-Likelihood**: `-4256.0` - The log-likelihood measures how well the model fits the data. The closer this value is to 0, the better the model fits the data.\n",
    "  \n",
    "7. **Deviance**: `1781.9` - This is a measure of goodness-of-fit. Lower values suggest that the model fits the data better.\n",
    "  \n",
    "8. **Pearson chi2**: `3.26e+03` - This is another goodness-of-fit measure. Like Deviance, lower values are better.\n",
    "  \n",
    "9. **No. Iterations**: `9` - The number of iterations taken by the IRLS algorithm to converge to a solution.\n",
    "  \n",
    "10. **Pseudo R-squ. (CS)**: `0.4207` - This is a measure of the proportion of variance explained by the model. It's analogous to the R-squared value in OLS regression, but keep in mind that it's not directly comparable. The value suggests that the model explains about 42.07% of the variance in the inscription count.\n",
    "\n",
    "11. **Covariance Type**: `nonrobust` - This specifies the type of covariance estimator used to calculate the standard errors. \"Nonrobust\" here simply means that no additional corrections were applied to the standard errors.\n",
    "\n",
    "The absence of p-values and confidence intervals in the table may vary depending on how the software package reports results. You might need to invoke specific methods to get these values, as they are essential for hypothesis testing.\n",
    "\n",
    "Overall, the Pseudo R-squared value indicates that the model explains a reasonable proportion of the variability in inscription counts, but it's not a perfect model by any means. Given that the model seems to be statistically significant, the predictor (population) is meaningful for predicting the inscription count, at least to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0f16d-27fd-42bb-bae1-7d89bec33abb",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the important parts of the Negative Binomial Regression output:\n",
    "\n",
    "1. **Dep. Variable**: `inscription_count` - This tells us the dependent variable we're trying to model.\n",
    "  \n",
    "2. **No. Observations**: `816` - This is the number of observations (i.e., data points) used for the model.\n",
    "  \n",
    "3. **Model Family**: `NegativeBinomial` - This confirms that the model uses the Negative Binomial distribution.\n",
    "  \n",
    "4. **Link Function**: `Log` - The log link function means that we're modeling the natural logarithm of the expected count as a linear function of the predictor variables.\n",
    "  \n",
    "5. **Method**: `IRLS` (Iteratively Reweighted Least Squares) - This is the optimization algorithm used to find the best-fitting model.\n",
    "  \n",
    "6. **Log-Likelihood**: `-4256.0` - The log-likelihood measures how well the model fits the data. The closer this value is to 0, the better the model fits the data.\n",
    "  \n",
    "7. **Deviance**: `1781.9` - This is a measure of goodness-of-fit. Lower values suggest that the model fits the data better.\n",
    "  \n",
    "8. **Pearson chi2**: `3.26e+03` - This is another goodness-of-fit measure. Like Deviance, lower values are better.\n",
    "  \n",
    "9. **No. Iterations**: `9` - The number of iterations taken by the IRLS algorithm to converge to a solution.\n",
    "  \n",
    "10. **Pseudo R-squ. (CS)**: `0.4207` - This is a measure of the proportion of variance explained by the model. It's analogous to the R-squared value in OLS regression, but keep in mind that it's not directly comparable. The value suggests that the model explains about 42.07% of the variance in the inscription count.\n",
    "\n",
    "11. **Covariance Type**: `nonrobust` - This specifies the type of covariance estimator used to calculate the standard errors. \"Nonrobust\" here simply means that no additional corrections were applied to the standard errors.\n",
    "\n",
    "The absence of p-values and confidence intervals in the table may vary depending on how the software package reports results. You might need to invoke specific methods to get these values, as they are essential for hypothesis testing.\n",
    "\n",
    "Overall, the Pseudo R-squared value indicates that the model explains a reasonable proportion of the variability in inscription counts, but it's not a perfect model by any means. Given that the model seems to be statistically significant, the predictor (population) is meaningful for predicting the inscription count, at least to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a03840-fdbf-4e6d-a139-e4ef6a379980",
   "metadata": {},
   "source": [
    "In a negative binomial regression model with a log link, the exponentiated coefficient represents the multiplicative effect on the dependent variable for a one-unit change in the predictor variable.\n",
    "\n",
    "So, to transform the log coefficient into real-world numbers, you would take the exponent of the coefficient for `log_urban_context_pop_est`, which is \\(e^{0.6866}\\).\n",
    "\n",
    "Calculating, we get:\n",
    "\\[\n",
    "e^{0.6866} \\approx 1.9871\n",
    "\\]\n",
    "\n",
    "Interpreted in the context of your study, this means that for each one-unit increase in the natural logarithm of the population, you can expect the number of inscriptions to be multiplied by approximately 1.9871.\n",
    "\n",
    "To make this even more concrete, let's say you have a city with a population of 1000. If the population increases to \\(e \\times 1000 \\approx 2718.28\\), the number of inscriptions would be expected to increase by a factor of about 1.9871. \n",
    "\n",
    "It's important to note that this is a multiplicative effect on the count, not an additive effect. So, if your city with 1000 people had, say, 50 inscriptions, a population increase to about 2718 would be expected to yield \\(50 \\times 1.9871 \\approx 99.355\\) inscriptions.\n",
    "\n",
    "Do note that this is an estimate and that real-world data often include other factors and variations not captured in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249aa77-e325-459e-b46e-e7392ca23000",
   "metadata": {},
   "source": [
    "The suitability of a Negative Binomial Regression model depends on several factors:\n",
    "\n",
    "1. **Count Data**: Your dependent variable, \"Inscription Count,\" is count data, which fits the assumptions of negative binomial regression.\n",
    "\n",
    "2. **Overdispersion**: Negative binomial regression is especially useful when the variance is greater than the mean, which is a sign of overdispersion. This is common in many types of count data and is a situation where Poisson regression models, for instance, may not be appropriate.\n",
    "\n",
    "3. **Log-Link Function**: The log transformation you applied makes sense given that your data spans several orders of magnitude. A log-link function is typically used for negative binomial regression and is well-suited to capture multiplicative effects on the dependent variable.\n",
    "\n",
    "4. **Interpretability**: Negative binomial models are relatively straightforward to interpret, especially when the link function is logarithmic.\n",
    "\n",
    "5. **Model Fit and Significance**: From the statistical output, it appears that the model fits the data better than the simple OLS regression, given the significance of the predictors and the pseudo R-squared value.\n",
    "\n",
    "6. **Complexity**: Negative binomial regression models are more complex than linear regression models but simpler than some machine learning models, making them a good middle-ground option for capturing complex relationships without becoming too hard to interpret.\n",
    "\n",
    "Given these factors and the nature of your data and research question, a negative binomial regression model seems like a reasonable choice for your analysis. It accounts for the specific statistical properties of count data and provides a more nuanced understanding of the relationship between population size and the number of inscriptions than a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070235af-31f3-47dc-82ce-9082727f13ab",
   "metadata": {},
   "source": [
    "The mean of the Inscription Count being approximately 88.5 and the variance being about 77,374 definitely suggests overdispersion in your data. Overdispersion is a situation where the variance is much greater than the mean, and it's a classic sign that a standard Poisson regression might not be appropriate for the data.\n",
    "\n",
    "Negative Binomial Regression is designed to handle overdispersion, so knowing these statistics actually strengthens the case for using it. It is specifically designed to model count data where the variance is greater than the mean, which appears to be the situation in your dataset.\n",
    "\n",
    "Given this information, I would say that Negative Binomial Regression is not just a good choice, but perhaps even the most appropriate choice given the characteristics of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a413e6-f98e-4e7b-9555-d52fdddb8ac8",
   "metadata": {},
   "source": [
    "## Bootstrap approach to Negative binomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad48506-1d5a-4d06-bee5-fc64b4be4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming grouped_df_sorted is your sorted DataFrame\n",
    "\n",
    "# Initialize directory and filenames for saving bootstrap samples\n",
    "bootstrap_dir = 'bootstrap-samples/modelling/'\n",
    "bootstrap_file = 'negative-binomial-pop-inscriptions.npz'\n",
    "bootstrap_path = os.path.join(bootstrap_dir, bootstrap_file)\n",
    "\n",
    "# Make directory if it doesn't exist\n",
    "if not os.path.exists(bootstrap_dir):\n",
    "            os.makedirs(bootstrap_dir)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap = 1000\n",
    "\n",
    "# Check if bootstrap samples file exists\n",
    "if os.path.exists(bootstrap_path):\n",
    "    # Load bootstrap coefficients and pseudo R-squared values from the .npz file\n",
    "    loaded_data = np.load(bootstrap_path)\n",
    "    # Extract individual arrays and convert back to DataFrames\n",
    "    bootstrap_coefs_df = pd.DataFrame(loaded_data['bootstrap_coefs'], columns=['const', 'log_urban_context_pop_est'])\n",
    "    bootstrap_pseudo_r2_df = pd.DataFrame(loaded_data['bootstrap_pseudo_r2'], columns=['pseudo_r2'])\n",
    "\n",
    "    print(\"Loaded saved bootstrap samples.\")\n",
    "    \n",
    "else:\n",
    "    # Initialize empty lists to store bootstrap estimates\n",
    "    bootstrap_coefs = []\n",
    "    bootstrap_pseudo_r2 = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for i in range(n_bootstrap):\n",
    "        # Sample with replacement from the original data\n",
    "        bootstrap_sample = grouped_df_sorted.sample(n=len(grouped_df_sorted), replace=True)\n",
    "    \n",
    "        # Prepare the data and fit the Negative Binomial model\n",
    "        X = bootstrap_sample['log_urban_context_pop_est']\n",
    "        y = bootstrap_sample['inscription_count']\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.GLM(y, X, family=sm.families.NegativeBinomial(alpha=1.0)).fit()\n",
    "    \n",
    "        # Store the estimated coefficients and pseudo R-squared value\n",
    "        bootstrap_coefs.append(model.params)\n",
    "        bootstrap_pseudo_r2.append(model.deviance / model.null_deviance)\n",
    "\n",
    "    # Convert bootstrap estimates to DataFrames\n",
    "    bootstrap_coefs_df = pd.DataFrame(bootstrap_coefs, columns=['const', 'log_urban_context_pop_est'])\n",
    "    bootstrap_pseudo_r2_df = pd.DataFrame(bootstrap_pseudo_r2, columns=['pseudo_r2'])\n",
    "    \n",
    "    # Save bootstrap coefficients and pseudo R-squared values to a single .npz file\n",
    "    np.savez(bootstrap_path, bootstrap_coefs=bootstrap_coefs_df.to_numpy(), bootstrap_pseudo_r2=bootstrap_pseudo_r2_df.to_numpy())\n",
    "\n",
    "    print(\"Generated new bootstrap samples and saved.\")\n",
    "\n",
    "# Print summary statistics for bootstrap coefficients\n",
    "print(\"Summary Statistics for Bootstrap Coefficients:\")\n",
    "print(bootstrap_coefs_df.describe(percentiles=[.025, .5, .975]))\n",
    "\n",
    "# Calculate the confidence intervals\n",
    "coef_ci_lower = bootstrap_coefs_df.quantile(0.025)\n",
    "coef_ci_upper = bootstrap_coefs_df.quantile(0.975)\n",
    "pseudo_r2_ci_lower = bootstrap_pseudo_r2_df['pseudo_r2'].quantile(0.025)\n",
    "pseudo_r2_ci_upper = bootstrap_pseudo_r2_df['pseudo_r2'].quantile(0.975)\n",
    "\n",
    "# Print summary statistics for bootstrap pseudo R-squared values\n",
    "print(\"Summary Statistics for Pseudo R-squared values:\")\n",
    "print(f\"Mean: {np.mean(bootstrap_pseudo_r2)}\")\n",
    "print(f\"Standard Deviation: {np.std(bootstrap_pseudo_r2)}\")\n",
    "print(f\"Minimum: {np.min(bootstrap_pseudo_r2)}\")\n",
    "print(f\"Maximum: {np.max(bootstrap_pseudo_r2)}\")\n",
    "\n",
    "# Create a figure with 3 subplots: one for each coefficient and one for pseudo R-squared\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Plot for Intercept (assuming it's named 'const')\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(bootstrap_coefs_df['const'], bins=30, edgecolor='k')\n",
    "plt.axvline(coef_ci_lower['const'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(coef_ci_upper['const'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Intercept')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for log_urban_context_pop_est\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(bootstrap_coefs_df['log_urban_context_pop_est'], bins=30, edgecolor='k')\n",
    "plt.axvline(coef_ci_lower['log_urban_context_pop_est'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(coef_ci_upper['log_urban_context_pop_est'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - log_urban_context_pop_est')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Pseudo R-squared\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(bootstrap_pseudo_r2_df['pseudo_r2'], bins=30, edgecolor='k')\n",
    "plt.axvline(pseudo_r2_ci_lower, color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(pseudo_r2_ci_upper, color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Pseudo R-squared')\n",
    "plt.xlabel('Pseudo R-squared Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda43c58-216e-4e98-9ee3-2e8794923cbb",
   "metadata": {},
   "source": [
    "## Negative binomial regression with both variables log-transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de7cc4-5e4a-43ba-9a5e-d0370bb84f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform both the population estimates and inscription count\n",
    "# This stabilizes the variance and may make the model better fit the data.\n",
    "grouped_df_sorted['log_urban_context_pop_est'] = np.log(grouped_df_sorted['urban_context_pop_est'])\n",
    "grouped_df_sorted['log_inscription_count'] = np.log(grouped_df_sorted['inscription_count'])\n",
    "\n",
    "# Prepare data for Negative Binomial Regression\n",
    "# Both predictor and response variables are log-transformed\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['log_inscription_count']\n",
    "\n",
    "# Add a constant term to the predictor variable\n",
    "# This is required for the intercept term in the model equation\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit a Negative Binomial model to the data\n",
    "# The family argument specifies that we are using a Negative Binomial model\n",
    "# This model is suitable for over-dispersed count data\n",
    "model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Display the model summary\n",
    "# This will show various statistics that help in understanding the fit and significance of the model\n",
    "print(model.summary())\n",
    "\n",
    "# Generate the fitted values and confidence intervals for plotting\n",
    "mu = model.mu  # Fitted values\n",
    "ci = model.conf_int(alpha=0.05)  # 95% Confidence intervals for the coefficients\n",
    "\n",
    "# Generate the frame to hold the fitted values and confidence intervals\n",
    "frame = pd.DataFrame({\n",
    "    'log_urban_context_pop_est': X['log_urban_context_pop_est'],\n",
    "    'mean': mu,\n",
    "})\n",
    "\n",
    "frame = frame.sort_values('log_urban_context_pop_est')\n",
    "\n",
    "# Calculate the confidence intervals for the mean response\n",
    "frame['mean_ci_lower'] = np.exp(frame['log_urban_context_pop_est'] * ci.iloc[1, 0] + ci.iloc[0, 0])\n",
    "frame['mean_ci_upper'] = np.exp(frame['log_urban_context_pop_est'] * ci.iloc[1, 1] + ci.iloc[0, 1])\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(frame['log_urban_context_pop_est'], frame['mean'], label='Fitted', color='red')\n",
    "\n",
    "# Plot the 95% confidence intervals\n",
    "plt.fill_between(frame['log_urban_context_pop_est'], frame['mean_ci_lower'], frame['mean_ci_upper'], color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Log of Inscription Count')\n",
    "plt.title('Negative Binomial Regression Fit (Log-Log)')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd6b91b-a096-4c0b-8a3f-d43644365848",
   "metadata": {},
   "source": [
    "## Interpretatoin of result - ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a93eb-8555-484f-aa00-98f39839c8af",
   "metadata": {},
   "source": [
    "\n",
    "The large change in pseudo R-squared value upon log-transforming the dependent variable (`inscription_count`) indicates that the new model doesn't explain the variance in the data as well as the previous model did. In simpler terms, the log-log model might not be as suitable for your data as the previous model where only the predictor was log-transformed. \n",
    "\n",
    "Here are some reasons why this could happen:\n",
    "\n",
    "### Loss of Information\n",
    "1. **Non-linearity in Original Scale**: The initial model may have captured a nonlinear relationship on the original scale that becomes less meaningful or even distorted when both variables are log-transformed.\n",
    "\n",
    "2. **Zero or Near-zero Values**: If your inscription count contains zeros or near-zero values, log-transforming those could result in undefined or extremely large/small values that distort the model fit.\n",
    "\n",
    "### Model Suitability\n",
    "3. **Different Underlying Process**: By log-transforming both variables, you're essentially hypothesizing that the relationship between the percentage change in `urban_context_pop_est` and the percentage change in `inscription_count` is linear. If this hypothesis doesn't fit the data well, the model's explanatory power will be lower.\n",
    "\n",
    "4. **Overdispersion**: The initial model used Negative Binomial regression to deal with overdispersion. By log-transforming the dependent variable, you may have inadvertently reduced the model's ability to handle overdispersion, affecting the fit.\n",
    "\n",
    "5. **Outliers or Leverage Points**: Log-transforming can sometimes amplify the effect of outliers or high-leverage points. If such points exist in your `inscription_count`, this could have a large effect on model fit.\n",
    "\n",
    "### Metric Sensitivity\n",
    "6. **Sensitivity of Pseudo R-squared**: It's worth noting that pseudo R-squared values are not as straightforward to interpret as R-squared values in linear regression. They are highly sensitive to model specification and are best used for comparing models on the same data, keeping in mind that they do not provide an absolute measure of goodness-of-fit.\n",
    "\n",
    "Given these considerations, if you observe a drastic reduction in model fit after the log transformation of the dependent variable, it would be worthwhile to further explore the data and possibly consider alternative models or transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6a023-5526-4cce-aa47-786340cb8107",
   "metadata": {},
   "source": [
    "### Reflections: Don't log-transform both variables for NBR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730c30d-133f-4452-84d1-6b8d7beb94f4",
   "metadata": {},
   "source": [
    "Log-transforming the response variable dramatically reduced the power of the relationship (pseudo $R^2$ dropped form 0.4207 to 0.02203). 'Normal negative binomial regressoin seems to work better, where only the predictor variable is log-transformed. ChatGPT suggests that pseudo $R^2$ is a good measure for comparing models (rather than an absolute indication of the model's power). Trying a model where both variable were log-transformed was worthwhile, but the 'normal' negative binomial regression works better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e82308-0e4f-43f1-9948-da9a96400f5d",
   "metadata": {},
   "source": [
    "## Bootstrap approach to OLS regression with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe514a70-f312-4ced-b6f4-999966edb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of bootstrap samples\n",
    "n_bootstraps = 1000\n",
    "\n",
    "# Initialize an empty list to hold the R-squared values\n",
    "bootstrap_r2 = []\n",
    "\n",
    "# Directory and file to save the bootstrap samples\n",
    "bootstrap_dir = 'bootstrap-samples/modelling/'\n",
    "bootstrap_file = 'ols-pop-inscriptions.npy'\n",
    "bootstrap_path = os.path.join(bootstrap_dir, bootstrap_file)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(bootstrap_dir):\n",
    "    os.makedirs(bootstrap_dir)\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(bootstrap_path):\n",
    "    print(\"Generating bootstrap samples...\")\n",
    "    bootstrap_coefs = []\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Resample the dataset with replacement\n",
    "        resampled_data = grouped_df_sorted.sample(n=len(grouped_df_sorted), replace=True)\n",
    "        \n",
    "        # Extract predictor and response variables\n",
    "        X = resampled_data['urban_context_pop_est']\n",
    "        y = resampled_data['inscription_count']\n",
    "        \n",
    "        # Add constant for the intercept term\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # Fit OLS model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        # Save coefficients\n",
    "        bootstrap_coefs.append(model.params.tolist())\n",
    "\n",
    "        # Save R-squared value\n",
    "        bootstrap_r2.append(model.rsquared)\n",
    "\n",
    "    # Save bootstrap coefficients and R-squared values to a file\n",
    "    np.savez(bootstrap_path.replace('.npy', '.npz'), coefs=bootstrap_coefs, r2=bootstrap_r2)\n",
    "    print(f\"Saved bootstrap samples to {bootstrap_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Bootstrap samples already exist in {bootstrap_path}. No need to regenerate.\")\n",
    "    saved_data = np.load(bootstrap_path.replace('.npy', '.npz'))\n",
    "    bootstrap_coefs = saved_data['coefs']\n",
    "    bootstrap_r2 = saved_data['r2']\n",
    "\n",
    "# If the file was freshly created or read from disk, we can proceed to analyze bootstrap_coefs\n",
    "\n",
    "# Create a DataFrame for easier handling\n",
    "bootstrap_df = pd.DataFrame(bootstrap_coefs, columns=['const', 'pop_est'])\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = bootstrap_df.describe(percentiles=[.025, .5, .975])\n",
    "print(\"Summary Statistics for Bootstrap Coefficients:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Extract 2.5% and 97.5% percentiles for confidence intervals\n",
    "ci_const = summary_stats.loc[['2.5%', '97.5%'], 'const']\n",
    "ci_pop_est = summary_stats.loc[['2.5%', '97.5%'], 'pop_est']\n",
    "\n",
    "# Generate summary statistics for R-squared values\n",
    "bootstrap_r2_array = np.array(bootstrap_r2)\n",
    "print(\"Summary Statistics for R-squared values:\")\n",
    "print(\"Mean:\", np.mean(bootstrap_r2_array))\n",
    "print(\"Standard Deviation:\", np.std(bootstrap_r2_array))\n",
    "print(\"Minimum:\", np.min(bootstrap_r2_array))\n",
    "print(\"Maximum:\", np.max(bootstrap_r2_array))\n",
    "\n",
    "# Calculate 2.5% and 97.5% percentiles for R-squared confidence intervals\n",
    "ci_r2 = np.percentile(bootstrap_r2, [2.5, 97.5])\n",
    "\n",
    "# Plot histograms for each coefficient and R-squared with confidence intervals\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Intercept\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(bootstrap_df['const'], bins=30, edgecolor='k')\n",
    "plt.axvline(ci_const['2.5%'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(ci_const['97.5%'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Intercept')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Population Estimate\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(bootstrap_df['pop_est'], bins=30, edgecolor='k')\n",
    "plt.axvline(ci_pop_est['2.5%'], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(ci_pop_est['97.5%'], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - Population Estimate')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# R-squared\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(bootstrap_r2, bins=30, edgecolor='k')\n",
    "plt.axvline(ci_r2[0], color='r', linestyle='dashed', linewidth=1, label='2.5% CI')\n",
    "plt.axvline(ci_r2[1], color='r', linestyle='dashed', linewidth=1, label='97.5% CI')\n",
    "plt.title('Bootstrap Distribution - R-squared')\n",
    "plt.xlabel('R-squared Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e06896-a7e0-4f6b-8e27-691212f428bb",
   "metadata": {},
   "source": [
    "### Reflections on bootrapping\n",
    "\n",
    "Bootstrapping an OLS model produced a somewhat higher R-squared result than an OLS model alone, and provided a better indication of uncertainty. Along with log transformation, consider using it as a secondary step considering the small size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b018cb-82ed-4ae9-b456-e11f5bbe367b",
   "metadata": {},
   "source": [
    "## Exploring alternative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0445ebe-4e94-4571-aa7a-468ff0afe4a7",
   "metadata": {},
   "source": [
    "_Output from ChatGPT_\n",
    "\n",
    "---\n",
    "\n",
    "### Preferred Methods for Highly Dispersed and Small Datasets\n",
    "\n",
    "#### 1. Negative Binomial Regression\n",
    "\n",
    "**Explanation**: The Negative Binomial Regression model extends the Poisson Regression model by introducing an extra parameter to handle overdispersion. Overdispersion occurs when your data exhibits more variability than what the Poisson model can accommodate, which is likely given the high dispersion in your dataset. This model is particularly suitable for count data like the number of inscriptions.\n",
    "\n",
    "**Advantages**: \n",
    "- Can handle overdispersion effectively.\n",
    "- Suitable for count data.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Assumes that counts are independent, which might not always be the case.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Robust Regression\n",
    "\n",
    "**Explanation**: Robust Regression aims to fit a regression model in the presence of corrupt or outlier data points. Unlike Ordinary Least Squares (OLS), which is sensitive to outliers, this method uses alternative loss functions to down-weight the influence of outliers, making it a suitable candidate for your dataset with many outliers.\n",
    "\n",
    "**Advantages**: \n",
    "- Less sensitive to outliers.\n",
    "- Similar to OLS but more robust.\n",
    "\n",
    "**Disadvantages**: \n",
    "- May have lower statistical power compared to OLS when the underlying assumptions of OLS are met (which is less likely in your case).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Generalized Linear Models (GLM)\n",
    "\n",
    "**Explanation**: GLMs extend linear models by allowing for response variables with error distribution models other than a normal distribution. Given that your dataset contains count data, a Poisson or Negative Binomial GLM could be appropriate. The link function in GLM specifies the relationship between the predictor and the mean of the response variable.\n",
    "\n",
    "**Advantages**:\n",
    "- Flexible; can handle different types of response variables.\n",
    "- Good for count data and can adjust for overdispersion.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Requires choosing the correct link function and distribution, which can be data-dependent.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Mixed-Effects Models\n",
    "\n",
    "**Explanation**: These models are particularly useful if there is a grouping variable (e.g., region, time period) that could introduce correlations between observations. By including both fixed effects (like a standard linear regression) and random effects (for the groupings), it can account for this kind of structure in the data.\n",
    "\n",
    "**Advantages**: \n",
    "- Can handle both fixed and random effects.\n",
    "- Useful for dealing with hierarchical or grouped data.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Model interpretation can be complex.\n",
    "- Requires sufficient data within each group for accurate random effects estimation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Spearman's Rank Correlation\n",
    "\n",
    "**Explanation**: Spearman's rank correlation is a non-parametric method that assesses the strength and direction of the relationship between two variables. It does this without making any assumptions about the distribution of the data, making it a robust choice for small datasets with outliers.\n",
    "\n",
    "**Advantages**: \n",
    "- Does not assume a specific distribution or relationship form.\n",
    "- Robust to outliers.\n",
    "\n",
    "**Disadvantages**: \n",
    "- Provides only a measure of association, not a full model.\n",
    "- Limited in capturing complex relationships.\n",
    "\n",
    "---\n",
    "\n",
    "Each of these methods has its own set of advantages and disadvantages, and the best choice may depend on specific characteristics of your dataset and what you find most critical: interpretability, robustness to outliers, or flexibility in modeling different forms of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef01862-8749-424c-8ab2-154c9d9972d9",
   "metadata": {},
   "source": [
    "## Robust regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621eff25-0f1b-42ea-82bf-982dd3d6ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data and fit the robust regression model\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "X = sm.add_constant(X)\n",
    "# Here 'HuberT' specifies the Huber T loss function, which is a commonly used function for robust regression\n",
    "model = sm.RLM(y, X, M=sm.robust.norms.HuberT()).fit()\n",
    "\n",
    "# Generate predictions and calculate the confidence interval for new observations\n",
    "# Note that for robust models, the confidence intervals can be less straightforward to compute\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(X['log_urban_context_pop_est'], predictions, label='Fitted', color='red')\n",
    "\n",
    "# Note: Unlike GLM models, getting the confidence interval for the predictions is less straightforward in robust regression\n",
    "# Therefore, this example doesn't include the confidence interval plotting\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Robust Regression Fit')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d511c1-47b5-467b-8b41-ec40abc39126",
   "metadata": {},
   "source": [
    "### Reflections on Robust regression\n",
    "\n",
    "Low p-values, low standard error compared to coefficient size. After running other models, perhaps return to this one and undertake one of the suggested approaches to measure the strength and quality of the relationship between variables.\n",
    "\n",
    "AIC/BIC: The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are often used for model selection and can be useful for comparing different types of models, though they're not measures of \"fit\" in the way that $R^2$ is. Lower values indicate a model that explains the data better, subject to a penalty for complexity.\n",
    "\n",
    "Residual Analysis: Plotting the residuals versus the fitted values can also provide valuable diagnostic information. In a good model, you'd like these to be randomly scattered with no obvious pattern.\n",
    "\n",
    "Cross-Validation: If your dataset is sufficiently large, you can use k-fold cross-validation to assess how well your model generalizes to new data. This is a robust way to assess model quality, but it may not be suitable for very small datasets.\n",
    "\n",
    "Bootstrapping: Since you've already been working with bootstrapping, you could apply it here as well to get more robust estimates of your coefficients and their confidence intervals.\n",
    "\n",
    "Out-of-Sample Prediction: If you have another dataset, or if you can partition your existing dataset, you can assess how well the model predicts the dependent variable in this out-of-sample data. This can be a very practical way of assessing model quality.\n",
    "\n",
    "Effect Size Measures: In the realm of statistical hypothesis testing, the effect size complements the p-value and gives you an idea of how \"big\" the observed relationship is. Cohen's \n",
    "$f^2$ is a commonly used effect size measure for regression analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae896ec-7b26-4884-b6c7-c925f5da35d2",
   "metadata": {},
   "source": [
    "## Generalised Linear Model assuming a Negative Binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85034e4e-bf53-418c-a868-0b32c4b5eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "\n",
    "# Add a constant term to the independent variable for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit a Generalized Linear Model assuming a Negative Binomial distribution\n",
    "nb_model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "\n",
    "# Print the summary statistics of the model fit\n",
    "print(nb_model.summary())\n",
    "\n",
    "# Generate predictions and calculate the confidence interval for new observations\n",
    "predictions = nb_model.get_prediction(X)\n",
    "frame = predictions.summary_frame(alpha=0.05)\n",
    "\n",
    "# Plot the observed data\n",
    "plt.scatter(X['log_urban_context_pop_est'], y, label='Observed', alpha=0.7)\n",
    "\n",
    "# Plot the fitted values\n",
    "plt.plot(X['log_urban_context_pop_est'], frame['mean'], label='Fitted', color='red')\n",
    "\n",
    "# Plot the 95% confidence intervals\n",
    "plt.fill_between(X['log_urban_context_pop_est'], frame['mean_ci_lower'], frame['mean_ci_upper'], color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Log of Population Estimate')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Generalized Linear Model (Negative Binomial) Fit')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a367d-5e63-45cf-afb9-fa2d3ae10028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "X = grouped_df_sorted['log_urban_context_pop_est']\n",
    "y = grouped_df_sorted['inscription_count']\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# GLM with Negative Binomial distribution\n",
    "nb_model = sm.GLM(y, X, family=sm.families.NegativeBinomial()).fit()\n",
    "print(\"Negative Binomial Model Summary:\")\n",
    "print(nb_model.summary())\n",
    "\n",
    "# GLM with Poisson distribution\n",
    "poisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n",
    "print(\"\\nPoisson Model Summary:\")\n",
    "print(poisson_model.summary())\n",
    "\n",
    "# GLM with Tweedie distribution\n",
    "# Note: The 'var_power' parameter can be adjusted; 1=Poisson, 2=Gamma, between 1 and 2 allows for Tweedie\n",
    "tweedie_model = sm.GLM(y, X, family=sm.families.Tweedie(var_power=1.5)).fit()\n",
    "print(\"\\nTweedie Model Summary:\")\n",
    "print(tweedie_model.summary())\n",
    "\n",
    "# You can extend this to plot and analyze the residuals and fitted values for each model if you wish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00120f-a51f-4ff1-9eaa-44744ea5f092",
   "metadata": {},
   "source": [
    "## Comparison from ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67facdc3-6fc5-4d8c-ae36-3fb4d3a8c0a0",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the relevant statistics for each model to compare their performance:\n",
    "\n",
    "### Negative Binomial Model\n",
    "- Log-Likelihood: -4256.0\n",
    "- Pseudo R-squared: 0.4207\n",
    "- Deviance: 1781.9\n",
    "- Pearson chi2: 3.26e+03\n",
    "\n",
    "### Poisson Model\n",
    "- Log-Likelihood: -79607.0\n",
    "- Pseudo R-squared: 1.000 (Note: This is likely incorrect, possibly a result of over-dispersion or other issues)\n",
    "- Deviance: 1.5520e+05\n",
    "- Pearson chi2: 3.85e+05\n",
    "\n",
    "### Tweedie Model\n",
    "- Log-Likelihood: -5562.3\n",
    "- Pseudo R-squared: 0.1144\n",
    "- Deviance: 13846.0\n",
    "- Pearson chi2: 3.37e+04\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "1. **Log-Likelihood**: Higher (closer to 0) is better. The Negative Binomial model has the highest log-likelihood, suggesting it fits the data better than the other models.\n",
    "  \n",
    "2. **Pseudo R-squared**: Closer to 1 is better, but take this with caution as it's not directly comparable to R-squared in OLS regression. The Negative Binomial model has a much more reasonable value compared to the Poisson model, whose Pseudo R-squared of 1 is likely not accurate.\n",
    "\n",
    "3. **Deviance and Pearson Chi-squared**: Lower values are generally better. Here, the Negative Binomial model has much lower values compared to the Poisson and Tweedie models, further supporting its superior fit.\n",
    "\n",
    "4. **Convergence & Iterations**: All models have converged, but the number of iterations for the Negative Binomial model is moderate, indicating that the model didn't have trouble fitting the data.\n",
    "\n",
    "5. **Coefficients & Standard Errors**: In all models, the coefficients are significant with a p-value of 0.000. This suggests that `log_urban_context_pop_est` is a significant predictor for `inscription_count` across all models.\n",
    "\n",
    "### Recommendation:\n",
    "Based on these statistics, the **Negative Binomial model** seems to be the best fit for your data among the three. It has the highest log-likelihood, a reasonable pseudo R-squared, and the lowest deviance and Pearson chi-squared statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7b0fc-e16b-49cc-a057-4199bdc331f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have two NumPy arrays:\n",
    "# log_urban_context_pop_est: the log of population estimates in urban context\n",
    "# inscription_count: the count of inscriptions\n",
    "log_urban_context_pop_est = np.array(grouped_df_sorted['log_urban_context_pop_est'])\n",
    "inscription_count = np.array(grouped_df_sorted['inscription_count'])\n",
    "\n",
    "# Perform Spearman's Rank Correlation\n",
    "spearman_corr, p_value = stats.spearmanr(log_urban_context_pop_est, inscription_count)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Spearman's Rank Correlation Coefficient: {spearman_corr}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Optional: Create a scatter plot for visualization\n",
    "plt.scatter(log_urban_context_pop_est, inscription_count)\n",
    "plt.xlabel('Log of Population Estimate in Urban Context')\n",
    "plt.ylabel('Inscription Count')\n",
    "plt.title('Scatter Plot of Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbc7d0-6f8a-454b-9a26-11c004b449cb",
   "metadata": {},
   "source": [
    "## To-do: residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d383b4d-0802-4643-a04b-f01816a507c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a30ba0a9-25f3-478a-952b-7284e95b0200",
   "metadata": {},
   "source": [
    "# SPA using letter counts\n",
    "\n",
    "The LIRE dataset, imported as the variable 'lire', contains a column called 'clearn_text_conservative'. We will use that column and count the number of letters in it, excluding spaces and special characters. The letter count will serve as a proxy for the amount of information contained in the inscription. This additional analysis is suggested in Hanson's article about inscription counts and populations (citation needed - see above).\n",
    "\n",
    "## First, view 50 rows of 'clean_text_conservative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e7655-8892-4e55-b222-2195421ecf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum column width to None for unrestricted display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Now display the first 50 rows of 'clean_text_conservative'\n",
    "print(lire['clean_text_conservative'].head(50))\n",
    "\n",
    "# Reset the maximum column width to 50 characters\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d1cd6-a486-4cd0-a19a-d3ece66093d3",
   "metadata": {},
   "source": [
    "## Count letters in 'clean_text_conservative'\n",
    "\n",
    "Append counts in new column called 'letter_count_ctc'\n",
    "\n",
    "### Latin-speaking empire, excluding Roma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785a33c-d757-43f2-a210-f07a5b6bbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dataset\n",
    "# lire = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Define the alphabets for Latin and a more comprehensive set for Ancient Greek\n",
    "latin_alphabet = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "greek_alphabet = set(\"αβγδεζηθικλμνξοπρστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩάέήίόύώϊϋΐΰἀἁἂἃἄἅἆἇἈἉἊἋἌἍἎἏἐἑἒἓἔἕἘἙἚἛἜἝἠἡἢἣἤἥἦἧἨἩἪἫἬἭἮἯἰἱἲἳἴἵἶἷἸἹἺἻἼἽἾἿὀὁὂὃὄὅὈὉὊὋὌὍὐὑὒὓὔὕὖὗὙὛὝὟὠὡὢὣὤὥὦὧὨὩὪὫὬὭὮὯὰὲὴὶὸὺὼᾀᾁᾂᾃᾄᾅᾆᾇᾈᾉᾊᾋᾌᾍᾎᾏᾐᾑᾒᾓᾔᾕᾖᾗᾘᾙᾚᾛᾜᾝᾞᾟᾠᾡᾢᾣᾤᾥᾦᾧᾨᾩᾪᾫᾬᾭᾮᾯ\")\n",
    "\n",
    "def count_letters(text):\n",
    "    if text is None:\n",
    "        return 0\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if char in latin_alphabet or char in greek_alphabet:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Apply the function to the 'clean_text_conservative' column\n",
    "lire['letter_count_ctc'] = lire['clean_text_conservative'].apply(count_letters)\n",
    "\n",
    "# Show the first few rows to verify\n",
    "# print(lire.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bebad44-2d36-45d9-8d60-9e2cf4693e74",
   "metadata": {},
   "source": [
    "## Letter count descriptive statistics and other characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a3eb5-c238-4456-a057-95fac33cb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'lire' is your DataFrame and 'letter_count_ctc' is the column you are interested in\n",
    "desc_stats = lire['letter_count_ctc'].describe()\n",
    "\n",
    "# Descriptive statistics\n",
    "\n",
    "print(\"Descriptive Statistics for 'letter_count_ctc':\")\n",
    "print(desc_stats)\n",
    "\n",
    "# Debugging \n",
    "\n",
    "# Count the number of 0-letter inscriptions\n",
    "zero_letter_count = lire['letter_count_ctc'].value_counts().get(0, 0)\n",
    "print(f\"Number of rows where 'letter_count_ctc' is 0: {zero_letter_count}\")\n",
    "\n",
    "# Count the number of null values in 'letter_count_ctc'\n",
    "num_null_values = lire['letter_count_ctc'].isna().sum()\n",
    "print(f\"The number of null values in 'letter_count_ctc' is: {num_null_values}\")\n",
    "\n",
    "# Count the number of negative values in 'letter_count_ctc'\n",
    "negative_count = len(lire[lire['letter_count_ctc'] < 0])\n",
    "print(f\"Number of negative values in 'letter_count_ctc': {negative_count}\")\n",
    "\n",
    "# Create a histogram\n",
    "\n",
    "# Filter out rows where 'letter_count_ctc' is zero if needed\n",
    "filtered_lire = lire[lire['letter_count_ctc'] > 0]\n",
    "\n",
    "# Compute the mean and standard deviation\n",
    "mean_letter_count = np.mean(filtered_lire['letter_count_ctc'])\n",
    "std_dev_letter_count = np.std(filtered_lire['letter_count_ctc'])\n",
    "\n",
    "# Compute the median letter count\n",
    "median_letter_count = filtered_lire['letter_count_ctc'].median()\n",
    "\n",
    "# Find the maximum value in 'letter_count_ctc'\n",
    "max_count = lire['letter_count_ctc'].max()\n",
    "print(f\"The highest count in 'letter_count_ctc' is: {max_count}\")\n",
    "\n",
    "# Sum the values in 'letter_count_ctc'\n",
    "total_letters = lire['letter_count_ctc'].sum()\n",
    "print(f\"The total number of letters in the dataset is: {total_letters}\")\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(filtered_lire['letter_count_ctc'], bins=range(0, int(filtered_lire['letter_count_ctc'].max()) + 3, 3), edgecolor='black')\n",
    "plt.xlabel('Letter Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Letter Counts in 3-character bins')\n",
    "\n",
    "# Set x-axis limits\n",
    "plt.xlim([0, 250])  # Change these numbers as you see fit\n",
    "\n",
    "# Add lines for the mean and standard deviation\n",
    "plt.axvline(mean_letter_count, color='r', linestyle='dashed', linewidth=1, label=f\"Mean: {mean_letter_count:.2f}\")\n",
    "plt.axvline(mean_letter_count - std_dev_letter_count, color='g', linestyle='dashed', linewidth=1, label=f\"1 SD Below Mean\")\n",
    "plt.axvline(mean_letter_count + std_dev_letter_count, color='g', linestyle='dashed', linewidth=1, label=f\"1 SD Above Mean\")\n",
    "\n",
    "# Add line for the median\n",
    "plt.axvline(median_letter_count, color='b', linestyle='dashed', linewidth=1, label=f\"Median: {median_letter_count:.2f}\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f103b0-0225-4e29-8014-20f159121f6a",
   "metadata": {},
   "source": [
    "## SPA weighted using letter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b3d4d-aa27-49e2-a40e-7ac4432aff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start summed probability function\n",
    "\n",
    "def compute_summed_probability(df, earliest_date, latest_date, resolution=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the summed probability distribution of inscriptions with error margins,\n",
    "    weighted by the number of letters in each inscription.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the date ranges and letter counts for each inscription.\n",
    "          The DataFrame is expected to have columns 'not_before', 'not_after', and 'letter_count_ctc'.\n",
    "          \n",
    "    - earliest_date, latest_date: The date limits for the distribution, \n",
    "                                  used for initializing the summed probability array.\n",
    "                                  \n",
    "    - resolution: The bin size for the histogram (in years). \n",
    "                  This determines the granularity of the summed probability array.\n",
    "    \n",
    "    Returns:\n",
    "    - summed_prob: The summed probability distribution, \n",
    "                   where each element represents the summed probability for a bin.\n",
    "                   \n",
    "    - std_devs: The standard deviations for each bin, \n",
    "                calculated from the variances of the probabilities that contribute to each bin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an array of zeros for storing summed probabilities\n",
    "    num_bins = int((latest_date - earliest_date) / resolution)\n",
    "    summed_prob = np.zeros(num_bins)\n",
    "    \n",
    "    # Initialize an array of zeros for storing the sum of the variances\n",
    "    std_devs = np.zeros(num_bins)\n",
    "    \n",
    "    # Loop through each row (inscription) in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the date range and the letter count of the inscription\n",
    "        start_date = row['not_before']\n",
    "        end_date = row['not_after']\n",
    "        letter_count = row['letter_count_ctc']\n",
    "        \n",
    "        # Convert the date range to index range in the summed_prob array\n",
    "        start_idx = int((start_date - earliest_date) / resolution)\n",
    "        end_idx = int((end_date - earliest_date) / resolution)\n",
    "        \n",
    "        # Ensure the indices are within bounds\n",
    "        start_idx = max(0, min(start_idx, num_bins - 1))\n",
    "        end_idx = max(0, min(end_idx, num_bins - 1))\n",
    "        \n",
    "        # Compute the uniform probability across the date range for the inscription\n",
    "        uniform_prob = 1.0 / (end_idx - start_idx + 1) if end_idx >= start_idx else 0\n",
    "        \n",
    "        # Distribute the probability across the relevant bins, weighted by the letter count\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            weighted_prob = uniform_prob * letter_count\n",
    "            summed_prob[i] += weighted_prob\n",
    "            \n",
    "    # Compute variance for each bin based on the final summed_prob\n",
    "    variance = summed_prob * (1 - summed_prob / np.sum(summed_prob))\n",
    "    \n",
    "    # Make sure no negative values exist due to numerical errors\n",
    "    # variance[variance < 0] = 0  \n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_devs = np.sqrt(variance)\n",
    "    \n",
    "    return summed_prob, std_devs\n",
    "\n",
    "# End summed probability function\n",
    "\n",
    "# Function to run the analysis\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... [compute_summed_probability stays the same] ...\n",
    "\n",
    "def run_analysis(df, bootstrap_samples_dir, file_name):\n",
    "    \"\"\"\n",
    "    Analyzes a dataset of inscriptions to calculate and visualize a summed probability distribution.\n",
    "    Bootstrapping is used to calculate the 95% confidence interval. The results are displayed as a bar graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing date ranges and letter counts for each inscription.\n",
    "          Expected columns: 'not_before', 'not_after', 'letter_count_ctc'\n",
    "    - bootstrap_samples_dir: Directory where bootstrap samples will be saved.\n",
    "    - file_name: A unique name to identify and save the bootstrap samples file.\n",
    "    \n",
    "    Returns:\n",
    "    None. The function generates a bar graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the lower and upper limits for the x-axis in the plot\n",
    "    X_LIM_LOW = -100\n",
    "    X_LIM_HIGH = 600\n",
    "    \n",
    "    # Initialize parameters\n",
    "    n_iterations = 1000  # Number of bootstrap iterations\n",
    "    earliest_date, latest_date = df['not_before'].min(), df['not_after'].max()  # Get the earliest and latest dates from the DataFrame\n",
    "    resolution = 5  # Bin size for histogram in years\n",
    "    window_size = 5  # Size of moving window for moving average\n",
    "    \n",
    "    # Create new directory for storing bootstrap samples if it doesn't already exist\n",
    "    os.makedirs(bootstrap_samples_dir, exist_ok=True)\n",
    "\n",
    "    # Define the saved_sample_path based on the directory and file name\n",
    "    saved_sample_path = os.path.join(bootstrap_samples_dir, f\"{file_name}.npy\")\n",
    "\n",
    "    # Calculate the number of inscriptions in the DataFrame\n",
    "    n_inscriptions = df.shape[0]\n",
    "    \n",
    "# Check if a bootstrap sample file already exists for this file_name\n",
    "    if os.path.exists(saved_sample_path):\n",
    "        print(f\"Loading bootstrap sample file for: {file_name}...\")\n",
    "        bootstrap_summed_probs = np.load(saved_sample_path)\n",
    "    else:\n",
    "        # If no bootstrap sample exists, generate a new one\n",
    "        print(f\"Calculating bootstrap samples for: {file_name}...\")\n",
    "        bootstrap_summed_probs = np.zeros((n_iterations, int((latest_date - earliest_date) / resolution)))\n",
    "        for i in tqdm(range(n_iterations)):\n",
    "            bootstrap_sample = df.sample(n=len(df), replace=True)\n",
    "            bootstrap_summed_prob, _ = compute_summed_probability(bootstrap_sample, earliest_date, latest_date, resolution)\n",
    "            bootstrap_summed_probs[i, :] = bootstrap_summed_prob\n",
    "        \n",
    "        # Save the newly generated bootstrap samples\n",
    "        print(f\"Saving bootstrap sample file for: {file_name}...\")\n",
    "        np.save(saved_sample_path, bootstrap_summed_probs)\n",
    "        \n",
    "    # Calculate the lower and upper bounds for the 95% confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_summed_probs, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(bootstrap_summed_probs, 97.5, axis=0)\n",
    "    \n",
    "    # Calculate the summed probability for the DataFrame\n",
    "    summed_prob, _ = compute_summed_probability(df, earliest_date, latest_date, resolution)\n",
    "    \n",
    "    # Generate x-values for plotting\n",
    "    x_values = np.arange(earliest_date, latest_date, resolution)[:len(summed_prob)]\n",
    "\n",
    "    # Calculate moving averages\n",
    "    moving_avg = uniform_filter1d(summed_prob, size=window_size, mode='nearest')\n",
    "    moving_avg_lower = uniform_filter1d(lower_bound, size=window_size, mode='nearest')\n",
    "    moving_avg_upper = uniform_filter1d(upper_bound, size=window_size, mode='nearest')\n",
    "    \n",
    "    # Plotting\n",
    "    plt.bar(x_values, summed_prob, width=resolution, color='blue', alpha=0.5, label='Summed Probability')\n",
    "    plt.fill_between(x_values, lower_bound, upper_bound, color='gray', alpha=0.5, label='95% CI')\n",
    "    plt.plot(x_values, moving_avg, color='red', linestyle='-', linewidth=2, label='5-point MA (True)')\n",
    "    plt.fill_between(x_values, moving_avg_lower, moving_avg_upper, color='orange', alpha=0.4, label='5-point MA (95% CI)')\n",
    "        \n",
    "    plt.title(f\"Summed Probability Distribution (n={n_inscriptions})\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.xlim([X_LIM_LOW, X_LIM_HIGH])\n",
    "    plt.ylabel('Summed Probability')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# End function to run the analysis\n",
    "\n",
    "# For only Latin-speaking provinces excluding Roma\n",
    "print(\"Running analysis for letter counts, clearn text conservative, Latin-speaking provinces excluding Roma...\")\n",
    "bootstrap_samples_dir_3 = \"bootstrap-samples/spa-letter-count/conservative/\"\n",
    "file_name_3 = \"latin-speaking-no-roma\"\n",
    "lire_3 = lire[(lire['province'] != 'Roma') & (lire['province_language'] == 'Latin')]\n",
    "filtered_lire_3 = lire_3[lire_3['letter_count_ctc'] > 0]\n",
    "min_value = filtered_lire_3['letter_count_ctc'].min()\n",
    "print(f\"The minimum value in the 'letter_count_ctc' column is {min_value}\") # debugging negative sq root problem\n",
    "run_analysis(filtered_lire_3, bootstrap_samples_dir_3, file_name_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37c1cb-40b3-4c7b-8b2a-d7944df3d0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
